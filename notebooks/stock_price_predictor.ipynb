{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stock Price Prediction with LSTM and Hyperparameter Optimization Using Optuna**\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup and Installation](#Setup-and-Installation)\n",
    "3. [Project Structure Overview](#Project-Structure-Overview)\n",
    "4. [Importing Libraries](#Importing-Libraries)\n",
    "5. [Utility Functions](#Utility-Functions)\n",
    "6. [Data Preprocessing](#Data-Preprocessing)\n",
    "7. [Model Definition](#Model-Definition)\n",
    "8. [Training and Evaluation Functions](#Training-and-Evaluation-Functions)\n",
    "9. [Hyperparameter Optimization with Optuna](#Hyperparameter-Optimization-with-Optuna)\n",
    "10. [Training the Final Model](#Training-the-Final-Model)\n",
    "11. [Model Evaluation and Inference](#Model-Evaluation-and-Inference)\n",
    "12. [Visualization](#Visualization)\n",
    "13. [Conclusion](#Conclusion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Introduction\"></a>\n",
    "## **1. Introduction**\n",
    "\n",
    "Welcome to the **Stock Price Prediction** tutorial! In this notebook, we will walk through the entire process of building a Long Short-Term Memory (LSTM) neural network to predict stock prices. We'll cover data preprocessing, model training with early stopping, hyperparameter optimization using Optuna, model evaluation, and visualization of results.\n",
    "\n",
    "By the end of this tutorial, you'll have a solid understanding of how to implement time series forecasting using LSTM networks and optimize your model's performance with advanced hyperparameter tuning techniques.\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"Setup-and-Installation\"></a>\n",
    "## **2. Setup and Installation**\n",
    "\n",
    "Before we begin, ensure that you have the necessary libraries installed. You can install them using `pip`. If you're using a virtual environment (which is recommended), activate it first.\n",
    "\n",
    "```bash\n",
    "pip install torch numpy pandas scikit-learn matplotlib optuna yfinance\n",
    "```\n",
    "\n",
    "**Note:** If you're running this notebook on a system without a GPU, training might be slower. Ensure that PyTorch is installed with the appropriate CUDA version if you intend to use GPU acceleration.\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"Project-Structure-Overview\"></a>\n",
    "## **3. Project Structure Overview**\n",
    "\n",
    "For clarity, here's a summary of the project structure we'll be emulating within this notebook:\n",
    "\n",
    "```\n",
    "├── data/\n",
    "│   └── *.parquet                 # Stock data for different symbols\n",
    "├── src/\n",
    "│   ├── data_preprocessing.py     # Data preprocessing functions\n",
    "│   ├── model_training.py         # Model definition and training functions\n",
    "│   ├── visualization.py          # Plotting functions\n",
    "│   └── utils.py                  # Utility functions such as directory setup, GPU check\n",
    "├── main.py                       # Main script to run the entire project\n",
    "├── README.md                     # Project documentation\n",
    "└── requirements.txt              # Project dependencies\n",
    "```\n",
    "\n",
    "In this notebook, we'll integrate all these components into a single, cohesive workflow with clear explanations and visual aids.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Importing-Libraries\"></a>\n",
    "## **4. Importing Libraries**\n",
    "\n",
    "Let's start by importing all the necessary libraries. We'll also set up logging to help with debugging and monitoring the progress of our computations.\n",
    " \n",
    "**Explanation:**\n",
    "\n",
    "- **Logging:** Helps track the flow of the program and debug issues.\n",
    "- **`functools.partial`:** Allows us to fix certain arguments of a function and generate a new function.\n",
    "- **`yfinance`:** Used to download stock data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "import yfinance as yf\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Utility-Functions\"></a>\n",
    "## **5. Utility Functions**\n",
    "\n",
    "Utility functions help keep our code organized and reusable. Below are essential utility functions for setting random seeds, ensuring directory existence, and checking GPU availability.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`set_seed`:** Ensures that our results are reproducible by setting seeds for various random number generators.\n",
    "- **`ensure_dir_exists`:** Creates a directory if it doesn't already exist, useful for saving data or models.\n",
    "- **`check_gpu`:** Determines whether to use a GPU or CPU for computations.\n",
    "- **`load_data`:** Loads stock data from a specified parquet file, ensuring the file exists.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): The seed value to set for random number generators.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Setting random seed: {seed}\")\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Function to download stock data for a list of symbols\n",
    "def download_stock_data(symbols, start_date=\"2015-01-01\", end_date=None, data_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Downloads stock data for the given symbols and saves them as parquet files.\n",
    "\n",
    "    Args:\n",
    "        symbols (list): List of stock symbols to download.\n",
    "        start_date (str): Start date for the data in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date for the data in 'YYYY-MM-DD' format. If None, defaults to today's date.\n",
    "        data_dir (str): Directory to save the downloaded data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting stock data download.\")\n",
    "    ensure_dir_exists(data_dir)\n",
    "    \n",
    "    if end_date is None:\n",
    "        end_date = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        logging.info(f\"Downloading data for {symbol} from {start_date} to {end_date}.\")\n",
    "        try:\n",
    "            stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "            \n",
    "            # Flatten MultiIndex if present and clean column names\n",
    "            if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "                stock_data.columns = ['_'.join(col).strip().replace(' ', '_') for col in stock_data.columns]\n",
    "\n",
    "            # Save data to Parquet format for efficient storage\n",
    "            file_path = os.path.join(data_dir, f\"{symbol}_stock_data.parquet\")\n",
    "            stock_data.to_parquet(file_path)\n",
    "            logging.info(f\"Data for {symbol} saved to {file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error downloading data for {symbol}: {e}\")\n",
    "\n",
    "def ensure_dir_exists(dir_path: str):\n",
    "    \"\"\"\n",
    "    Ensures that the specified directory exists; creates it if it does not.\n",
    "    \n",
    "    Args:\n",
    "        dir_path (str): Path to the directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        logging.info(f\"Directory created: {dir_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Directory already exists: {dir_path}\")\n",
    "\n",
    "# Check if GPU is available\n",
    "def check_gpu():\n",
    "    \"\"\"\n",
    "    Checks if a GPU is available and returns the appropriate device.\n",
    "\n",
    "    Returns:\n",
    "        device (torch.device): 'cuda' if GPU is available, else 'cpu'.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logging.info(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logging.info(\"No GPU detected. Using CPU.\")\n",
    "    \n",
    "    logging.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "    logging.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    return device\n",
    "\n",
    "# Load data function\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads stock data from a parquet file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the parquet file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): DataFrame containing the stock data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data from {filepath}\")   \n",
    "    try:\n",
    "        df = pd.read_parquet(filepath)\n",
    "        logging.info(f\"Data loaded successfully from {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {filepath}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data-Preprocessing\"></a>\n",
    "## **6. Data Preprocessing**\n",
    "\n",
    "Data preprocessing is a crucial step in any machine learning pipeline. Here, we'll normalize the stock price data and create input sequences suitable for training our LSTM model.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`preprocess_data`:** Normalizes the target stock price using `MinMaxScaler` to scale the data between 0 and 1, which is beneficial for training neural networks.\n",
    "- **`create_sequences`:** Generates input sequences of a specified length (`sequence_length`) and their corresponding future labels (`pred_steps`).\n",
    "- **`split_data`:** Divides the data into training and validation sets based on the `test_size` ratio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column=\"Adj_Close\", feature_columns=None, scaler=None):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by selecting the target column and normalizing it.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the stock data.\n",
    "        target_column (str, optional): The name of the target column. Defaults to \"Adj_Close\".\n",
    "        feature_columns (_type_, optional): _description_. Defaults to None.\n",
    "        scaler (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        data_normalized (np.ndarray): Normalized data.\n",
    "        scaler (MinMaxScaler): Scaler used for normalization.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting data preprocessing.\")\n",
    "    if target_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{target_column}' not found in DataFrame.\")\n",
    "\n",
    "    # Select relevant features for LSTM\n",
    "    feature_columns = feature_columns or [target_column]\n",
    "    data = df[feature_columns].values\n",
    "\n",
    "    # Handle missing values (forward and backward fill)\n",
    "    data = pd.DataFrame(data).fillna(method='ffill').fillna(method='bfill').values\n",
    "\n",
    "    # Initialize scaler if none provided\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # Normalize data to the range [0, 1]\n",
    "    data_normalized = scaler.fit_transform(data)\n",
    "    \n",
    "    logging.info(f\"Data normalization complete. Shape: {data_normalized.shape}\")\n",
    "    return data_normalized, scaler\n",
    "\n",
    "def create_sequences(data, sequence_length, pred_steps):\n",
    "    \"\"\"\n",
    "    Creates input sequences and corresponding labels for multi-step prediction.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The normalized data array.\n",
    "        sequence_length (int): Length of each input sequence.\n",
    "        pred_steps (int): Number of future steps to predict.\n",
    "\n",
    "    Returns:\n",
    "        sequences (np.ndarray): Array of input sequences.\n",
    "        labels (np.ndarray): Array of corresponding labels (future values).\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating sequences.\")\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length - pred_steps + 1):\n",
    "        seq = data[i:i + sequence_length]\n",
    "        label = data[i + sequence_length:i + sequence_length + pred_steps]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "    logging.info(f\"Sequences created. Number of sequences: {len(sequences)}, Sequence length: {sequence_length}\")\n",
    "    return sequences, labels\n",
    "\n",
    "def split_data(sequences, labels, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the sequences and labels into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        sequences (np.ndarray): Input sequences.\n",
    "        labels (np.ndarray): Corresponding labels.\n",
    "        test_size (float): Proportion of the dataset to include in the validation split.\n",
    "\n",
    "    Returns:\n",
    "        X_train (np.ndarray): Training sequences.\n",
    "        X_val (np.ndarray): Validation sequences.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Splitting data with test size {test_size}.\")\n",
    "    split_index = int(len(sequences) * (1 - test_size))\n",
    "    X_train = sequences[:split_index]\n",
    "    y_train = labels[:split_index]\n",
    "    X_val = sequences[split_index:]\n",
    "    y_val = labels[split_index:]\n",
    "    logging.info(f\"Data split into {len(X_train)} training and {len(X_val)} validation samples.\")\n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Model-Definition\"></a>\n",
    "## **7. Model Definition**\n",
    "\n",
    "Here, we define the LSTM model architecture and implement early stopping to prevent overfitting during training.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`SequentialLSTM`:** Defines the LSTM architecture tailored for multi-step predictions. The output layer is adjusted to output predictions for multiple future steps.\n",
    "- **`EarlyStopping`:** Monitors the validation loss and stops training if it doesn't improve after a specified number of epochs (`patience`), helping to prevent overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for sequential data.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Number of input features.\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "        num_layers (int): Number of stacked LSTM layers.\n",
    "        output_size (int): Number of output features.\n",
    "        pred_steps (int): Number of future steps to predict.\n",
    "        dropout (float): Dropout rate to prevent overfitting.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, pred_steps, dropout=0.2):\n",
    "        super(SequentialLSTM, self).__init__()\n",
    "        self.pred_steps = pred_steps\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size * pred_steps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): Output tensor of shape (batch_size, pred_steps, output_size).\n",
    "        \"\"\"\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        out = output[:, -1, :]  # Take the output of the last time step\n",
    "        out = self.fc(out)      # Linear layer\n",
    "        out = out.view(-1, self.pred_steps, 1)  # Reshape for multi-step prediction\n",
    "        return out\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation loss doesn't improve.\n",
    "    \n",
    "    Args:\n",
    "        patience (int): Number of epochs to wait before stopping.\n",
    "        min_delta (float): Minimum change to qualify as an improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            logging.debug(f\"Initial validation loss set to {val_loss:.6f}.\")\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            logging.debug(f\"No improvement in validation loss for {self.counter} epochs.\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                logging.info(\"Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            logging.debug(f\"Validation loss improved to {val_loss:.6f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Training-and-Evaluation-Functions\"></a>\n",
    "## **8. Training and Evaluation Functions**\n",
    "\n",
    "These functions handle the training process, including early stopping, and evaluate the model's performance on the validation set.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`train_model_with_early_stopping`:** Handles the training loop, including forward and backward passes, loss computation, optimizer steps, validation, and early stopping based on validation loss improvements.\n",
    "- **`evaluate_model`:** Computes the validation loss, which is crucial for monitoring model performance and for hyperparameter optimization.\n",
    "- **`save_checkpoint`:** Saves the model's state dictionary, allowing us to reload the best-performing model later.\n",
    "- **`load_model`:** Loads a saved model's state dictionary and prepares it for inference or further training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(\n",
    "    model, X_train, y_train, X_val, y_val,\n",
    "    epochs=500, batch_size=32, learning_rate=0.001, patience=5, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        X_train (np.ndarray): Training input data.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        X_val (np.ndarray): Validation input data.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "        epochs (int): Maximum number of epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        patience (int): Patience for early stopping.\n",
    "        verbose (bool): Whether to print training progress.\n",
    "    \n",
    "    Returns:\n",
    "        model (nn.Module): The trained model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logging.info(f\"Training on device: {device}\")\n",
    "\n",
    "    # Convert data to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            inputs = X_train_tensor[i:i + batch_size].to(device)\n",
    "            targets = y_train_tensor[i:i + batch_size].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val_tensor.to(device)).item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = epoch_loss / len(X_train_tensor)\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, filepath=\"best_model_temp.pth\")\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            if verbose:\n",
    "                logging.info(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(torch.load(\"best_model_temp.pth\"))\n",
    "    logging.info(\"Loaded the best model weights.\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model.\n",
    "        X_val (np.ndarray): Validation input data.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "    \n",
    "    Returns:\n",
    "        val_loss (float): Validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "    logging.debug(f\"Validation loss: {val_loss:.6f}\")\n",
    "    return val_loss\n",
    "\n",
    "def save_checkpoint(model, filepath=\"best_model.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the model's state dictionary to a file.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to save.\n",
    "        filepath (str): Path to the file where the model will be saved.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    logging.info(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model: nn.Module, load_path: str, device: torch.device = torch.device(\"cpu\")) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Loads the model state dictionary from a file.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model instance to load the state into.\n",
    "        load_path (str): Path to the saved model file.\n",
    "        device (torch.device): Device to map the model to.\n",
    "    \n",
    "    Returns:\n",
    "        model (nn.Module): The model with loaded state dictionary.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logging.info(f\"Model loaded from {load_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Hyperparameter-Optimization-with-Optuna\"></a>\n",
    "## **9. Hyperparameter Optimization with Optuna**\n",
    "\n",
    "Hyperparameter tuning is essential for maximizing model performance. We'll use Optuna, a powerful hyperparameter optimization framework, to find the best set of hyperparameters for our LSTM model.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`objective`:** This function defines the hyperparameter space and evaluates the model's performance with different hyperparameter configurations. Optuna will use this function to suggest and evaluate different combinations.\n",
    "- **Hyperparameters Tuned:**\n",
    "  - **`hidden_size`:** Number of features in the hidden state of the LSTM.\n",
    "  - **`num_layers`:** Number of stacked LSTM layers.\n",
    "  - **`dropout`:** Dropout rate to prevent overfitting.\n",
    "  - **`learning_rate`:** Learning rate for the optimizer.\n",
    "  - **`batch_size`:** Number of samples per gradient update.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 20:21:26,733 - INFO - Setting random seed: 42\n",
      "2024-11-28 20:21:26,734 - INFO - GPU detected: NVIDIA GeForce RTX 4090\n",
      "2024-11-28 20:21:26,735 - INFO - CUDA version: 12.4\n",
      "2024-11-28 20:21:26,735 - INFO - PyTorch version: 2.5.1+cu124\n",
      "2024-11-28 20:21:26,736 - INFO - Using device: cuda\n",
      "2024-11-28 20:21:26,736 - INFO - Starting stock data download.\n",
      "2024-11-28 20:21:26,736 - INFO - Downloading data for AAPL from 2015-01-01 to 2024-11-28.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "2024-11-28 20:21:26,781 - INFO - Data for AAPL saved to ../data/AAPL_stock_data.parquet\n",
      "2024-11-28 20:21:26,781 - INFO - Downloading data for MSFT from 2015-01-01 to 2024-11-28.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "2024-11-28 20:21:26,814 - INFO - Data for MSFT saved to ../data/MSFT_stock_data.parquet\n",
      "2024-11-28 20:21:26,814 - INFO - Downloading data for NVDA from 2015-01-01 to 2024-11-28.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "2024-11-28 20:21:26,844 - INFO - Data for NVDA saved to ../data/NVDA_stock_data.parquet\n",
      "2024-11-28 20:21:26,845 - INFO - Downloading data for ['AAPL', 'MSFT', 'NVDA']...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set up the root directory for the project and add it to sys.path\n",
    "root_dir = os.path.abspath(\"../\")  # Set the project root directory\n",
    "sys.path.append(root_dir)  # Add the project root to sys.path for imports\n",
    "\n",
    "# Set up the data directory for the project\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging for detailed tracking of progress\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"project_log.log\")  # Log to a file as well\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set pandas options for better readability\n",
    "pd.set_option('display.max_columns', None)  # to show all columns\n",
    "pd.set_option('display.max_rows', None)  # to show all rows\n",
    "\n",
    "from src.data_preprocessing import create_sequences, preprocess_data, split_data\n",
    "from src.model_training import SequentialLSTM, train_model_with_early_stopping, evaluate_model, save_checkpoint, load_model\n",
    "from src.utils import load_data, set_seed, check_gpu, download_stock_data\n",
    "from src.visualization import plot_predictions, plot_data\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Get device\n",
    "device = check_gpu()\n",
    "\n",
    "# Download and dave data\n",
    "symbols = [\"AAPL\", \"MSFT\", \"NVDA\"]\n",
    "download_stock_data(symbols, data_dir='../data')\n",
    "logging.info(f\"Downloading data for {symbols}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Training-the-Final-Model\"></a>\n",
    "## **10. Training the Final Model**\n",
    "\n",
    "After finding the best hyperparameters, we'll train the final model using these settings and save it for future use.\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`main_training_loop`:** This function encapsulates the entire training process, from data loading to hyperparameter optimization and final model training.\n",
    "- **Process Flow:**\n",
    "  1. **Data Loading and Preprocessing:** Loads the stock data, normalizes it, and creates input sequences.\n",
    "  2. **Data Splitting:** Divides the data into training and validation sets.\n",
    "  3. **Hyperparameter Optimization:** Uses Optuna to find the best hyperparameters by minimizing validation loss.\n",
    "  4. **Final Model Training:** Trains the model using the best hyperparameters found.\n",
    "  5. **Model Saving:** Saves the trained model for future use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop():\n",
    "    \"\"\"\n",
    "    Executes the main training loop: hyperparameter optimization, training the final model, and saving it.\n",
    "    \n",
    "    Returns:\n",
    "        best_hyperparams (dict): The best hyperparameters found by Optuna.\n",
    "        best_model (nn.Module): The trained model with the best hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define parameters\n",
    "    symbol = \"AAPL\"\n",
    "    data_file_path = f\"data/{symbol}_stock_data.parquet\"\n",
    "    target_column = f\"Adj_Close_{symbol}\"\n",
    "    sequence_length = 60  # Number of past days to use for each prediction\n",
    "    pred_steps = 5        # Number of future days to predict\n",
    "\n",
    "    # Load the stock data\n",
    "    df = load_data(data_file_path)\n",
    "\n",
    "    # Preprocess the data\n",
    "    data_normalized, scaler = preprocess_data(df, target_column)\n",
    "\n",
    "    # Create sequences and labels\n",
    "    sequences, labels = create_sequences(data_normalized, sequence_length, pred_steps)\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = split_data(sequences, labels, test_size=0.2)\n",
    "\n",
    "    # Define the objective function with additional arguments using partial\n",
    "    study_objective = partial(objective, pred_steps=pred_steps, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "    # Create the Optuna study\n",
    "    study = optuna.create_study(direction='minimize', study_name='LSTM_Hyperparameter_Optimization')\n",
    "\n",
    "    # Run the hyperparameter optimization\n",
    "    logging.info(\"Starting hyperparameter optimization with Optuna.\")\n",
    "    study.optimize(study_objective, n_trials=20, timeout=None)  # Adjust n_trials as needed\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_hyperparams = study.best_params\n",
    "    logging.info(\"Best hyperparameters found:\")\n",
    "    for key, value in best_hyperparams.items():\n",
    "        logging.info(f\"{key}: {value}\")\n",
    "\n",
    "    # Instantiate the best model\n",
    "    best_model = SequentialLSTM(\n",
    "        input_size=1,\n",
    "        hidden_size=best_hyperparams['hidden_size'],\n",
    "        num_layers=best_hyperparams['num_layers'],\n",
    "        output_size=1,\n",
    "        pred_steps=pred_steps,\n",
    "        dropout=best_hyperparams['dropout']\n",
    "    )\n",
    "\n",
    "    # Train the best model with the best hyperparameters\n",
    "    logging.info(\"Training the best model with the best hyperparameters.\")\n",
    "    best_model = train_model_with_early_stopping(\n",
    "        best_model, X_train, y_train, X_val, y_val,\n",
    "        epochs=100,\n",
    "        batch_size=best_hyperparams['batch_size'],\n",
    "        learning_rate=best_hyperparams['learning_rate'],\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Save the best model\n",
    "    save_checkpoint(best_model, filepath=\"best_model.pth\")\n",
    "\n",
    "    return best_hyperparams, best_model, scaler, sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Model-Evaluation-and-Inference\"></a>\n",
    "## **11. Model Evaluation and Inference**\n",
    "\n",
    "With the trained model, we can now evaluate its performance on the validation set and make future predictions.\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`perform_inference`:** Uses the trained model to predict future stock prices based on the latest available data. It processes the last input sequence, makes predictions, inversely transforms the normalized predictions to their original scale, and logs the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, scaler, sequence_length, pred_steps, data_normalized):\n",
    "    \"\"\"\n",
    "    Performs inference to predict future stock prices.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model.\n",
    "        scaler (MinMaxScaler): Scaler used for data normalization.\n",
    "        sequence_length (int): Length of input sequences.\n",
    "        pred_steps (int): Number of future steps to predict.\n",
    "        data_normalized (np.ndarray): Normalized stock price data.\n",
    "    \n",
    "    Returns:\n",
    "        future_prices (np.ndarray): Predicted future stock prices.\n",
    "    \"\"\"\n",
    "    device = check_gpu()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare the last sequence from the data for inference\n",
    "    last_sequence = data_normalized[-sequence_length:]\n",
    "    last_sequence = last_sequence.reshape(1, sequence_length, 1)  # Shape: (1, sequence_length, input_size)\n",
    "    last_sequence_tensor = torch.tensor(last_sequence, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Predict future prices\n",
    "    logging.info(f\"Predicting the next {pred_steps} days of stock prices.\")\n",
    "    with torch.no_grad():\n",
    "        future_predictions = model(last_sequence_tensor)  # Shape: (1, pred_steps, output_size)\n",
    "        future_predictions = future_predictions.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    # Inverse transform to get actual prices\n",
    "    future_prices = scaler.inverse_transform(future_predictions).flatten()\n",
    "\n",
    "    # Print predicted future prices\n",
    "    logging.info(f\"Predicted prices for the next {pred_steps} days:\")\n",
    "    for i, price in enumerate(future_prices, 1):\n",
    "        logging.info(f\"Day {i}: ${price:.2f}\")\n",
    "\n",
    "    return future_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Visualization\"></a>\n",
    "## **12. Visualization**\n",
    "\n",
    "Visualizing data and model predictions is vital for understanding performance and trends. Here, we'll plot the actual vs. predicted prices and visualize the stock price trends.\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **`plot_final_predictions`:** Combines the last 60 days of actual stock prices with the predicted future prices and plots them for visual comparison. This helps in assessing how well the model is predicting future trends based on recent data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_predictions(last_60_days, future_prices, symbol, sequence_length, pred_steps):\n",
    "    \"\"\"\n",
    "    Plots the last 60 days of actual stock prices and the predicted next days.\n",
    "    \n",
    "    Args:\n",
    "        last_60_days (np.ndarray): Array of the last 60 days of actual prices.\n",
    "        future_prices (np.ndarray): Array of predicted future prices.\n",
    "        symbol (str): Stock symbol.\n",
    "        sequence_length (int): Number of past days used for prediction.\n",
    "        pred_steps (int): Number of future days predicted.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Plotting the final predictions.\")\n",
    "    plot_prices = np.concatenate((last_60_days, future_prices))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(sequence_length), last_60_days, label='Last 60 Days')\n",
    "    plt.plot(range(sequence_length, sequence_length + pred_steps), future_prices, label='Predicted Next Days', linestyle='--', marker='o')\n",
    "    plt.title(f\"{symbol} Last {sequence_length} Days and Predicted Next {pred_steps} Days\")\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Conclusion\"></a>\n",
    "## **13. Conclusion**\n",
    "\n",
    "Congratulations! You've successfully built and trained an LSTM model for stock price prediction, optimized its hyperparameters using Optuna, and visualized the results. Here's a recap of what we've accomplished:\n",
    "\n",
    "1. **Data Preprocessing:** Loaded, normalized, and created sequences from stock price data.\n",
    "2. **Model Definition:** Defined an LSTM architecture suitable for multi-step predictions.\n",
    "3. **Training with Early Stopping:** Implemented training loops that halt when the model stops improving, preventing overfitting.\n",
    "4. **Hyperparameter Optimization:** Leveraged Optuna to find the best set of hyperparameters, enhancing model performance.\n",
    "5. **Evaluation and Inference:** Assessed the model's performance on validation data and made future price predictions.\n",
    "6. **Visualization:** Plotted actual vs. predicted prices to intuitively understand model accuracy.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- **Expand the Dataset:** Incorporate more features (e.g., volume, technical indicators) to potentially improve prediction accuracy.\n",
    "- **Experiment with Different Models:** Explore other architectures like GRUs or Transformer-based models.\n",
    "- **Deploy the Model:** Integrate the trained model into a production environment for real-time predictions.\n",
    "- **Continuous Learning:** Implement mechanisms to update the model with new data over time.\n",
    "\n",
    "Feel free to experiment further and adapt the pipeline to suit your specific requirements. Happy forecasting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Appendix: Complete Code Snippets**\n",
    "\n",
    "For convenience, below are the complete code snippets for each section. You can copy and paste them into separate cells in your Jupyter Notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): The seed value to set for random number generators.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Setting random seed: {seed}\")\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Function to download stock data for a list of symbols\n",
    "def download_stock_data(symbols, start_date=\"2015-01-01\", end_date=None, data_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Downloads stock data for the given symbols and saves them as parquet files.\n",
    "\n",
    "    Args:\n",
    "        symbols (list): List of stock symbols to download.\n",
    "        start_date (str): Start date for the data in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date for the data in 'YYYY-MM-DD' format. If None, defaults to today's date.\n",
    "        data_dir (str): Directory to save the downloaded data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting stock data download.\")\n",
    "    ensure_dir_exists(data_dir)\n",
    "    \n",
    "    if end_date is None:\n",
    "        end_date = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        logging.info(f\"Downloading data for {symbol} from {start_date} to {end_date}.\")\n",
    "        try:\n",
    "            stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "            \n",
    "            # Flatten MultiIndex if present and clean column names\n",
    "            if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "                stock_data.columns = ['_'.join(col).strip().replace(' ', '_') for col in stock_data.columns]\n",
    "\n",
    "            # Save data to Parquet format for efficient storage\n",
    "            file_path = os.path.join(data_dir, f\"{symbol}_stock_data.parquet\")\n",
    "            stock_data.to_parquet(file_path)\n",
    "            logging.info(f\"Data for {symbol} saved to {file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error downloading data for {symbol}: {e}\")\n",
    "\n",
    "def ensure_dir_exists(dir_path: str):\n",
    "    \"\"\"\n",
    "    Ensures that the specified directory exists; creates it if it does not.\n",
    "    \n",
    "    Args:\n",
    "        dir_path (str): Path to the directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        logging.info(f\"Directory created: {dir_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Directory already exists: {dir_path}\")\n",
    "\n",
    "# Check if GPU is available\n",
    "def check_gpu():\n",
    "    \"\"\"\n",
    "    Checks if a GPU is available and returns the appropriate device.\n",
    "\n",
    "    Returns:\n",
    "        device (torch.device): 'cuda' if GPU is available, else 'cpu'.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logging.info(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logging.info(\"No GPU detected. Using CPU.\")\n",
    "    \n",
    "    logging.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "    logging.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    return device\n",
    "\n",
    "# Load data function\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads stock data from a parquet file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the parquet file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): DataFrame containing the stock data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data from {filepath}\")   \n",
    "    try:\n",
    "        df = pd.read_parquet(filepath)\n",
    "        logging.info(f\"Data loaded successfully from {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {filepath}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column=\"Adj_Close\", feature_columns=None, scaler=None):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by selecting the target column and normalizing it.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the stock data.\n",
    "        target_column (str, optional): The name of the target column. Defaults to \"Adj_Close\".\n",
    "        feature_columns (_type_, optional): _description_. Defaults to None.\n",
    "        scaler (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        data_normalized (np.ndarray): Normalized data.\n",
    "        scaler (MinMaxScaler): Scaler used for normalization.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting data preprocessing.\")\n",
    "    if target_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{target_column}' not found in DataFrame.\")\n",
    "\n",
    "    # Select relevant features for LSTM\n",
    "    feature_columns = feature_columns or [target_column]\n",
    "    data = df[feature_columns].values\n",
    "\n",
    "    # Handle missing values (forward and backward fill)\n",
    "    data = pd.DataFrame(data).fillna(method='ffill').fillna(method='bfill').values\n",
    "\n",
    "    # Initialize scaler if none provided\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # Normalize data to the range [0, 1]\n",
    "    data_normalized = scaler.fit_transform(data)\n",
    "    \n",
    "    logging.info(f\"Data normalization complete. Shape: {data_normalized.shape}\")\n",
    "    return data_normalized, scaler\n",
    "\n",
    "def create_sequences(data, sequence_length, pred_steps):\n",
    "    \"\"\"\n",
    "    Creates input sequences and corresponding labels for multi-step prediction.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The normalized data array.\n",
    "        sequence_length (int): Length of each input sequence.\n",
    "        pred_steps (int): Number of future steps to predict.\n",
    "\n",
    "    Returns:\n",
    "        sequences (np.ndarray): Array of input sequences.\n",
    "        labels (np.ndarray): Array of corresponding labels (future values).\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating sequences.\")\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length - pred_steps + 1):\n",
    "        seq = data[i:i + sequence_length]\n",
    "        label = data[i + sequence_length:i + sequence_length + pred_steps]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "    logging.info(f\"Sequences created. Number of sequences: {len(sequences)}, Sequence length: {sequence_length}\")\n",
    "    return sequences, labels\n",
    "\n",
    "def split_data(sequences, labels, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the sequences and labels into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        sequences (np.ndarray): Input sequences.\n",
    "        labels (np.ndarray): Corresponding labels.\n",
    "        test_size (float): Proportion of the dataset to include in the validation split.\n",
    "\n",
    "    Returns:\n",
    "        X_train (np.ndarray): Training sequences.\n",
    "        X_val (np.ndarray): Validation sequences.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Splitting data with test size {test_size}.\")\n",
    "    split_index = int(len(sequences) * (1 - test_size))\n",
    "    X_train = sequences[:split_index]\n",
    "    y_train = labels[:split_index]\n",
    "    X_val = sequences[split_index:]\n",
    "    y_val = labels[split_index:]\n",
    "    logging.info(f\"Data split into {len(X_train)} training and {len(X_val)} validation samples.\")\n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for sequential data.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Number of input features.\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "        num_layers (int): Number of stacked LSTM layers.\n",
    "        output_size (int): Number of output features.\n",
    "        pred_steps (int): Number of future steps to predict.\n",
    "        dropout (float): Dropout rate to prevent overfitting.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, pred_steps, dropout=0.2):\n",
    "        super(SequentialLSTM, self).__init__()\n",
    "        self.pred_steps = pred_steps\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size * pred_steps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): Output tensor of shape (batch_size, pred_steps, output_size).\n",
    "        \"\"\"\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        out = output[:, -1, :]  # Take the output of the last time step\n",
    "        out = self.fc(out)      # Linear layer\n",
    "        out = out.view(-1, self.pred_steps, 1)  # Reshape for multi-step prediction\n",
    "        return out\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation loss doesn't improve.\n",
    "    \n",
    "    Args:\n",
    "        patience (int): Number of epochs to wait before stopping.\n",
    "        min_delta (float): Minimum change to qualify as an improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            logging.debug(f\"Initial validation loss set to {val_loss:.6f}.\")\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            logging.debug(f\"No improvement in validation loss for {self.counter} epochs.\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                logging.info(\"Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            logging.debug(f\"Validation loss improved to {val_loss:.6f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training and Evaluation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(\n",
    "    model, X_train, y_train, X_val, y_val,\n",
    "    epochs=500, batch_size=32, learning_rate=0.001, patience=5, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        X_train (np.ndarray): Training input data.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        X_val (np.ndarray): Validation input data.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "        epochs (int): Maximum number of epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        patience (int): Patience for early stopping.\n",
    "        verbose (bool): Whether to print training progress.\n",
    "    \n",
    "    Returns:\n",
    "        model (nn.Module): The trained model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logging.info(f\"Training on device: {device}\")\n",
    "\n",
    "    # Convert data to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            inputs = X_train_tensor[i:i + batch_size].to(device)\n",
    "            targets = y_train_tensor[i:i + batch_size].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val_tensor.to(device)).item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = epoch_loss / len(X_train_tensor)\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, filepath=\"best_model_temp.pth\")\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            if verbose:\n",
    "                logging.info(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(torch.load(\"best_model_temp.pth\"))\n",
    "    logging.info(\"Loaded the best model weights.\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model.\n",
    "        X_val (np.ndarray): Validation input data.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "    \n",
    "    Returns:\n",
    "        val_loss (float): Validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "    logging.debug(f\"Validation loss: {val_loss:.6f}\")\n",
    "    return val_loss\n",
    "\n",
    "def save_checkpoint(model, filepath=\"best_model.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the model's state dictionary to a file.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to save.\n",
    "        filepath (str): Path to the file where the model will be saved.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    logging.info(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model: nn.Module, load_path: str, device: torch.device = torch.device(\"cpu\")) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Loads the model state dictionary from a file.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model instance to load the state into.\n",
    "        load_path (str): Path to the saved model file.\n",
    "        device (torch.device): Device to map the model to.\n",
    "    \n",
    "    Returns:\n",
    "        model (nn.Module): The model with loaded state dictionary.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logging.info(f\"Model loaded from {load_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Optimization with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, pred_steps, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna trial object.\n",
    "        pred_steps (int): Number of future steps to predict.\n",
    "        X_train (np.ndarray): Training input data.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        X_val (np.ndarray): Validation input data.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "    \n",
    "    Returns:\n",
    "        val_loss (float): Validation loss after training.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout = trial.suggest_uniform('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    logging.info(f\"Trial {trial.number}: hidden_size={hidden_size}, num_layers={num_layers}, dropout={dropout}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = SequentialLSTM(\n",
    "        input_size=1,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        output_size=1,\n",
    "        pred_steps=pred_steps,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model_with_early_stopping(\n",
    "        model, X_train, y_train, X_val, y_val,\n",
    "        epochs=100,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        patience=5,\n",
    "        verbose=False  # Suppress output during optimization\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_loss = evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    logging.info(f\"Trial {trial.number}: Validation Loss={val_loss:.6f}\")\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Final Model Training and Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, scaler, sequence_length, pred_steps, data_normalized):\n",
    "    \"\"\"\n",
    "    Performs inference to predict future stock prices.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model.\n",
    "        scaler (MinMaxScaler): Scaler used for data normalization.\n",
    "        sequence_length (int): Length of input sequences.\n",
    "        pred_steps (int): Number of future steps to predict.\n",
    "        data_normalized (np.ndarray): Normalized stock price data.\n",
    "    \n",
    "    Returns:\n",
    "        future_prices (np.ndarray): Predicted future stock prices.\n",
    "    \"\"\"\n",
    "    device = check_gpu()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare the last sequence from the data for inference\n",
    "    last_sequence = data_normalized[-sequence_length:]\n",
    "    last_sequence = last_sequence.reshape(1, sequence_length, 1)  # Shape: (1, sequence_length, input_size)\n",
    "    last_sequence_tensor = torch.tensor(last_sequence, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Predict future prices\n",
    "    logging.info(f\"Predicting the next {pred_steps} days of stock prices.\")\n",
    "    with torch.no_grad():\n",
    "        future_predictions = model(last_sequence_tensor)  # Shape: (1, pred_steps, output_size)\n",
    "        future_predictions = future_predictions.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    # Inverse transform to get actual prices\n",
    "    future_prices = scaler.inverse_transform(future_predictions).flatten()\n",
    "\n",
    "    # Print predicted future prices\n",
    "    logging.info(f\"Predicted prices for the next {pred_steps} days:\")\n",
    "    for i, price in enumerate(future_prices, 1):\n",
    "        logging.info(f\"Day {i}: ${price:.2f}\")\n",
    "\n",
    "    return future_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_predictions(last_60_days, future_prices, symbol, sequence_length, pred_steps):\n",
    "    \"\"\"\n",
    "    Plots the last 60 days of actual stock prices and the predicted next days.\n",
    "    \n",
    "    Args:\n",
    "        last_60_days (np.ndarray): Array of the last 60 days of actual prices.\n",
    "        future_prices (np.ndarray): Array of predicted future prices.\n",
    "        symbol (str): Stock symbol.\n",
    "        sequence_length (int): Number of past days used for prediction.\n",
    "        pred_steps (int): Number of future days predicted.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Plotting the final predictions.\")\n",
    "    plot_prices = np.concatenate((last_60_days, future_prices))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(sequence_length), last_60_days, label='Last 60 Days')\n",
    "    plt.plot(range(sequence_length, sequence_length + pred_steps), future_prices, label='Predicted Next Days', linestyle='--', marker='o')\n",
    "    plt.title(f\"{symbol} Last {sequence_length} Days and Predicted Next {pred_steps} Days\")\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Complete Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop():\n",
    "    \"\"\"\n",
    "    Executes the main training loop: hyperparameter optimization, training the final model, and saving it.\n",
    "    \n",
    "    Returns:\n",
    "        best_hyperparams (dict): The best hyperparameters found by Optuna.\n",
    "        best_model (nn.Module): The trained model with the best hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define parameters\n",
    "    symbol = \"AAPL\"\n",
    "    data_file_path = f\"../data/{symbol}_stock_data.parquet\"\n",
    "    target_column = f\"Adj_Close_{symbol}\"\n",
    "    sequence_length = 60  # Number of past days to use for each prediction\n",
    "    pred_steps = 5        # Number of future days to predict\n",
    "\n",
    "    # Load the stock data\n",
    "    df = load_data(data_file_path)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    data_normalized, scaler = preprocess_data(df, target_column)\n",
    "\n",
    "    # Create sequences and labels\n",
    "    sequences, labels = create_sequences(data_normalized, sequence_length, pred_steps)\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = split_data(sequences, labels, test_size=0.2)\n",
    "\n",
    "    # Define the objective function with additional arguments using partial\n",
    "    study_objective = partial(objective, pred_steps=pred_steps, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "    # Create the Optuna study\n",
    "    study = optuna.create_study(direction='minimize', study_name='LSTM_Hyperparameter_Optimization')\n",
    "\n",
    "    # Run the hyperparameter optimization\n",
    "    logging.info(\"Starting hyperparameter optimization with Optuna.\")\n",
    "    study.optimize(study_objective, n_trials=20, timeout=None)  # Adjust n_trials as needed\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_hyperparams = study.best_params\n",
    "    logging.info(\"Best hyperparameters found:\")\n",
    "    for key, value in best_hyperparams.items():\n",
    "        logging.info(f\"{key}: {value}\")\n",
    "\n",
    "    # Instantiate the best model\n",
    "    best_model = SequentialLSTM(\n",
    "        input_size=1,\n",
    "        hidden_size=best_hyperparams['hidden_size'],\n",
    "        num_layers=best_hyperparams['num_layers'],\n",
    "        output_size=1,\n",
    "        pred_steps=pred_steps,\n",
    "        dropout=best_hyperparams['dropout']\n",
    "    )\n",
    "\n",
    "    # Train the best model with the best hyperparameters\n",
    "    logging.info(\"Training the best model with the best hyperparameters.\")\n",
    "    best_model = train_model_with_early_stopping(\n",
    "        best_model, X_train, y_train, X_val, y_val,\n",
    "        epochs=100,\n",
    "        batch_size=best_hyperparams['batch_size'],\n",
    "        learning_rate=best_hyperparams['learning_rate'],\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Save the best model\n",
    "    save_checkpoint(best_model, filepath=\"best_model.pth\")\n",
    "\n",
    "    return best_hyperparams, best_model, scaler, sequence_length, pred_steps, data_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Running the Training Loop and Performing Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 20:21:27,048 - INFO - Loading data from ../data/AAPL_stock_data.parquet\n",
      "2024-11-28 20:21:27,052 - INFO - Data loaded successfully from ../data/AAPL_stock_data.parquet\n",
      "2024-11-28 20:21:27,052 - INFO - Starting data preprocessing.\n",
      "/tmp/ipykernel_1230896/3010830827.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = pd.DataFrame(data).fillna(method='ffill').fillna(method='bfill').values\n",
      "2024-11-28 20:21:27,054 - INFO - Data normalization complete. Shape: (2494, 1)\n",
      "2024-11-28 20:21:27,055 - INFO - Creating sequences.\n",
      "2024-11-28 20:21:27,058 - INFO - Sequences created. Number of sequences: 2430, Sequence length: 60\n",
      "2024-11-28 20:21:27,058 - INFO - Splitting data with test size 0.2.\n",
      "2024-11-28 20:21:27,059 - INFO - Data split into 1944 training and 486 validation samples.\n",
      "[I 2024-11-28 20:21:27,060] A new study created in memory with name: LSTM_Hyperparameter_Optimization\n",
      "2024-11-28 20:21:27,060 - INFO - Starting hyperparameter optimization with Optuna.\n",
      "/tmp/ipykernel_1230896/435873919.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.0, 0.5)\n",
      "/tmp/ipykernel_1230896/435873919.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "2024-11-28 20:21:27,062 - INFO - Trial 0: hidden_size=57, num_layers=2, dropout=0.16887262800514458, learning_rate=0.0010946805578140005, batch_size=16\n",
      "2024-11-28 20:21:27,080 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:27,346 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:28,446 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:29,918 - INFO - Early stopping triggered.\n",
      "/tmp/ipykernel_1230896/3808255889.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model_temp.pth\"))\n",
      "2024-11-28 20:21:29,920 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:29,922 - INFO - Trial 0: Validation Loss=0.023248\n",
      "[I 2024-11-28 20:21:29,922] Trial 0 finished with value: 0.023247644305229187 and parameters: {'hidden_size': 57, 'num_layers': 2, 'dropout': 0.16887262800514458, 'learning_rate': 0.0010946805578140005, 'batch_size': 16}. Best is trial 0 with value: 0.023247644305229187.\n",
      "2024-11-28 20:21:29,924 - INFO - Trial 1: hidden_size=111, num_layers=2, dropout=0.21948921339410943, learning_rate=0.000296686358246387, batch_size=32\n",
      "2024-11-28 20:21:29,926 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:30,078 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:30,617 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:30,751 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:31,402 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:31,404 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:31,406 - INFO - Trial 1: Validation Loss=0.002750\n",
      "[I 2024-11-28 20:21:31,407] Trial 1 finished with value: 0.002749714534729719 and parameters: {'hidden_size': 111, 'num_layers': 2, 'dropout': 0.21948921339410943, 'learning_rate': 0.000296686358246387, 'batch_size': 32}. Best is trial 1 with value: 0.002749714534729719.\n",
      "2024-11-28 20:21:31,409 - INFO - Trial 2: hidden_size=99, num_layers=3, dropout=0.12348312450287569, learning_rate=1.4987014944711178e-05, batch_size=32\n",
      "2024-11-28 20:21:31,412 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:31,597 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:31,781 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:31,979 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:32,217 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:32,455 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:32,693 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:32,931 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:33,171 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:33,412 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:33,649 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:33,889 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:34,071 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:34,244 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:34,414 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:34,559 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:34,731 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:34,901 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:35,073 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:35,242 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:35,390 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:35,550 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:35,706 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:35,861 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:36,019 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:36,177 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:36,943 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:36,946 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:36,948 - INFO - Trial 2: Validation Loss=0.004235\n",
      "[I 2024-11-28 20:21:36,949] Trial 2 finished with value: 0.004234988708049059 and parameters: {'hidden_size': 99, 'num_layers': 3, 'dropout': 0.12348312450287569, 'learning_rate': 1.4987014944711178e-05, 'batch_size': 32}. Best is trial 1 with value: 0.002749714534729719.\n",
      "2024-11-28 20:21:36,951 - INFO - Trial 3: hidden_size=88, num_layers=1, dropout=0.019003717403953235, learning_rate=3.42700382351602e-05, batch_size=64\n",
      "/home/nyx/.cache/pypoetry/virtualenvs/time-series-stock-prediction-oD0mulHE-py3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.019003717403953235 and num_layers=1\n",
      "  warnings.warn(\n",
      "2024-11-28 20:21:36,953 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:37,022 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,092 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,162 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,232 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,302 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,371 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,440 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,510 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,581 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,647 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,707 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,767 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,828 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,887 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,928 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:37,969 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,010 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,054 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,095 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,136 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,177 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,217 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,258 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,299 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,341 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,382 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,423 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,464 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,505 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,546 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,587 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,627 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,668 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,708 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,749 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,790 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,831 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,872 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,912 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,953 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:38,994 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:39,035 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:39,076 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:39,116 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:39,318 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:39,319 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:39,320 - INFO - Trial 3: Validation Loss=0.004098\n",
      "[I 2024-11-28 20:21:39,320] Trial 3 finished with value: 0.004098452161997557 and parameters: {'hidden_size': 88, 'num_layers': 1, 'dropout': 0.019003717403953235, 'learning_rate': 3.42700382351602e-05, 'batch_size': 64}. Best is trial 1 with value: 0.002749714534729719.\n",
      "2024-11-28 20:21:39,321 - INFO - Trial 4: hidden_size=128, num_layers=2, dropout=0.2315166070255631, learning_rate=7.201498658407104e-05, batch_size=16\n",
      "2024-11-28 20:21:39,323 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:39,511 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:40,236 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:40,573 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:41,483 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:41,484 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:41,486 - INFO - Trial 4: Validation Loss=0.001620\n",
      "[I 2024-11-28 20:21:41,487] Trial 4 finished with value: 0.0016201436519622803 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2315166070255631, 'learning_rate': 7.201498658407104e-05, 'batch_size': 16}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:41,488 - INFO - Trial 5: hidden_size=111, num_layers=2, dropout=0.15480400193859745, learning_rate=0.0018984727112100344, batch_size=32\n",
      "2024-11-28 20:21:41,489 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:41,611 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:41,978 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:42,474 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:42,476 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:42,477 - INFO - Trial 5: Validation Loss=0.037193\n",
      "[I 2024-11-28 20:21:42,478] Trial 5 finished with value: 0.037193480879068375 and parameters: {'hidden_size': 111, 'num_layers': 2, 'dropout': 0.15480400193859745, 'learning_rate': 0.0018984727112100344, 'batch_size': 32}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:42,479 - INFO - Trial 6: hidden_size=107, num_layers=3, dropout=0.23735884654798423, learning_rate=0.004290812400001628, batch_size=64\n",
      "2024-11-28 20:21:42,482 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:42,545 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:42,664 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,064 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:43,067 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:43,070 - INFO - Trial 6: Validation Loss=0.015232\n",
      "[I 2024-11-28 20:21:43,071] Trial 6 finished with value: 0.015231749974191189 and parameters: {'hidden_size': 107, 'num_layers': 3, 'dropout': 0.23735884654798423, 'learning_rate': 0.004290812400001628, 'batch_size': 64}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:43,072 - INFO - Trial 7: hidden_size=81, num_layers=2, dropout=0.29636605435733565, learning_rate=0.00010656612095583634, batch_size=64\n",
      "2024-11-28 20:21:43,075 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:43,153 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,232 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,308 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,386 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,463 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,540 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,616 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,693 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,770 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,847 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:43,924 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,001 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,078 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,410 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,524 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,741 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:44,742 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:44,743 - INFO - Trial 7: Validation Loss=0.003326\n",
      "[I 2024-11-28 20:21:44,743] Trial 7 finished with value: 0.00332562904804945 and parameters: {'hidden_size': 81, 'num_layers': 2, 'dropout': 0.29636605435733565, 'learning_rate': 0.00010656612095583634, 'batch_size': 64}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:44,744 - INFO - Trial 8: hidden_size=128, num_layers=1, dropout=0.25139566916545036, learning_rate=4.331410412955243e-05, batch_size=64\n",
      "/home/nyx/.cache/pypoetry/virtualenvs/time-series-stock-prediction-oD0mulHE-py3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25139566916545036 and num_layers=1\n",
      "  warnings.warn(\n",
      "2024-11-28 20:21:44,746 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:44,788 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,828 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,868 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,909 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,951 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:44,992 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,033 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,074 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,116 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,158 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,200 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,241 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,282 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,323 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,365 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,408 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,450 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,491 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,614 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,775 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:45,816 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:46,015 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:46,016 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:46,017 - INFO - Trial 8: Validation Loss=0.007963\n",
      "[I 2024-11-28 20:21:46,018] Trial 8 finished with value: 0.007962954230606556 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.25139566916545036, 'learning_rate': 4.331410412955243e-05, 'batch_size': 64}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:46,019 - INFO - Trial 9: hidden_size=70, num_layers=1, dropout=0.21061030117450097, learning_rate=1.0945082635792273e-05, batch_size=16\n",
      "/home/nyx/.cache/pypoetry/virtualenvs/time-series-stock-prediction-oD0mulHE-py3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.21061030117450097 and num_layers=1\n",
      "  warnings.warn(\n",
      "2024-11-28 20:21:46,020 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:46,171 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:46,323 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:46,474 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:46,624 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:46,773 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:46,922 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:47,072 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:47,222 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:47,369 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:47,520 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:47,671 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:47,823 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:47,976 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:48,129 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:48,281 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:48,434 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:48,590 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:48,744 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:48,898 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:49,053 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:49,207 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:49,361 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:49,514 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:49,669 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:49,822 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:49,977 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:50,130 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:50,283 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:50,436 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:50,586 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:50,738 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:50,892 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:51,046 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:51,200 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:51,353 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:52,118 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:52,119 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:52,120 - INFO - Trial 9: Validation Loss=0.008941\n",
      "[I 2024-11-28 20:21:52,120] Trial 9 finished with value: 0.008940626867115498 and parameters: {'hidden_size': 70, 'num_layers': 1, 'dropout': 0.21061030117450097, 'learning_rate': 1.0945082635792273e-05, 'batch_size': 16}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:52,129 - INFO - Trial 10: hidden_size=32, num_layers=3, dropout=0.455594853276087, learning_rate=0.00025947663553753885, batch_size=16\n",
      "2024-11-28 20:21:52,131 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:52,360 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:53,466 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:54,245 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:54,445 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:54,641 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:55,413 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:55,610 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:56,001 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:56,977 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:56,978 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:56,979 - INFO - Trial 10: Validation Loss=0.024228\n",
      "[I 2024-11-28 20:21:56,980] Trial 10 finished with value: 0.024227634072303772 and parameters: {'hidden_size': 32, 'num_layers': 3, 'dropout': 0.455594853276087, 'learning_rate': 0.00025947663553753885, 'batch_size': 16}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:56,989 - INFO - Trial 11: hidden_size=120, num_layers=2, dropout=0.35308477826442597, learning_rate=0.0003050762028566997, batch_size=32\n",
      "2024-11-28 20:21:56,991 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:57,088 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:57,464 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:57,559 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:58,029 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:21:58,030 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:21:58,032 - INFO - Trial 11: Validation Loss=0.004790\n",
      "[I 2024-11-28 20:21:58,032] Trial 11 finished with value: 0.004789991304278374 and parameters: {'hidden_size': 120, 'num_layers': 2, 'dropout': 0.35308477826442597, 'learning_rate': 0.0003050762028566997, 'batch_size': 32}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:21:58,042 - INFO - Trial 12: hidden_size=122, num_layers=2, dropout=0.357940223161944, learning_rate=0.00010053625310214483, batch_size=16\n",
      "2024-11-28 20:21:58,044 - INFO - Training on device: cuda\n",
      "2024-11-28 20:21:58,228 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:58,415 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:58,980 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:21:59,344 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:00,268 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:00,269 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:00,271 - INFO - Trial 12: Validation Loss=0.003803\n",
      "[I 2024-11-28 20:22:00,272] Trial 12 finished with value: 0.0038025483954697847 and parameters: {'hidden_size': 122, 'num_layers': 2, 'dropout': 0.357940223161944, 'learning_rate': 0.00010053625310214483, 'batch_size': 16}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:00,281 - INFO - Trial 13: hidden_size=97, num_layers=2, dropout=0.06272177123109324, learning_rate=0.0007821279938625644, batch_size=32\n",
      "2024-11-28 20:22:00,283 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:00,378 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:00,473 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:00,744 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:00,831 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:00,926 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:01,396 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:01,398 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:01,399 - INFO - Trial 13: Validation Loss=0.002044\n",
      "[I 2024-11-28 20:22:01,400] Trial 13 finished with value: 0.0020441878587007523 and parameters: {'hidden_size': 97, 'num_layers': 2, 'dropout': 0.06272177123109324, 'learning_rate': 0.0007821279938625644, 'batch_size': 32}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:01,409 - INFO - Trial 14: hidden_size=91, num_layers=1, dropout=0.004990770049016768, learning_rate=0.0008464424497159459, batch_size=32\n",
      "/home/nyx/.cache/pypoetry/virtualenvs/time-series-stock-prediction-oD0mulHE-py3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.004990770049016768 and num_layers=1\n",
      "  warnings.warn(\n",
      "2024-11-28 20:22:01,410 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:01,489 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:01,847 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:01,940 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:02,034 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:02,125 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:02,577 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:02,578 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:02,579 - INFO - Trial 14: Validation Loss=0.002310\n",
      "[I 2024-11-28 20:22:02,580] Trial 14 finished with value: 0.002310405485332012 and parameters: {'hidden_size': 91, 'num_layers': 1, 'dropout': 0.004990770049016768, 'learning_rate': 0.0008464424497159459, 'batch_size': 32}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:02,589 - INFO - Trial 15: hidden_size=65, num_layers=2, dropout=0.061368280990365974, learning_rate=0.0048943703065990355, batch_size=16\n",
      "2024-11-28 20:22:02,591 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:02,835 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:03,875 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:04,980 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:04,982 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:04,983 - INFO - Trial 15: Validation Loss=0.027222\n",
      "[I 2024-11-28 20:22:04,984] Trial 15 finished with value: 0.027222203090786934 and parameters: {'hidden_size': 65, 'num_layers': 2, 'dropout': 0.061368280990365974, 'learning_rate': 0.0048943703065990355, 'batch_size': 16}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:04,994 - INFO - Trial 16: hidden_size=99, num_layers=3, dropout=0.07941274376884128, learning_rate=0.0007237358446528869, batch_size=16\n",
      "2024-11-28 20:22:04,997 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:05,208 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:05,986 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:06,994 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:07,955 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:07,957 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:07,960 - INFO - Trial 16: Validation Loss=0.030781\n",
      "[I 2024-11-28 20:22:07,961] Trial 16 finished with value: 0.030781297013163567 and parameters: {'hidden_size': 99, 'num_layers': 3, 'dropout': 0.07941274376884128, 'learning_rate': 0.0007237358446528869, 'batch_size': 16}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:07,970 - INFO - Trial 17: hidden_size=44, num_layers=2, dropout=0.4320644566502481, learning_rate=9.59734019653593e-05, batch_size=32\n",
      "2024-11-28 20:22:07,972 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:08,066 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,160 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,254 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,348 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,442 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,536 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,630 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,723 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,816 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:08,911 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:09,371 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:09,373 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:09,374 - INFO - Trial 17: Validation Loss=0.012259\n",
      "[I 2024-11-28 20:22:09,375] Trial 17 finished with value: 0.012259399518370628 and parameters: {'hidden_size': 44, 'num_layers': 2, 'dropout': 0.4320644566502481, 'learning_rate': 9.59734019653593e-05, 'batch_size': 32}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:09,385 - INFO - Trial 18: hidden_size=76, num_layers=1, dropout=0.4976658999920652, learning_rate=0.00969664978231875, batch_size=32\n",
      "/home/nyx/.cache/pypoetry/virtualenvs/time-series-stock-prediction-oD0mulHE-py3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4976658999920652 and num_layers=1\n",
      "  warnings.warn(\n",
      "2024-11-28 20:22:09,387 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:09,464 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:09,693 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:10,072 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:10,074 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:10,075 - INFO - Trial 18: Validation Loss=0.016413\n",
      "[I 2024-11-28 20:22:10,076] Trial 18 finished with value: 0.0164125207811594 and parameters: {'hidden_size': 76, 'num_layers': 1, 'dropout': 0.4976658999920652, 'learning_rate': 0.00969664978231875, 'batch_size': 32}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:10,086 - INFO - Trial 19: hidden_size=102, num_layers=3, dropout=0.10040707355382467, learning_rate=4.3972062516767654e-05, batch_size=16\n",
      "2024-11-28 20:22:10,088 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:10,317 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:10,939 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:11,136 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:11,333 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:11,527 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:11,724 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:12,691 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:12,694 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:12,696 - INFO - Trial 19: Validation Loss=0.007153\n",
      "[I 2024-11-28 20:22:12,697] Trial 19 finished with value: 0.007152935490012169 and parameters: {'hidden_size': 102, 'num_layers': 3, 'dropout': 0.10040707355382467, 'learning_rate': 4.3972062516767654e-05, 'batch_size': 16}. Best is trial 4 with value: 0.0016201436519622803.\n",
      "2024-11-28 20:22:12,698 - INFO - Best hyperparameters found:\n",
      "2024-11-28 20:22:12,698 - INFO - hidden_size: 128\n",
      "2024-11-28 20:22:12,698 - INFO - num_layers: 2\n",
      "2024-11-28 20:22:12,699 - INFO - dropout: 0.2315166070255631\n",
      "2024-11-28 20:22:12,699 - INFO - learning_rate: 7.201498658407104e-05\n",
      "2024-11-28 20:22:12,700 - INFO - batch_size: 16\n",
      "2024-11-28 20:22:12,702 - INFO - Training the best model with the best hyperparameters.\n",
      "2024-11-28 20:22:12,703 - INFO - Training on device: cuda\n",
      "2024-11-28 20:22:12,890 - INFO - Epoch 1/100, Train Loss: 0.017440, Val Loss: 0.045274\n",
      "2024-11-28 20:22:12,893 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:13,081 - INFO - Epoch 2/100, Train Loss: 0.062510, Val Loss: 0.184195\n",
      "2024-11-28 20:22:13,272 - INFO - Epoch 3/100, Train Loss: 0.052136, Val Loss: 0.133038\n",
      "2024-11-28 20:22:13,459 - INFO - Epoch 4/100, Train Loss: 0.050412, Val Loss: 0.095876\n",
      "2024-11-28 20:22:13,645 - INFO - Epoch 5/100, Train Loss: 0.043740, Val Loss: 0.015033\n",
      "2024-11-28 20:22:13,648 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:13,837 - INFO - Epoch 6/100, Train Loss: 0.033886, Val Loss: 0.002317\n",
      "2024-11-28 20:22:13,839 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:14,026 - INFO - Epoch 7/100, Train Loss: 0.022178, Val Loss: 0.001438\n",
      "2024-11-28 20:22:14,029 - INFO - Model saved to best_model_temp.pth\n",
      "2024-11-28 20:22:14,216 - INFO - Epoch 8/100, Train Loss: 0.016712, Val Loss: 0.003309\n",
      "2024-11-28 20:22:14,402 - INFO - Epoch 9/100, Train Loss: 0.011419, Val Loss: 0.006309\n",
      "2024-11-28 20:22:14,593 - INFO - Epoch 10/100, Train Loss: 0.007294, Val Loss: 0.007498\n",
      "2024-11-28 20:22:14,780 - INFO - Epoch 11/100, Train Loss: 0.004190, Val Loss: 0.005874\n",
      "2024-11-28 20:22:14,956 - INFO - Epoch 12/100, Train Loss: 0.002307, Val Loss: 0.004604\n",
      "2024-11-28 20:22:14,957 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:14,957 - INFO - Early stopping triggered.\n",
      "2024-11-28 20:22:14,959 - INFO - Loaded the best model weights.\n",
      "2024-11-28 20:22:14,961 - INFO - Model saved to best_model.pth\n",
      "2024-11-28 20:22:14,962 - INFO - GPU detected: NVIDIA GeForce RTX 4090\n",
      "2024-11-28 20:22:14,963 - INFO - CUDA version: 12.4\n",
      "2024-11-28 20:22:14,963 - INFO - PyTorch version: 2.5.1+cu124\n",
      "2024-11-28 20:22:14,963 - INFO - Using device: cuda\n",
      "2024-11-28 20:22:14,965 - INFO - Predicting the next 5 days of stock prices.\n",
      "2024-11-28 20:22:14,966 - INFO - Predicted prices for the next 5 days:\n",
      "2024-11-28 20:22:14,966 - INFO - Day 1: $232.48\n",
      "2024-11-28 20:22:14,967 - INFO - Day 2: $237.23\n",
      "2024-11-28 20:22:14,967 - INFO - Day 3: $231.51\n",
      "2024-11-28 20:22:14,968 - INFO - Day 4: $234.57\n",
      "2024-11-28 20:22:14,968 - INFO - Day 5: $231.20\n"
     ]
    }
   ],
   "source": [
    "# Execute the main training loop\n",
    "best_hyperparams, best_model, scaler, sequence_length, pred_steps, data_normalized = main_training_loop()\n",
    "\n",
    "# Perform inference to predict future stock prices\n",
    "future_prices = perform_inference(best_model, scaler, sequence_length, pred_steps, data_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualization of Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 20:22:14,974 - INFO - Plotting the final predictions.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+0AAAIjCAYAAAB20vpjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADb/UlEQVR4nOzdd3gU5fbA8e/upvfeSCGEDqErAgqiCAiCHRWliFhRwa73dy3IVezXdsV6QQW7wkVABCmKNAHpvYYE0kjv2ezO74/JLAnpyW52k5zP8+QJuzs7826YbPbMOe95dYqiKAghhBBCCCGEEMLh6O09ACGEEEIIIYQQQlRPgnYhhBBCCCGEEMJBSdAuhBBCCCGEEEI4KAnahRBCCCGEEEIIByVBuxBCCCGEEEII4aAkaBdCCCGEEEIIIRyUBO1CCCGEEEIIIYSDkqBdCCGEEEIIIYRwUBK0CyGEEEIIIYQQDkqCdiGEEEI0mE6n44UXXrD3MOo0depU2rdvX+k+Rxt7dWMUQgghNBK0CyGEDXzwwQfodDoGDhxYr+2ffPJJdDodt9xyS7WPnzp1Cp1OZ/kyGAxER0dz/fXXs2vXrkrb6nQ6HnzwwQaP+YUXXkCn03Hu3LkGP7cxNm3axAsvvEB2dnaDnvftt98yaNAgPD098fPzY/Dgwaxdu7bKdp999hndunXDzc2NTp068d5779Vr/wsWLKj0s3ZzcyMiIoJRo0bx7rvvkpeX16DxtmUNOW8d3YEDB3jhhRc4deqU3cZw+eWXo9PpGDduXJXHtJ/1G2+8YbPjN/R3VntPufDLzc2tXs9v37695Tl6vR4/Pz/i4+O555572Lp1axNeiRBCtCxO9h6AEEK0RosWLaJ9+/b89ddfHDt2jI4dO9a4raIofP3117Rv356ff/6ZvLw8vL29q932tttuY8yYMZhMJg4ePMi8efP45Zdf2LJlC3369LHRq7GNTZs2MXv2bKZOnYqfn1+9nvPCCy/w4osvctNNNzF16lSMRiP79u3jzJkzlbb76KOPuO+++7jxxht59NFH2bBhAw8//DCFhYU89dRT9TrWiy++SGxsLEajkZSUFNavX8+sWbN46623WLp0Kb169WroS26zHO28LSoqwsmpYR+BDhw4wOzZs7n88svtnhVftmwZO3bsoH///s163Mb8zgLMmzcPLy8vy22DwVDv5/bp04fHHnsMgLy8PA4ePMj333/PJ598wiOPPMJbb71V730JIURLJUG7EEJY2cmTJ9m0aRM//fQT9957L4sWLeL555+vcfv169eTlJTE2rVrGTVqFD/99BNTpkypdtt+/fpxxx13WG4PGTKE8ePHM2/ePD766COrvxZHsmXLFl588UXefPNNHnnkkRq3Kyoq4v/+7/8YO3YsP/zwAwB33303ZrOZOXPmcM899+Dv71/n8a6++moGDBhguf3MM8+wdu1arrnmGsaPH8/Bgwdxd3dv+gtrAxpz3hYUFODp6WmT8dQ30+uIoqOjycvLY/bs2SxdutTew6mXm266iaCgoEY9t127dpXOHYBXX32ViRMn8u9//5tOnTpx//33W2OYQgjhsKQ8XgghrGzRokX4+/szduxYbrrpJhYtWlTn9t27d2f48OGMGDGizu0ruuKKKwD1QkFzyMzM5PHHHyc+Ph4vLy98fHy4+uqr2b17d5Vt33vvPXr06IGHhwf+/v4MGDCAr776ClAz5k888QQAsbGxlhLY2kqP3377bcLCwpg5cyaKopCfn1/tduvWrSMjI4MHHnig0v0zZsygoKCA5cuXN/LVqz/vZ599loSEBBYuXGi5f8+ePUydOpUOHTrg5uZGWFgY06ZNIyMjo9K4dDodixcvrrLfr776Cp1Ox+bNmwFISUnhzjvvJDIyEldXV8LDw7n22mvrLM2uzzjgfNnysWPHLFlTX19f7rzzTgoLCyttW1JSwiOPPEJwcDDe3t6MHz+epKSkhv7oKrnwvNWmJPz+++888MADhISEEBkZadn+l19+4bLLLsPT0xNvb2/Gjh3L/v37q+x3yZIl9OzZEzc3N3r27Fntzxqqn9N+5swZ7rrrLiIiInB1dSU2Npb777+f0tJSFixYwM033wzA8OHDLefr+vXrbTbGmnh7e/PII4/w888/8/fff9e5fXZ2NrNmzSIqKgpXV1c6duzIq6++itlsBtRKn+HDhxMcHExaWprleaWlpcTHxxMXF0dBQUGjfmc1iqKQm5uLoigNeq01cXd358svvyQgIICXXnqp0n7feOMNBg8eTGBgIO7u7vTv399y8U4zbNgwevfuXe2+u3TpwqhRoyy3v/nmG/r374+3tzc+Pj7Ex8fzzjvvWOV1CCFEfUnQLoQQVrZo0SJuuOEGXFxcuO222zh69Cjbtm2rdtuSkhJ+/PFHbrvtNkAtI167di0pKSn1Otbx48cBCAwMtM7g63DixAmWLFnCNddcw1tvvcUTTzzB3r17GTZsGGfPnrVs98knn/Dwww/TvXt33n77bWbPnk2fPn0s81BvuOEGy2v+97//zZdffsmXX35JcHBwjcdes2YNF110Ee+++64lgAwPD+f999+vtN3OnTsBKmXJAfr3749er7c83liTJk0CYNWqVZb7Vq9ezYkTJ7jzzjt57733uPXWW/nmm28YM2aMJaC4/PLLiYqKqvaizKJFi4iLi2PQoEEA3HjjjSxevJg777yTDz74gIcffpi8vDxOnz5d69jqM46KJkyYQF5eHnPnzmXChAksWLCA2bNnV9pm+vTpvP3224wcOZJXXnkFZ2dnxo4d27Af2gVqOm8feOABDhw4wHPPPcfTTz8NwJdffsnYsWPx8vLi1Vdf5dlnn+XAgQNceumllQLGVatWceONN6LT6Zg7dy7XXXcdd955J9u3b69zPGfPnuXiiy/mm2++4ZZbbuHdd99l0qRJ/P777xQWFjJ06FAefvhhAP7xj39Yztdu3bo12xgrmjlzJv7+/nU20yssLGTYsGEsXLiQyZMn8+677zJkyBCeeeYZHn30UUC9gPHf//6X4uJi7rvvPstzn3/+efbv38/8+fPx9PRs1O+spkOHDvj6+uLt7c0dd9xBampqg15vdby8vLj++us5c+YMBw4csNz/zjvv0LdvX1588UVefvllnJycuPnmmytdrJs0aRJ79uxh3759lfa5bds2jhw5Ysnsr169mttuuw1/f39effVVXnnlFS6//HI2btzY5PELIUSDKEIIIaxm+/btCqCsXr1aURRFMZvNSmRkpDJz5sxqt//hhx8UQDl69KiiKIqSm5uruLm5Kf/+978rbXfy5EkFUGbPnq2kp6crKSkpyvr165W+ffsqgPLjjz9atgWUGTNmNHjszz//vAIo6enpNW5TXFysmEymKmNzdXVVXnzxRct91157rdKjR49aj/f6668rgHLy5Mk6x5aZmakASmBgoOLl5aW8/vrryrfffquMHj1aAZQPP/zQsu2MGTMUg8FQ7X6Cg4OVW2+9tdZjzZ8/XwGUbdu21biNr6+v0rdvX8vtwsLCKtt8/fXXCqD88ccflvueeeYZxdXVVcnOzrbcl5aWpjg5OSnPP/+8oiiKkpWVpQDK66+/Xus4q1PfcWj/19OmTau07fXXX68EBgZabu/atUsBlAceeKDSdhMnTlQAy5hrUt/zVvuZX3rppUpZWZnl+Xl5eYqfn59y9913V9pvSkqK4uvrW+n+Pn36KOHh4ZV+tqtWrVIAJSYmptLzLxz75MmTFb1eX+3/udlsVhRFUb7//nsFUNatW1fpcVuNsTrDhg2z/F7Nnj1bAZQdO3YoinL+Z13xvJkzZ47i6empHDlypNJ+nn76acVgMCinT5+23PfRRx8pgLJw4UJly5YtisFgUGbNmlXpeQ35nVUURXn77beVBx98UFm0aJHyww8/KDNnzlScnJyUTp06KTk5OXU+PyYmRhk7dmyNj//73/9WAOV///uf5b4LfwdKS0uVnj17KldccYXlvuzsbMXNzU156qmnKm378MMPK56enkp+fr6iKIoyc+ZMxcfHp9I5KYQQ9iCZdiGEsKJFixYRGhrK8OHDASwd4b/55htMJlO12w8YMMDSqE4rq62pRP75558nODiYsLAwLr/8co4fP86rr77KDTfcYLsXVYGrqyt6vfqnw2QykZGRgZeXF126dKlUquvn50dSUlKNFQYNpZXCZ2Rk8Omnn/L4448zYcIEli9fTvfu3fnXv/5l2baoqAgXF5dq9+Pm5kZRUVGTx+Pl5VWpi3zFue3FxcWcO3eOSy65BKDSz2Xy5MmUlJRUKtf99ttvKSsrs2T33N3dcXFxYf369WRlZTVoXPUdh6ZiZhXgsssuIyMjg9zcXABWrFgBYMkya2bNmtWgcdX3vL377rsrNSlbvXo12dnZ3HbbbZw7d87yZTAYGDhwIOvWrQMgOTmZXbt2MWXKFHx9fS3Pv+qqq+jevXutYzObzSxZsoRx48ZVqc4A9Xe4Ns0xxupo2fYLKyMq+v7777nsssvw9/evNLYRI0ZgMpn4448/LNvec889jBo1ioceeohJkyYRFxfHyy+/3OBxXTjG9957j4kTJ3LjjTfy9ttv8/nnn3P06FE++OCDJu0bsDS3q+l3MSsri5ycHC677LJK57+vry/XXnstX3/9taUCxWQy8e2333LddddZein4+flRUFDA6tWrmzxWIYRoCgnahRDCSkwmE9988w3Dhw/n5MmTHDt2jGPHjjFw4EBSU1NZs2ZNpe2zs7NZsWIFw4YNs2x77NgxhgwZwvbt2zly5EiVY9xzzz2sXr2aNWvWsGPHDtLS0njyySeb6yViNpstzZ9cXV0JCgoiODiYPXv2kJOTY9nuqaeewsvLi4svvphOnToxY8aMJpWUah/EnZ2duemmmyz36/V6brnlFpKSkiyl4+7u7pSWlla7n+LiYqs0j8vPz6/U4T8zM5OZM2cSGhqKu7s7wcHBxMbGAlT6uXTt2pWLLrqo0kWZRYsWcckll1gu3Li6uvLqq6/yyy+/EBoaytChQ3nttdfqNWWivuPQREdHV7qtNejTLhYkJCSg1+uJi4urtF2XLl3qHEtF9T1vtbFqjh49Cqhz4IODgyt9rVq1yjIHOyEhAYBOnTpV2WddY01PTyc3N5eePXs26DU15xir4+vry6xZs1i6dGmNUz6OHj3KypUrq4xrxIgRAJXmsIO6TGJhYSFHjx5lwYIFNmm0OHHiRMLCwvjtt9+avC/tYl7F38Vly5ZxySWX4ObmRkBAAMHBwcybN6/K+T958mROnz7Nhg0bAPjtt99ITU21TH8BdbpG586dufrqq4mMjGTatGmsXLmyyeMWQoiGku7xQghhJWvXriU5OZlvvvmGb775psrjixYtYuTIkZbb33//PSUlJbz55pu8+eab1W5/YRatU6dOlg/c9vDyyy/z7LPPMm3aNObMmUNAQAB6vZ5Zs2ZZGlsBdOvWjcOHD7Ns2TJWrlzJjz/+yAcffMBzzz1Xa2awJgEBAbi5ueHn51dluaiQkBBADTSjo6MJDw/HZDKRlpZmeQzUxloZGRlEREQ08tWrkpKSyMnJqbSM34QJE9i0aRNPPPEEffr0wcvLC7PZzOjRoyv9XEANFmbOnElSUhIlJSVs2bKlyrz8WbNmMW7cOJYsWcKvv/7Ks88+y9y5c1m7di19+/atcWwNGQfUvPSWYqWGYZr6nrcXBonamL/88kvCwsKqbN/QZdtswZ5jnDlzJv/+97+ZPXs2b7/9drVju+qqq2q8sNe5c+dKt9evX09JSQkAe/futfRYsLaoqCgyMzObvB9tTrr2u7hhwwbGjx/P0KFD+eCDDwgPD8fZ2Zn58+dbmmBqRo0aRWhoKAsXLmTo0KEsXLiQsLCwSudpSEgIu3bt4tdff+WXX37hl19+Yf78+UyePJnPP/+8yeMXQoj6sv9fOyGEaCUWLVpESEgI//nPf6o89tNPP7F48WI+/PBDS2CyaNEievbsWe1ycB999BFfffVVowJcW/rhhx8YPnw4n332WaX7s7Ozqyzp5OnpyS233MItt9xCaWkpN9xwAy+99BLPPPMMbm5udZYdV6TX6+nTpw/btm2jtLS0Uvm71gBPa4ilrfu9fft2xowZY9lu+/btmM3mJq8L/uWXXwJYOkxnZWWxZs0aZs+ezXPPPWfZTsvAXujWW2/l0Ucf5euvv6aoqAhnZ2duueWWKtvFxcXx2GOP8dhjj3H06FH69OnDm2++WalrfUUNHUd9xMTEYDabOX78eKVs8OHDhxu9z4bQMvwhISG1Bv0xMTFA9a+1rrEGBwfj4+NTpSnZhWo6X5tjjDXRsu0vvPBCtctExsXFkZ+fX68LJsnJyTz00EOMHDkSFxcXHn/8cUaNGmUZN9Q9VaA+FEXh1KlTtV58qo/8/HwWL15MVFSUpSHgjz/+iJubG7/++iuurq6WbefPn1/l+QaDgYkTJ7JgwQJeffVVlixZUmV6BoCLiwvjxo1j3LhxmM1mHnjgAT766COeffbZShfuhBDClqQ8XgghrKCoqIiffvqJa665hptuuqnK14MPPkheXp5lXeXExET++OMPJkyYUO32d955J8eOHbN0W3cUBoOhShb2+++/58yZM5Xuu3CJMRcXF7p3746iKBiNRgDLvNHs7Ox6HfuWW27BZDJVynAVFxdblszTMuhXXHEFAQEBzJs3r9Lz582bh4eHR5M6n69du5Y5c+YQGxvL7bffDpzPVl/4c6ku8wkQFBTE1VdfzcKFC1m0aBGjR4+udMGjsLCQ4uLiSs+Ji4vD29vbkgWtTkPHUR9XX301AO+++67V9tkQo0aNwsfHh5dfftly3lSUnp4OQHh4OH369OHzzz+vVAa9evXqSp3Fq6PX67nuuuv4+eefq+3irv08azpfm2OMtZk1axZ+fn68+OKLVR6bMGECmzdv5tdff63yWHZ2NmVlZZbbd999N2azmc8++4yPP/4YJycn7rrrrkrnU0N/Z7XXXtG8efNIT09n9OjR9dpHdYqKipg0aRKZmZn83//9n+VigsFgQKfTVeofcurUKZYsWVLtfiZNmkRWVhb33nsv+fn5VdaDv/B9TK/X06tXL4BafxeFEMLaJNMuhBBWsHTpUvLy8hg/fny1j19yySUEBwezaNEibrnlFr766isURalx+zFjxuDk5MSiRYsYOHBgg8ezffv2Ss3ZNJdffjmXXnpprc9966238PDwqHSfXq/nH//4B9dccw0vvvgid955J4MHD2bv3r0sWrSIDh06VNp+5MiRhIWFMWTIEEJDQzl48CDvv/8+Y8eOtcw/7d+/PwD/93//x6233oqzszPjxo2zBAYXuvfee/n000+ZMWMGR44cITo6mi+//JKEhAR+/vlny3bu7u7MmTOHGTNmcPPNNzNq1Cg2bNjAwoULeemllwgICKj7B4i67vahQ4coKysjNTWVtWvXsnr1amJiYli6dClubm4A+Pj4WOadG41G2rVrx6pVqyxrkFdn8uTJlrn5c+bMqfTYkSNHuPLKK5kwYQLdu3fHycmJxYsXk5qayq233lrjPhszjrr06dOH2267jQ8++ICcnBwGDx7MmjVrOHbsWKP32RA+Pj7MmzePSZMm0a9fP2699VaCg4M5ffo0y5cvZ8iQIZapBXPnzmXs2LFceumlTJs2jczMTN577z169Ohhmftck5dffplVq1YxbNgw7rnnHrp160ZycjLff/89f/75J35+fvTp0weDwcCrr75KTk4Orq6uXHHFFYSEhDTLGGvi6+vLzJkzq63KeeKJJ1i6dCnXXHMNU6dOpX///hQUFLB3715++OEHTp06RVBQEPPnz2f58uUsWLCAyMhIAN577z3uuOMO5s2bxwMPPAA0/Hc2JiaGW265hfj4eNzc3Pjzzz/55ptv6NOnD/fee2+9Xt+ZM2cs1SX5+fkcOHCA77//npSUFB577LFK+xk7dixvvfUWo0ePZuLEiaSlpfGf//yHjh07smfPnir77tu3Lz179uT777+nW7du9OvXr9Lj06dPJzMzkyuuuILIyEgSEhJ477336NOnjyW7L4QQzcJebeuFEKI1GTdunOLm5qYUFBTUuM3UqVMVZ2dn5dy5c0p8fLwSHR1d6z4vv/xyJSQkRDEajdUu51QToMavOXPm1Pg8bRmw6r60JdSKi4uVxx57TAkPD1fc3d2VIUOGKJs3b1aGDRumDBs2zLKvjz76SBk6dKgSGBiouLq6KnFxccoTTzxRZZmnOXPmKO3atVP0en29lpJKTU1VpkyZogQEBCiurq7KwIEDlZUrV1a77ccff6x06dJFcXFxUeLi4pR///vfluW7aqMtP6Z9ubi4KGFhYcpVV12lvPPOO0pubm6V5yQlJSnXX3+94ufnp/j6+io333yzcvbs2RqXRSspKVH8/f0VX19fpaioqNJj586dU2bMmKF07dpV8fT0VHx9fZWBAwcq3333XZ1jr+84alreT3vtFf8fioqKlIcfflgJDAxUPD09lXHjximJiYkNWvKtrvO2rmX21q1bp4waNUrx9fVV3NzclLi4OGXq1KnK9u3bK233448/Kt26dVNcXV2V7t27Kz/99JMyZcqUOpd8UxRFSUhIUCZPnqwEBwcrrq6uSocOHZQZM2YoJSUllm0++eQTpUOHDorBYKiy/Ju1x1idiku+VZSVlaX4+vpW+7POy8tTnnnmGaVjx46Ki4uLEhQUpAwePFh54403lNLSUiUxMVHx9fVVxo0bV2W/119/veLp6amcOHHCcl9DfmenT5+udO/eXfH29lacnZ2Vjh07Kk899VS1v0PViYmJsfwe6nQ6xcfHR+nRo4dy9913K1u3bq32OZ999pnSqVMnxdXVVenatasyf/58y/lenddee00BlJdffrnKYz/88IMycuRIJSQkRHFxcVGio6OVe++9V0lOTq7X+IUQwlp0imLlbjNCCCGEqFVZWRkRERGMGzeuSn8AIUTzeeedd3jkkUc4depUldUUhBDCUcicdiGEEKKZLVmyhPT0dCZPnmzvoQjRZimKwmeffcawYcMkYBdCODSZ0y6EEEI0k61bt7Jnzx7mzJlD3759GTZsmL2HJESbU1BQwNKlS1m3bh179+7lf//7n72HJIQQtZKgXQghhGgm8+bNY+HChfTp04cFCxbYezhCtEnp6elMnDgRPz8//vGPf9TYEFQIIRyFzGkXQgghhBBCCCEclMxpF0IIIYQQQgghHJQE7UIIIYQQQgghhIOSOe2A2Wzm7NmzeHt7o9Pp7D0cIYQQQgghhBCtnKIo5OXlERERgV5fcz5dgnbg7NmzREVF2XsYQgghhBBCCCHamMTERCIjI2t8XIJ2wNvbG1B/WD4+PnYeTc2MRiOrVq1i5MiRODs723s4ws7kfBAVyfkgKpLzQVQk54OoSM4HUZGcD/aVm5tLVFSUJR6tiQTtYCmJ9/Hxcfig3cPDAx8fH/mlEnI+iErkfBAVyfkgKpLzQVQk54OoSM4Hx1DXFG1pRCeEEEIIIYQQQjgoCdqFEEIIIYQQQggHJUG7EEIIIYQQQgjhoGROez0oikJZWRkmk8mu4zAajTg5OVFcXGz3sQj7s+X5YDAYcHJykiUQhRBCCCGEsDMJ2utQWlpKcnIyhYWF9h4KiqIQFhZGYmKiBFPC5ueDh4cH4eHhuLi4WH3fQgghhBBCiPqRoL0WZrOZkydPYjAYiIiIwMXFxa7BstlsJj8/Hy8vL/R6mdnQ1tnqfFAUhdLSUtLT0zl58iSdOnWS800IIYQQQgg7kaC9FqWlpZjNZqKiovDw8LD3cDCbzZSWluLm5iZBlLDp+eDu7o6zszMJCQmWYwghhBBCCCGan0R+9SABsmiL5LwXQgghhBDC/uRTuRBCCCGEEEII4aAkaBdCCCGEEEIIIRyUBO1CCCGEEEIIIYSDkqC9lZo6dSrXXXedTfa9fv16dDod2dnZdW6rKApvvPEGnTt3xtXVlXbt2vHSSy9V2V+/fv1wdXWlY8eOLFiwoF7H1+l06PV6fH196du3L08++STJyclNeGVCCCGEEEII4Vike7ywqZkzZ7Jq1SreeOMN4uPjyczMJDMz0/L4yZMnGTt2LPfddx+LFi1izZo1TJ8+nfDwcEaNGlXrvg8fPoyPjw+5ubn8/fffvPbaa3z22WesX7+e+Ph4W780IYQQQgghhLA5ybQ3kKIoFJaW2eVLURSrvY633nqL+Ph4PD09iYqK4oEHHiA/P9/yeEJCAuPGjcPf3x9PT0969OjBihUrOHXqFMOHDwfA398fnU7H1KlTqz3GwYMHmTdvHv/73/8YP348sbGx9O/fn6uuusqyzYcffkhsbCxvvvkm3bp148EHH+Smm27i3//+d52vISQkhLCwMDp37sytt97Kxo0bCQ4O5v7777dss23bNq666iqCgoLw9fVl2LBh/P3335bHp02bxjXXXFNpv0ajkZCQED777DMAfvjhB+Lj43F3dycwMJARI0ZQUFBQ9w9ZCCGEEEKI1sZsgpMbYO8P6nezyd4javUk095ARUYT3Z/71S7H3vfCVXVvVE96vZ53332X2NhYTpw4wQMPPMCTTz7JBx98AMCMGTMoLS3ljz/+wNPTkwMHDuDl5UVUVBQ//vgjN954oyXT7e7uXu0xfv75Zzp06MCyZcsYPXo0iqIwYsQIXnvtNQICAgDYvHkzI0aMqPS8UaNGMWvWrAa/Jnd3d+677z4eeeQR0tLSCAkJIS8vjylTpvDee++hKApvvvkmY8aM4ejRo3h7ezN9+nSGDh1KcnIy4eHhACxbtozCwkJuueUWkpOTue2223jttde4/vrrycvLY8OGDVa9gCKEEEIIIUSLcGAprHwKcs+ev88nAka/Ct3H229crZwE7W1UxaC4ffv2/Otf/+K+++6zBO2nT5/mxhtvtJSZd+jQwbK9FnCHhITg5+dX4zFOnDhBQkIC33//PV988QUmk4lHHnmEm266ibVr1wKQkpJCaGhopeeFhoaSm5tLUVFRjRcEatK1a1cATp06RUhICFdccUWlxz/++GP8/Pz4/fffueaaaxg8eDBdunThyy+/5MknnwRg/vz53HzzzXh5eXHkyBHKysq44YYbiImJAZDSeyGEEEII0fYcWArfTQYuSF7lJqv3T/hCAncbkaC9gdydDRx4sfa51rbiatCRV2ydff3222/MnTuXQ4cOkZubS1lZGcXFxRQWFuLh4cHDDz/M/fffz6pVqxgxYgQ33ngjvXr1atAxzGYzJSUlfPHFF3Tu3BmAzz77jP79+3P48GG6dOlinRdTgZYB1+l0AKSmpvLPf/6T9evXk5aWhslkorCwkNOnT1ueM336dD7++GOefPJJUlNT+eWXXywXFXr37s2VV15JfHw8o0aNYuTIkdx00034+/tbfexCCCGEEEI4JLNJzbBfGLBD+X06WPk0dB0LekMzD671kzntDaTT6fBwcbLLlxaINtWpU6e45ppr6NWrFz/++CM7duzgP//5DwClpaWAGsieOHGCSZMmsXfvXgYMGMB7773XoOOEh4fj5ORkCdgBunXrBmAJmsPCwkhNTa30vNTU1FrL7mtz8OBBQK0eAJgyZQq7du3inXfeYdOmTezatYvAwEDL6wSYPHkyJ06cYPPmzSxcuJDY2Fguu+wyAAwGA6tXr+aXX36he/fuvPfee3Tp0oWTJ082eGxCCCGEEEK0SAmbKpfEV6FA7hl1O2F1ErS3QTt27MBsNvPmm29yySWX0LlzZ86erfpLGBUVxX333cdPP/3EY489xieffAKAi4sLACZT7U0nhgwZQllZGcePH7fcd+TIEQBLqfmgQYNYs2ZNpeetXr2aQYMGNfh1FRUV8fHHHzN06FCCg4MB2LhxIw8//DBjxoyhR48euLq6cu7cuUrPCwwM5LrrrmP+/PksWLCAO++8s9LjOp2OIUOGMHv2bHbu3ImLiwuLFy9u8PiEEEIIIYRokfJT696mIduJBpHy+FYsJyeHXbt2VbovMDCQjh07YjQaee+99xg3bhwbN27kww8/rLTdrFmzuPrqq+ncuTNZWVmsW7fOkiWPiYlBp9OxbNkyxowZg7u7O15eXlWOP2LECPr168e0adN4++23MZvNzJgxg6uuusqSfb/vvvt4//33efLJJ5k2bRpr167lu+++Y/ny5XW+vrS0NIqLi8nLy2PHjh289tprnDt3jp9++smyTadOnfjyyy8ZMGAAubm5PPHEE9Vm8KdPn84111yDyWRiypQplvu3bt3KmjVrGDlyJCEhIWzdupX09HTLz0IIIYQQQohWzyu07m0asp1oEMm0t2Lr16+nb9++lb5mz55N7969eeutt3j11Vfp2bMnixYtYu7cuZWeazKZmDFjBt26dWP06NF07tzZ0qSuXbt2zJ49m6effprQ0FAefPDBao+v1+v5+eefCQoKYujQoYwdO5Zu3brxzTffWLaJjY1l+fLlrF69mt69e/Pmm2/y6aef1rlGO0CXLl2IiIigf//+vPLKK4wYMYJ9+/bRvXt3yzafffYZWVlZ9OvXj0mTJvHwww8TEhJSZV8jRoywrA0fERFhud/Hx4c//viDMWPG0LlzZ/75z3/y5ptvcvXVV9c5PiGEEEIIIVqFmMFql3hqmq6rA5926nbC6nSKrF1Fbm4uvr6+5OTk4OPjY7m/uLiYkydPEhsbi5ubmx1HqDKbzeTm5uLj44NeL9dbrCk/P5927doxf/58brjhBnsPp15sfT442vkvamc0GlmxYgVjxozB2dnZ3sMRdibng6hIzgdRkZwPoqIGnQ81dY/XAnnpHt9gNcWhF5LIT7RpZrOZtLQ05syZg5+fH+PHyxuNEEIIIYQQVXQfD+PfrXq/T4QE7DYmc9pFm3b69GliY2OJjIxkwYIFODnJr4QQQgghhBDVCuxY+fbQJ+DyZ2SZNxuTCEW0ae3bt0dmiAghhBBCCFEPPhFw5XOw5kX1touXBOzNQMrjhRBCCCGEEELUzb89XPYYXPqoejsnya7DaSskaBdCCCGEEEIIUX++ker33DP2HUcbIeXxQgghhBBCCCHqdupP8AwG7zD1dk6ifcfTRkimXQghhBBCCCFE7RQFvr4N/nMxlJWo90l5fLOQTLsQQgghhBBCiNoVZkBJLqCD2KHQbzL4RILZDHrJBduSBO1CCCGEEEIIIWqXeUL97tMOPINg/Hv2HU8bIpdEmovZBCc3wN4f1O9mk71HZDVTp07luuuus9y+/PLLmTVrVrOPY/369eh0OrKzs5v92EIIIYQQQrRqWtAeEGvfcbRBErQ3hwNL4e2e8Pk18ONd6ve3e6r328jUqVPR6XTodDpcXFzo2LEjL774ImVlZTY7puann35izpw59dq2uQPt9u3bo9Pp2LJlS6X7Z82axeWXX26145w6dQqdTseuXbvqtZ325e3tTY8ePZgxYwZHjx612niEEEIIIYRoksyT6veADur30kJIPwK5Z+03pjZCgnZbO7AUvptc9WTOTVbvt2HgPnr0aJKTkzl69CiPPfYYL7zwAq+//nq125aWllrtuAEBAXh7e1ttf9bm5ubGU089Ze9hVPLbb7+RnJzM7t27efnllzl48CC9e/dmzZo19h6aEEIIIYQQFTLt5UH7qv+D/1wE2+fbb0xthATtjVVaUPOXsVjdxmyClU8BSjU7KL9v5VOVS+Vr2mcjuLq6EhYWRkxMDPfffz8jRoxg6VL1IoFW0v7SSy8RERFBly5dAEhMTGTChAn4+fkREBDAtddey6lTpyz7NJlMPProo/j5+REYGMiTTz6JolR+fReWx5eUlPDUU08RFRWFq6srHTt25LPPPuPUqVMMHz4cAH9/f3Q6HVOnTlV/dGYzc+fOJTY2Fnd3d3r37s0PP/xQ6TgrVqygc+fOuLu7M3z48ErjrM0999zDli1bWLFiRa3bffrpp3Tr1g03Nze6du3KBx98YHls2rRp9OrVi5IStXNmaWkpffv2ZfLkyQDExqplQ3379kWn09WZxQ8MDCQsLIwOHTpw7bXX8ttvvzFw4EDuuusuTCb1/Dh+/DjXXnstoaGheHl5MXDgQNavX2/Zx4svvkjPnj2r7LtPnz48++yzgFrZcPHFF+Pp6Ymfnx9DhgwhISGh9h+YEEIIIYQQFwbt2lrt0kHe5iRob6yXI2r++m6Suk3CpjrKRRT18YRN5+96O776fVqBu7t7pYz6mjVrOHz4MKtXr2bZsmUYjUZGjRqFt7c3GzZsYOPGjXh5eTF69GjL8958800WLFjAf//7X/78808yMzNZvHhxrcedPHkyX3/9Ne+++y4HDx7ko48+wsvLi6ioKH788UcADh8+THJyMu+88w4Ac+fO5YsvvuDDDz9k//79PPLII9xxxx38/vvvgHpx4YYbbmDcuHHs2rWL6dOn8/TTT9fr5xAbG8t9993HM888g9lsrnabRYsW8dxzz/HSSy9x8OBBXn75ZZ599lk+//xzAN59910KCgosx/y///s/srOzef/99wH466+/gPMZ9J9++qleY9Po9XpmzpxJQkICO3bsACA/P58xY8awZs0adu7cyahRo7jttts4ffo0oF5IOHjwINu2bbPsZ+fOnezZs4c777yTsrIyrrvuOoYNG8aePXvYvHkz99xzDzqdrkFjE0IIIYQQbdBlj8IV/4SIPuptHy1ol7XabU26x9tSfqp1t2skRVFYs2YNv/76Kw899JDlfk9PTz799FNcXFwAWLhwIWazmU8//dQSyM2fPx8/Pz/Wr1/PyJEjefvtt3nmmWe44YYbAPjwww/59ddfazz2kSNH+O6771i9ejUjRowAoEOHDpbHAwICAAgJCcHPzw9QM/Mvv/wyv/32G4MGDbI8588//+Sjjz5i2LBhzJs3j7i4ON58800AunTpwt69e3n11Vfr9TP55z//yfz581m0aBGTJk2q8vjzzz/Pm2++aXmdsbGxHDhwgI8++ogpU6bg5eXFwoULGTZsGN7e3rz99tusW7cOHx8fAIKDg4HzGfTG6Nq1K6DOe7/44ovp3bs3vXv3tjz+4osv8uOPP/Lzzz/z0EMPERkZyahRo5g/fz4XXXQRoP7/DRs2jA4dOpCZmUlOTg7XXHMNcXFxAHTr1q1RYxNCCACzWeFYej4dg73Q6+UCoBBCtGpdx6pfGsm0NxsJ2hvrH7Vk0HUG9btXaP32VXG7WXsbP6YLLFu2DC8vL4xGI2azmYkTJ/LCCy9YHo+Pj7cE7AC7d+/m2LFjVeajFxcXc/z4cXJyckhOTmbgwIGWx5ycnBgwYECVEnnNrl27MBgMDBs2rN7jPnbsGIWFhVx11VWV7tdK0AEOHjxYaRyAJcCvj+DgYB5//HGee+45brnllkqPFRQUcPz4ce666y7uvvtuy/1lZWX4+vpWOt7jjz/OnDlzeOqpp7j00kvrffz60H6m2gWU/Px8XnjhBZYvX05ycjJlZWUUFRVZMu0Ad999N9OmTeOtt95Cr9fz1Vdf8e9//xtQL5BMnTqVUaNGcdVVVzFixAgmTJhAeHi4VccthGg7PvvzJC+tOMgVXUP4z8R+uLsY7D0kIYQQzUUL2nPPylrtNiZBe2O5eNa9Tcxg8IlQm85VO69dpz4eM7hh+62n4cOHM2/ePFxcXIiIiMDJqfJ/t6dn5WPl5+fTv39/Fi1aVGVfWua4odzd3Rv8nPz8fACWL19Ou3btKj3m6uraqHFU59FHH+WDDz6oNFe94vE/+eSTKhcGDIbzH0jNZjMbN27EYDBw7Ngxq41Lc/DgQeD8/PjHH3+c1atX88Ybb9CxY0dcXV258cYbK015GDduHK6urixevBgXFxeMRiM33XST5fH58+fz8MMPs3LlSr799lv++c9/snr1ai655BKrj18I0boVG0189Ic6v3HtoTSm/PcvPp06AB83ZzuPTAghhNWdOwaZxyGkG/hFq/f5RAA6MJVA4TnwCrHrEFszuRxiS3oDjNbKtS8sGyy/PfoVdTsb8PT0pGPHjkRHR1cJ2KvTr18/jh49SkhICB07dqz05evri6+vL+Hh4WzdutXynLKyMsuc6+rEx8djNpstc9EvpGX6tWZrAN27d8fV1ZXTp09XGUdUVBSglnVr88Y1Fy7jVhcvLy+effZZXnrpJfLy8iz3h4aGEhERwYkTJ6ocXwugAV5//XUOHTrE77//zsqVK5k//3znzOpeV0OYzWbeffddYmNjLdUFGzduZOrUqVx//fXEx8cTFhZWKcsOauXDlClTmD9/PvPnz+fWW2+tcuGkb9++PPPMM2zatImePXvy1VdfNWqMQoi2bemus5zLLyHIywVvVyf+OpXJbR9v4Vx+ib2HJoQQwtoOLIavJsD6V87fZ3AG7/KKTZnXblMStNta9/Ew4QvwuaAE2SdCvb/7ePuMqxq33347QUFBXHvttWzYsIGTJ0+yfv16Hn74YZKS1LkqM2fO5JVXXmHJkiUcOnSIBx54oNY11tu3b8+UKVOYNm0aS5Yssezzu+++AyAmJgadTseyZctIT08nPz8fb29vHn/8cR555BE+//xzjh8/zt9//817771naQR33333cfToUZ544gkOHz7MV199xYIFCxr8mu+55x58fX2rBK6zZ89m7ty5vPvuuxw5coS9e/cyf/583nrrLUBt8Pbcc8/x6aefMmTIEN566y1mzpzJiRNq1ikkJAR3d3dWrlxJamoqOTk5tY4jIyODlJQUTpw4wdKlSxkxYgR//fUXn332mSW736lTJ3766Sd27drF7t27uf3226udljB9+nTWrl3LypUrmTZtmuX+kydP8swzz7B582YSEhJYtWoVR48elXntQogGUxSFTzao73d3X9aBr++5hEBPF/afzWXCh5s5k11k5xEKIYSwKssa7bGV7794Ogz/P/BsXFWuqB8J2ptD9/Ewax9MWQY3fqZ+n7XXoQJ2AA8PD/744w+io6O54YYb6NatG3fddRfFxcWWBmuPPfYYkyZNYsqUKQwaNAhvb2+uv/76Wvc7b948brrpJh544AG6du3K3XffTUGBuoxdu3btmD17Nk8//TShoaE8+OCDAMyZM4dnn32WuXPn0q1bN0aPHs3y5cstme7o6Gh+/PFHlixZQu/evfnwww95+eWXG/yanZ2dmTNnDsXFxZXunz59Op9++inz588nPj6eYcOGsWDBAmJjYykuLuaOO+5g6tSpjBs3DlCD/+HDhzNp0iRMJhNOTk68++67fPTRR0RERHDttdfWOo4RI0YQHh5OfHw8Tz/9NN26dWPPnj2WJfEA3nrrLfz9/Rk8eDDjxo1j1KhR9OrVq8q+OnXqxODBg+natWul8n4PDw8OHTrEjTfeSOfOnbnnnnuYMWMG9957b4N/bkKItm39kXSOpuXj5erEbQOj6dnOl+/vG0Q7P3dOnCvg5nmbOJ6eb+9hCiGEsBZtuTf/C4L2yx6DYU+eL5kXNqFTauog1obk5ubi6+tLTk6OJTgFtQHbyZMniY2Nxc3NzY4jVJnNZnJzc/Hx8UEvjR7avJrOB0VR6NSpEw888ACPPvpoo/fvaOe/qJ3RaGTFihWMGTMGZ2eZU9zW2fp8uP3TLWw8lsFdl8by7DXdLfefzS5i0mdbOZ5eQICnC19Mu5ie7Xxr2ZNoDvL+ICqS80FUVO/z4Y3O6opXd6+Ddv2ab4CtXE1x6IUk8hOiFUlPT+f9998nJSWFO++8097DEUK0QvvP5rDxWAYGvY47h7Sv9FiEnzvf3TuI+Ha+ZBaUctvHW9h6IsM+AxVCCGEdJfnnl6i+sDzeWAzpRyDFeitgiaokaBeiFQkJCeHFF1/k448/xt/f397DEUK0Qp9tUOc1Xt0zjEh/jyqPB3q58tXdAxkYG0BeSRmT//sXaw+lNvcwhRBCWEtW+Xx29wBwv+Dz5fE18J+L4OeZzT+uNkSCdiFaEUVRSE9PZ+LEifYeihCiFUrJKWbp7rOA2oCuJt5uznw+7WKu7BpCSZmZe77Ywf92nWmuYQohhLAmSxO6at73tbXac5KabzxtkATtQgghhKiXBZtOUWZWuDg2gN5RfrVu6+Zs4MNJ/bmuTwRlZoVZ3+7iyy0JzTNQIYQQ1hM5AG74BIZUk033KQ/a81OhTJb8tJW6F+8W1S6rJURrJ+e9EKKi/JIyFm1Vg+7asuwVORv0vDWhDz7uznyxOYFnl+wjp7CUGcM7otPpbDlcIYQQ1uITAb0mVP+YRwA4uUNZEeSeqT4bL5pMMu210DooFhYW2nkkQjQ/7byXzrJCCIDvtiWSV1xGhyBPruwaUu/n6fU6Zo/vwcNXdATgjVVHeHnFQbkwKIQQrYFOV6FEXqZB2Ypk2mthMBjw8/MjLS0NUNe5tmdmwGw2U1paSnFxsSz5Jmx2PiiKQmFhIWlpafj5+WEwGKy2byFEy1RmMvPfjeqcxmmXxqLXN+xvoU6n49GRXfBxd+Zfyw/yyYaTuLs48ehVnW0xXCGEENb095fgHQ7th4Cze9XHfSMh46jMa7chCdrrEBYWBmAJ3O1JURSKiopwd3eXskJh8/PBz8/Pcv4LIdq2X/enkpRVRICnCzf2i2z0fqZf1gG9TseLyw6wfM9ZCdqFEMLRGYtg6YPqv584XkPQ3k79LkG7zUjQXgedTkd4eDghISEYjUa7jsVoNPLHH38wdOhQKVkWNj0fnJ2dJcMuhADUC4QfbzgBwB2XxODu0rT3hhHdQnlx2QGSsoowm5UGZ+2FEEI0o6zyBqKuPuARWP02Xa8BvxiIHdp842pjJGivJ4PBYPcgxmAwUFZWhpubmwTtQs4HIUSz2J6Qxe7EbFyc9EweFNPk/YX7uWHQ6ygpM5OeX0Koj5sVRimEEMImMtWLtgTEqvPXq9PlavVL2IxMjBZCCCFEjT75Q/3AdkPfdgR5uTZ5f84GPeG+aqCemCmNXoUQwqFZgnbpCm9PErQLIYQQolonzxWw+mAqANMvi7XafqP8PQA4LUG7EEI4tiy1CWmtQbvZBOlH4PhakJVBbEKCdiGEEEJU679/nkRR4IquIXQM8bbafqMD1KA9MbPIavsUQghhA1qm3b+WC7emUvjPRfDl9VCc3SzDamskaBdCCCFEFVkFpXy/IxGwbpYdICpA7T6cmCWZdiGEcGj1KY93dgePIPXf0kHeJqQRnRBCCCGqWLglgWKjmR4RPgzqUEPH4EaKCpDyeCGEaBFu/AwyjkFoj9q3820HhefUoD0svnnG1obYNdM+d+5cLrroIry9vQkJCeG6667j8OHDlba59957iYuLw93dneDgYK699loOHTpUaRudTlfl65tvvmnOlyKEEEK0GsVGE59vVpf5ufuyDuhq6hjcSFrQniRBuxBCOLbIAdD7VnD3q3073yj1u2TabcKuQfvvv//OjBkz2LJlC6tXr8ZoNDJy5EgKCgos2/Tv35/58+dz8OBBfv31VxRFYeTIkZhMpkr7mj9/PsnJyZav6667rplfjRBCCNE6LN11lnP5JYT7ujG2V7jV9681okvOLaakzFTH1kII0fbkFhtJyipEaSmN3Xwj1e8StNuEXcvjV65cWen2ggULCAkJYceOHQwdOhSAe+65x/J4+/bt+de//kXv3r05deoUcXFxlsf8/PwICwtrnoELIYQQrZSiKHz6pzqH8c4h7XE2WP/6fpCXC+7OBoqMJs5mFxMb5Gn1YwghREtVVGriqrd+JzW3BF93Z3q286FnhC892vnSM8KH9oGe6PXWrYCqVsImSN0PUQMhvFft20rQblMONac9JycHgICAgGofLygoYP78+cTGxhIVFVXpsRkzZjB9+nQ6dOjAfffdx5133lljOV9JSQklJSWW27m5uQAYjUaMRqM1XopNaGNz5DGK5iPng6hIzgdRUVPOhz+OnuNIaj6ergZu6htus3Mq0t+No2kFnEzPJdLXxSbHECp5fxAVyfng+H4/nEZqrhqr5BQZ2Xgsg43HMiyPe7oY6BbuTY8IH3qE+9AjwpsOQZ44NeIia23ng37fYgzbPsZ0yYOYg7rVuh+dVzhOgDk7EZOcW/VW399DneIgNRdms5nx48eTnZ3Nn3/+WemxDz74gCeffJKCggK6dOnC8uXLK2XZ58yZwxVXXIGHhwerVq3i+eef57XXXuPhhx+u9lgvvPACs2fPrnL/V199hYeHh3VfmBBCCNGC/OeAniM5ei4PN3N9e7PNjvPJIT37svTcHGvi0jCH+CgihBAO4Zvjejan6RkcYmZImJnEfB1JBerXmUIwmqsmJp11ChGeEOmpcGmomQgrFDANPP4mYbm72RV1JwlBw2vd1qMklciszeS5tSPZ76KmH7yNKCwsZOLEieTk5ODj41Pjdg4TtN9///388ssv/Pnnn0RGRlZ6LCcnh7S0NJKTk3njjTc4c+YMGzduxM3Nrdp9Pffcc8yfP5/ExMRqH68u0x4VFcW5c+dq/WHZm9FoZPXq1Vx11VU4OzvbezjCzuR8EBXJ+SAqauz5cCA5l2s/2IJBr2PNI5fSzs/dZmOcs/wQX2w5zd2XtufJUZ1tdhwh7w+iMjkfHJvZrHDp67+Tnl/Kf6f047KOQZUeLzOZOXmukP3Juew/m8v+5DwOJOdSUHK+P8jCaQMYGFt95fKFajsfnOYNRJd5nLLbf0JpP7TpL05UkZubS1BQUJ1Bu0OUxz/44IMsW7aMP/74o0rADuDr64uvry+dOnXikksuwd/fn8WLF3PbbbdVu7+BAwcyZ84cSkpKcHV1rfK4q6trtfc7Ozu3iDevljJO0TzkfBAVyfkgKmro+fD5ZvVi99U9w2gfbNuL2O2DvAA4k1Ms52wzkfcHUZGcD45pV2I26fmleLk6MaRTCM5OhkqPOztD90hXukf6c3P5fWazQkJmIfvO5LDvbA69ogMa/H9b5XwwmyD7NABOwZ3VAwurq+//k12DdkVReOihh1i8eDHr168nNja2Xs9RFKVSpvxCu3btwt/fv9rAXAghhBBVpeQUs3T3WQDuGdrB5sfTln1LzCyy+bGEEKKl+O1AKgBDOwfhekHAXhO9XkdskCexQZ6M6x1hnYHkJIHZCAZX8GlXv+dknoDMkxDaE7xDrTMOAdg5aJ8xYwZfffUV//vf//D29iYlJQVQM+vu7u6cOHGCb7/9lpEjRxIcHExSUhKvvPIK7u7ujBkzBoCff/6Z1NRULrnkEtzc3Fi9ejUvv/wyjz/+uD1fmhBCCNGifL75FGVmhYtjA+gV6Wfz40UFqKX3p2WtdiGEsPjtoBq0j+hm56A3U11FBP/2oK9ng7v/PQQJf8KNn0H8TU0fg9mkdrDPTwWvUIgZDPr6XchobewatM+bNw+Ayy+/vNL98+fPZ+rUqbi5ubFhwwbefvttsrKyCA0NZejQoWzatImQkBBALSn4z3/+wyOPPIKiKHTs2JG33nqLu+++u7lfjhBCCNFiadmdKYPaN8vxtLXac4qM5BYb8XGT0kshRNuWmFnIoZQ89DoY3iXEvoPRgvaABlReWZZ9q76vWIMcWAorn4Lcs+fv84mA0a9C9/FN338LY/fy+NpERESwYsWKWrcZPXo0o0ePtuawhBBCiDYlq6CUo2n5AAyKC2yWY3q6OhHo6UJGQSmJmYX0iPBtluMKIYSjWlOeZR/QPgB/TzsvhdnzRgjtAYYGXFD1LS+jb+pa7QeWwneTgQtixdxk9f4JX7S5wL3hi/kJIYQQolXZkZAFQMcQLwKa8YPi+XntUiIvhBC/HUwD4Cp7l8YDuPtB9CXQrn/9n2PJtDchaDeb1Az7hQE7nL9v5dPqdm2IBO1CCCFEG7ctIROAATH+zXpcaUYnhBCq3GIjW09mAHBlNzuXxjeWb5T6PedM4/eRsKlySXwVCuSeUbdrQyRoF0IIIdq4HafUTPuA9vVb19daoqUZnRBCAPDHkXSMJoUOwZ50CPay72DMZlj9HGz7DIzF9X+eNea056dad7tWwiHWaRdCCCGEfRQbTexJygHgovbNnGkvb0aXmCVBuxCibdOagTpEaXx+Cmx8B/RO0G9K/Z+nLQ1XnA0leeDq3fBje9Xz9dd3u1ZCgnYhhBCiDdt7JodSk5lgb1eiy8vVm4tWHi+ZdiFEW2Y0mVl7SJ3PPqK7AwSjWud4v2gwNCBcdPOBK58H7zDQNXJptpjBapf43GSqn9euUx+PGdy4/bdQUh4vhBBCtGHbTqnz2S9q749Op2vWY2sXCZKyijCba19RRgghWqvtp7LILS7D38OZftHNW/FULcsa7bENf+5lj0KfieDSyIvAeoO6rFu1yv9GjX6lza3XLkG7EEII0YZtL5/P3j+meeezA4T7umHQ6ygtM5OeX9LsxxdCCEegLfU2vGsIBn3zXjytVmPWaLem7uNh/LtV7/cOa5PLvYEE7UIIIUSbZTYrbK+QaW9uTgY9EX5ugJTICyHaJkVRWH3QgeazQ9OC9rwUOLYGzu5s2hiCOoOLD/i1B8/ybvrj3m2TATtI0C6EEEK0WcfS88ktLsPDxUD3cB+7jMHSjE6CdiFEG3Q8PZ+EjEJcDHou6xxs7+GomhK07/oKFt4AWz9q2hiiL4GnE2D6aoi9TL0vdW/T9tmCSdAuhBBCtFHafPa+0X44GezzkSBamtEJIdqw1QfUBnSXxAXi5eoAPcIVBTJPqf9uTNBuWfYtqelj0evBKwTCe6u3k3c3fZ8tlAOcGUIIIYSwB20++wA7zGfXaB3kEzOL7DYGIYSwlzWW0vgQO4+kgge3qdl2//YNf6411mq/UHgf9fvZXdbbZwsjmXYhhBCijdIy7QPsMJ9dE+nvDkh5vBCi7cnIL2HHafXi6ZWOMp9dpwOfcGg/BJxcGv58LWjPPQtmc+PGkLwH3u4FP89Sb4f3Ur9nJ0BRVuP22cJJ0C6EEEK0QSk5xSRlFaHXQV87LjGklccnZknQLoRoW9YeSkNRoEeEDxF+7vYejnV4h4NOD6ZSKEhv3D6StqkBetYp9ba7//msfxstkZfyeCGEEKIN2p6gZtm7R/jYdR6lVh6fkltMSZkJV6e2tfauEKLt+q28NN5hsuwA+35UM92dR0PMoIY/3+CsBu65Z9R57d6NeG1ndqjfIwecv6/fZDAWgU9kw/fXCkjQLoQQQrRBjjCfHSDQ0wUPFwOFpSbOZBXRIdjLruMRQojmUGw0seHoOcCBlnoDOLQC9v0AHgGNC9oBfNqVB+2JENm/4c9P2qZ+j7zo/H2XPda4sbQSErQLIYQQbdA2y/rs9g3adTodUf4eHE7NI1GCdiFEG7H5RAaFpSZCfVzp2c4+S25WqynLvWkGPwilBdCuEQF7URacO6L+uzHPb6UkaBdCCCHamLxiIweTcwH7NqHTRAWoQbss+yaEaCt+O6CWxo/oFopOp7PzaCqwRtDe/drGP1crjfePBc+gyo/lp6kd5GMGg2vbusArjeiEEEKINmbn6WzMCkQFuBPq42bv4RAVoDZgSpKgXQjRBiiKwpqD6vrsIxypNL4wE4qz1X83Zrk3a0jS5rNfVPWxT66Ar26G5F3NOiRHIEG7EEII0cZs10rj7TyfXRPlrzajk0y7EKIt2H82l5TcYtydDQyKC7T3cM7LOql+9w4HF8/G76c4F46tgYPLGv5cjwAIi4fogVUfC++tfm+D67VLebwQQgjRxmxPKG9CZ+f57BpZ9k0I0ZasLi+NH9o5CDdnB1oxI7M8aPePbdp+sk7BwhvAMwS6XdOw5158t/pVnfA+cGhZm1z2TTLtQgghRDM4nJJHz+d/5a3VR+w6DqPJzM7T2QBc5ADz2eH8sm+JmUV2HokQQtiettSbQ5XGw/mgvSnz2QF8y5dlK0gDY3HT9lVRRB/1u5THCyGEEMIWFu88Q35JGQs2nsRoMtttHAfO5lJkNOHn4Uycg3Rq1+a05xQZySky2nk0QghhO2ezi9h/NhedDoZ3DbH3cCq77DF45ABc/nTT9uPuD87qxVhyz9T/eYWZUFZS8+Naefy5o1CS3/jxtUAStAshhBDNYNNxdT3e3OIyNh/PsNs4tKXe+kf7o9c7RsdiDxcngrxcAEiUee1CiFZszSG1AV2/aH+CvFztPJoL6PXg2w78opq2H53ufLa9IUH7updgbiRs/k/1j3uFgHcEoEDK3qaNsYWRoF0IIYSwsZxCI3vP5Fhur9yfYrexbD/lWPPZNZH+Wol82wjaz+WX8N22RI6l5dl7KEKIZlRxqbdWTQvac5Lq/5yk7WAqVRvh1UTLtrexee3SiE4IIYSwsc0nMlAUcDHoKTWZWbU/hTnX9sTQzJluRVEsTegcZT67JjrAg12J2a26GZ2iKGw5kcmirQn8uj8Fo0mhe7gPK2ZeZu+hCSGaQX7J+Uqrq7o7WGl8SR4sfkSdz37FP0HfxAZ5DQ3aSwshdZ/67+qWe9NcdBd0Gwexbet9U4J2IYQQwsY2l5fG39g/kuV7znIuv5QdCVlcHNu82e6EjELO5Zfg4qQnPtK3WY9dF21ee2tsRpddWMqPf59h0dYETqQXVHrsUEouRaUm3F0cqIO0EMImNhxJp9Rkpn2gh8P0FLHIOgkHloBHEIx4vun789GC9sT6bZ+8G8xl4BV2PuCvTqermj62FkiCdiGEEMLGNpZnVoZ1DqbEaOKnnWdYuS+l2YN2bT5770hfXJ0cK0jUln1rLWu1K4rC36ezWbQ1geV7kikpU5sPeroYuLZvOyZeHM3U+ds4l1/C4dQ8+kT52XfAQgib++2gOp/9ym6h6HSO0VNEo8uyUud4Tdcx6tz40J712z5pm/o9coA6J15UIkG7EEIIYUOpucUcS8tHp4NBHQLR6eCnnWf4dX8Kz17TrVk/uGnz2fvHONZ8doAo/9axVntesZElO8+waOtpDqWcn6/eLdyHOy6J5to+7fByVT9+dY/w4Y8j6Rw4mytBuxCtnMmssPaQ485n11lruTdNWLz6VV9ntqvfIwfUY9u/1SC/4wgIjGvc+FoYCdqFEEIIG9K6xveM8MXXw5lhnYNxdzZwJruIvWdy6BXp12xj2ZagZtodbT47nF+rPSmzCLNZcZjO9vW170wOi7Ym8L9dZyksNQHg6qRnXO8Ibh8YTZ8ovyoXaLqFe6tBe3JOdbsUQrQif5/OIqvQiK+7MwMc8D3Y6pn2hkrSgvZa5rNr1s+Fo6tgzBsStAshhBCi6TYeU0vjB3cMBMDN2cDwrsGs2JvCyn0pzRa0Z+SXWOZT949xvA+M4b5uGPQ6Sk1m0vJKCPN1s/eQ6u23A6lM/2K75XbHEC9uHxjNDX0j8fVwrvF53cN9ADhwNtfmYxRC2JfWNX54l2CcDQ64gJctgvbjayE7EXreAK7eNW9nNsHA+9TseXifuvcb3lsN2pN3WWukDk+CdiGEEMJGFEVh0zE10z4kLshy/6geYZag/YlRXZqlRH5Hedf4zqFe+Hm42Px4DeVk0BPh50ZiZhGnMwtbVND++eZTAFzaMYiHrujIxbEB9fo/7RGhBu2HUvJaZHWBEKL+Vh9Ug/YrHbA0HmyUaf/pHihIh4i+EN6r5u30BhjycP33qwX2bWjZNwe8zCOEEEK0DqcyCjmbU4yLQc9FFdZFv6JrCC4GPSfOFXA0Lb9ZxqIt9eZo67NXpDWja0lrtWcWlLKpvNHgv67rycAOgfW+CBMb5IWbs57CUhMJLeg1CyEa5kR6PifSC3DS6xjWJdjew6lCbzZCoTp9ioBY6+3Yp536vSFrtdeHtlZ72kEwFlt33w5KgnYhhBDCRrT57H2j/Sot6eXt5sylndTM+8p9Kc0yFq1zvCPOZ9e0xGZ0v+5PwWRW6BHhQ/sgzwY916DX0SVULRmVEnkhWq815V3jL+kQiI9bzVNm7MWsd6bsydMway94WPHCbn3Xaj+2BjJPgKLUf78egeoScWkHmjbGFkKCdiGEEMJGNpXPZx/SMajKY6N7hgHwSzME7UWlJvadUZudDXDAzvGaqBa47NvyPckAjO0V3qjndy8vkZdmdEK0Xlpp/IhuIXYeSS30BvCLtu4+faPU77m1BO2mMvjmdni3L5w7Ur/96nTns+1tZF67BO1CCCGEDZjNiiXTPjgusMrjI7qFYtDrOJicS0JGgU3HsjspG6NJIdTHlUh/d5seqykqdpBvCTLyS9h8Qr0wMza+kUF7eTO6g8l5dWwphGiJsgpK2V5e6eSo89ltpj6Z9rT9UFYErr4Q2Kn++9bmtZ/d1djRtSgStAshhBA2cDAll6xCI54uBnpXswZ3gKcLA2PVrPev+22bbd9RYT57c64L31DRLSzT/uv+VExmhZ7tfIgJbFhpvMaSaZfyeCFapS0nMjArahNQ7cKko+mYugzD4ulwfJ11d1yfoD1pm/o9sj/oGxCa9r0D7lwJo15q/PhaEAnahRCiHsxmheSclpH9E45BK42/ODagxuV9rm6mEnnLfHYHXOqtoqjyKoDUvGKKjSY7j6Zuy/eeBWBsfESj99ElTA3aU3KLycgvscq4hBCO4+/T6kXTixy4CWhw3n70B5ZAXrJ1d1yvoL0B67NXFBgHMYNqX0quFZGgXQgh6uHVXw8xaO5afj+Sbu+hiBZiY3lpfHXz2TUje6hB+87T2aTk2KYDrsmsVMq0O7IATxc8XAwoCpzJduyLZBn5JWw+3rTSeAAvVyfaB6rZNymRF6L1+ft0NgB9ox33oqlniTrn3qrLvQEEdYLr5sH1H9W8jRa0txtg3WO3MhK0CyFEPew4pQY9exKz7TsQ0SKUlpn566Sa3R4cV3PQHurjRr9oPwBWHbBNtv1Iah55xWV4uhjoGubYGQmdTtdiln1buT8FswLx7XyJDmxayatWIn8wWUrkhWhNSsvM7C1vAqq91zsUswndiXV4lKoXmfG1ciM6N1/oMxFiL6v+8cJMyDiq/juyEUH78bXwy1Nw5NfGj7GFkKBdCCHqQVuCKi1PyldF3XYnZVNYaiLA06XOQPnqnmqW9pe9tgnatQZI/WL8caqhTN+RRFqWfXPsTHtTu8ZXpDWjOyBBuxCtyv6zOZSWmfH3cCa2gUtC2tyBpfB2T5y+vhlLp5PPrlTvby5n/la/B8Q1bqm54+tg64cStAshhIBio4nUXDVYT8uzTQmzaF20+eyDOgSi19fe+G1UeYn81pMZZBaUWn0s28qrRBx5qbeKWkKm/Vx+CVua2DW+om7h0oxOiNZoZ4XSeIdqAnpgKXw3GXLPVr4/N1m935qB+5kdsONzSNlb9bF2/WDClzD8H43bt2XZt92NH18LIUG7EELU4WyFubVa8C5EbbT57IM7Vl3q7ULRgR50D/fBrMBvB1KtPhZtPvtF7R13PmVFUQFqMzpHDtpX7lNL43tF+lqlG7RWHn8sPb9FNOATQtSP1oSubzUriNiN2QQrnwKUah4sv2/l0+p21vDXJ/Dzw3B0ddXHPAKg+3iIv6lx+47oq35P3aeu996KSdAuhBB1qFimmy7l8aIOhaVl7Cz/oDaklvnsFZ3vIm/dzr1ns4s4k12EQa+jjyPOp6xGS1j2zVIab4UsO0CYjxv+Hs6YzArH0vKtsk8hhP1pmfZ+jrRyR8Kmqhn2ShTIPaNuZw0+7dTvtXWQbyz/WHDxhrJiOHfY+vt3IBK0CyFEHSpm/NLzSlCU6q5OC6HadioLo0mhnZ87MfVsUDa6PGjfeCyD3GKj1cayo/wDY88IHzxcnKy2X1uKcvDy+PS8EraeVEvjx1gpaNfpdLJeuxCtTGpuMWeyi9DroLcjZdrz61nRVd/t6lLTsm/Zp+H31+DkH43ft15/vkT+7K7G76cFkKBdCCHqkFQh015qMpNdaL2gSrQ+m46Vl8bHBdZ7DmOnUG/igj0pNZlZdyjNamPZkZANQP8WMp8dILJ8rfbc4jJyHPB3Tesa39tKpfGabmHSjE6I1kSruOoc6o2XqwNdNPUKte52dfGNUr/nnql8/6k/Yd1LsO7lpu2/jcxrl6BdCCHqoHWO10gHeVGb+qzPXh0t275yn/W6yLe0+ewAHi5OBHm5AlV/9xzB8j1qWak1usZXJJl2IVoXh12fPWYw+EQANV1U1qkl7TGDrXM8S6Y9sfL9SdvU741Z6q0iLWjPPt20/Tg4CdqFEKIOSZkXBu3SQV5UL7uwlP3lQdeguLqb0FU0uocaBK4/nE5RadMbABWVweHy+dH9W1DQDo7bjC4tr5i/TqpL6GlL9VlLxbXaZQqOEC3f3+UXTR1ufXa9AUa/Wn7jwsC9/PboV9TtrMG3fE57cQ4UV7goaQnaL2ra/ruOgccOw8RvmrYfBydBuxBC1EFrRBfk5QJAmnSQFzXYciIDRYGOIV6E+rg16Lk92/nQzs+dIqOJ34+kN3ksp/J0KAq0D/QgxLthY7E3R21G92t51/jeUX5WLY0HiAv2wsWgJ6+krNKUHCFEy1NaZmbPmRzAwZrQabqPhwlfgM8FFx99ItT7u4+33rFcvcHNV/23ViJfWgCpB9R/t2tipt3VG7zDmraPFkCCdiGEqEVBSZll7WytxE3K40VNNpavzz6kgVl2UJuRaSXyv+5veon8iTw1YzKgfcuZz66J8i9vRudg5fHLyrvGX2OlBnQVORv0dAr1ArBUawghWqaDybmUlpnx83CmQ5CnvYdTve7jYdY+yu5YwvaY+ym7YwnM2mvdgF1z3TyY8jP4Rau3z+4CxQTeEecz8aJWErQLIUQttIyXj5sTccHqB2opjxc1Ob8+e8Pms2u0oP23g6mUlpmbNBZL0O6IWZ46aOXxpzMdJ+OcllfMX6fKS+PjbZPV6R4uzeiEaA0qrs9e34akdqE3oMRcypmAQSgxl1qvJP5CXcdC7FBwKb+AYa357Jojq2DRzWo3+lZKgnYhhKiFNqc2KsCDEG+1OZaUx4vqpOQUcyK9AL0OLunQ8Ew7QP9of4K9XckrLmNT+QWAxigtM5OQp/67RWbay0vPL+wnYU8r96WgKNAnyo9If+uWxmsqzmsXQrRcDtuEzlEk71K/N3U+u6bwHBxdBSfWW2d/DkiCdiGEqEVSeXlulL8HIT7lQbtk2kU1NpYv9Rbfzhdfd+dG7UOv1zGyu7rMTlNK5Pcn52JUdPh7OBMX7KClmbXQyuOTsoowmx2jKZulNN7KXeMrsmTapTxeiBbtfBM6CdoByDwJOz6HA/9Tb9/wKdz3J8TfbJ39W5Z92wPmplWpOSoJ2oUQohZaE7pIf3dLMy+Z0y6qo5XGD4prXGm8RutKvmp/KqZGBqxalqd/tIOXZtYg3NcNJ72OUpOZVAe4SJaWW8w2S2m87YL2ruVB+5nsIodco14IUbe03GLOZBeh00HvKF97D8cxnNkBPz8MWz9WbxucICy+aiO8xgrqAk5uUJoHmSess08HI0G7EELUoqbyeFmSSVSkKAqbtCZ0HRtXGq8Z2CEAX3dnMgpKLYFiQ+1IyAagfwuczw7gZNAT4act+2b/ee2/lJfG9432o135uGzB192ZSH91/zKvXYiWSbto2iXUG2+3xlVdtTo+5c3mLlyr3VoMThDaU/23VnrfykjQLoQQtdAy7VEB7pby+CKjifySMnsOSziYk+cKSMktxsWgZ0BM0+aQOxv0jOimlsiv3NewEvm03GKe/nEPaw6lAWqmvaVypGXflpeXxo+1YZZdI83ohGjZdmpN6Frw+6/V+Uaq33PPwsZ3YfH9kLDJuseI6KN+l6BdCCHanopz2j1cnPB2dQKkRF5UtvG4mmXvF+OHu0vTu+9eXWHpt/pUdRSWlvH2b0cY9vp6vtmWiFmBgcFm+rTg0kytg3yinYP21NxitiWoFQ9jmiNol2Z0QrRols7xMp/9PO9w0OnBbITtn8HuryArwbrH0Oa1n91l3f06CAnahRCiBjmFRvKK1Yx6u/KS1WAf6SAvqtpU3oRuSBPns2su7RSEh4uB5Jxidifl1Lidyazw3bZELn99PW//dpQio4m+0X58e/fFTOxobpHz2TVah3Z7B+2/7E1GUaBftJ+lZN+WukkzOiFarNIyM3vK37OlCV0FBid1TXaArFPqd2t1jteE9wEXb3Dxsu5+HYSTvQcghBCOKrE8yx7k5YKHi/p2GeLtyon0AukgLyzMZoXNJ9RMe2PXZ7+Qm7OB4V1DWL4nmZX7UugT5Vdlmz+OpPPyioMcSlHXdosKcOfp0d0YEx9GWVkZK/ZZZSh2o5XHa7+H9rJ8b3lpfK+IZjmeVh5/NC2P0jIzLk6SXxGipTiUkktJmRlfd2c6BLW8lTtsyicCcpPUfzt7gn976+4/tCc8fRr0rfM9s3W+KiGEsAKtNL7imsxaB/l0KY8X5Q4k55JdaMTTxUCvSOuVo2sl8iv3JVcqkT+Uksvk//7F5P/+xaGUPHzdnfnn2G789ugwxvYKb9HZ9Yq0tdrt2YguJaeY7eVLN42JD2uWY0b6u+Pt5oTRpHAsLb9ZjimEsA5tqbc+UX7o9a3jvdgqDiyFlL3nbxsL4J149X5r0etbbcAOkmkXQogaacGC1s0ZsHSQT82VTLtQaeuzD+wQiLPBeh8YLu8SgouTnlMZhRxOzSPAw4U3Vx3h+x3qnHVng47Jg9rz0BUd8fNwsdpxHYWWaU/JLabYaMLNuem9Ahrql31qaXz/GH/CfW1fGg+g0+noHu7D1pOZHEzOtcxxF0I4Pq1zvJTGV3BgKXw3GbigP0tusnr/hC+g+3jrHtNsAn3z/82wpdZ7OUIIIZpIK8vVMn6ApYO8NKITGq0J3eC4pi31diEvVyeGdlLL7Z/5aS/DXl/Pt9vVgH1MfBi/PTqMZ6/p3ioDdgB/D2c8y5v6ncm2T7a9ObvGV9RNOsgL0SJpTej6xfjZdyCOwmyClU9RJWCH8/etfFrdzhpOrIf3+sOim6yzPwciQbsQQtTAskZ7NeXx0ohOgNp0aNtJtbP4ECvNZ69odE81WNx5OtvSZO7H+wfxwe39iQls3fMldTqd5YKZPZZ9q1wa37xBu5Zdl2Z0QrQcaXnFJGUVodNRbR+SNilhk7rMW40UyD1jveXfXH0g4xic3Qn1WHmlJZHyeCGEqEFShTXaNVp5vDSiEwC7EtVgOtDThS6h3lbf/1XdQ2kf6IFOp+PxkV0YEx/Wauas10dUgAeHUvJIskPQvqK8Ad2AGH/CfN2a9dgV12pXFKVN/Z8L0VLtLC+N7xzijbebs30H4yjyU627XV1CuoPeCYqyICcR/KKts18HIEG7EEJUQ1EUS9BeqRGdlMeLCrT57IPiAm3SdMjX3Zl1j1/eZoM2rcrFHpn2813jmzfLDtAp1AsnvY6cIiPJOcXNstScEKJpzq/P7mffgTgSr1DrblcXZzcI6aY2vUve3aqCdimPF0KIapzLL6XIaEKngwi/81m2EB/133nFZRQbrTQHS7RYm46rQftgK63PXp22GrADRJdXuTR3B/nknCJ2JGSh08HVPZs/aHd1MtAxRF1rWErkhWgZdiZkA9KErpKYwepSb9T0d0wHPu3U7awlLF79vutrOLnBevPl7UyCdiGEqIbWhC7Mxw1Xp/MdSL1dnXBzVt86ZV5721ZQUmYphxzS0bpN6IQqyk5rta/YmwLYpzRe012a0QnRYhhNZvacyQakCV0legOMfrX8xoWBe/nt0a9Yr9P7gaVwaLn678PL4fNr4O2e1l1azk4kaBdCiGqcL42vXJaq0+nON6OTee1t2l+nMikzK7Tzc7csTyasK9pOjeiW71EbJzV31/iKpBmdEC3HoeQ8io1mfNyc6BDkZe/hOJbu49Vl3XwueD/1ibDucm/a0nLFOZXv15aWa+GBu8xpF0KIalTXOV4T4u3K6cxCmdfexm0qn88+pGNgmy5htyWtn0RecRk5hUZ8PWzf3OlsdhF/n85WS+PtGbSXZ9oPpkjQLoSj0+az94n2t0l/kxav+3joOlbtEp+fqs5hjxlsvQx7nUvL6dSl5bqObbHrt0umXQghqpFUXo4bWU0GVWtGl5ormfa2bOMxdX12Wyz1JlTuLgaCy1dsaK5su9Y1/qKYAEJ97FMaD+fXak/IKCSv2Gi3cQgh6mZZn12a0NVMb4DYyyD+JvW7NYPn5l5azg4kaBdCiGpYlnvzr9q1+Xx5vGTa26qsglLLXONBcTKf3Za038HmmNeekFHAu2uOAnBNb/tl2QH8PV0IL59Pfyglz65jEULUTutvIk3o7KS5l5azAwnahRCiGlp5fGQ15fFa5k8a0bVdm0+oWfZOIV6WizjCNqKaaV57YWkZ9365g9ziMvpE+XHLRVE2PV59WJrRybx2IRzWufwSTmcWotNBH8m020dzLy1nBxK0CyHEBUxmhTPZ5Zn2gOoy7dpa7VIe31b9dTITgMGSZbc5rRldog2DdkVReOrHvRxKySPIy5UP7+hfadUIe9Ga0R2UDvJCOKy/E9TS+I7BXvi42b7vhqiGPZaWa2YStAshxAVSc4sxmhSc9DrCqpnTqq3Vni7l8W2WVhrfK9LPvgNpA7RmkIlZtlur/bM/T/Lz7rM46XV8cHs/uy3zdqFusuybEA7vbymNt7/mXlrODiRoF0KIC2jz2cP93HAyVH2bPJ9pl6C9LVIUhcPlc4y7hHnbeTStX5SNM+2bjp9j7i+HAPjn2G5cHBtgk+M0hlYefygljzKT2c6jEUJUx9KETtZnt6/mWlrOTmTJNyGEuEBty70Blo7SmQWllJaZcXGS659tSUpuMTlFRgx6HR1DZD1eW9OmqJzJKsJkVjBYcTmls9lFPPjVTkxmhRv6tmPK4PZW27c1RAd44OlioKDUxIlzBXQOlYtEQjiSMpOZPUnZgGTaHYKtl5azI/mkKYQQF9C6VNcUtPt7OONsUAOHc/mSbW9rtE7esUGeuDm3/A8Cji7c1x0nvY5Sk9mqyywWG03ct3AHmQWldA/34aXr49HpHGt9Zb1ed75EXprRCeFwDqXkUWw04+3mRFywXMR1CLZcWs6OJGgXQogLWJZ7q6YJHYBOpyPYS0rk26pDyWrQ3lVK45uFQa+jnbbsm5VK5BVF4bn/7WNPUg5+Hs58NKk/7i6O+cFOC9qlGZ0Qjkcrje8T5YfeilVAQlxIgnYhhLhAbcu9aYLLS+TTrJj5Ey3D4RQ1eJKgvfloVS/WWvZt0dbTfLc9Cb0O3rutr2XevCPSOshLMzohHI/WOV5K44WtSdAuhBAXqCvTDueb0aVKpr3N0crju4b52HkkbYelGZ0VOsjvSMhi9s/7AXhiVFcu6xTc5H3aUsW12hVFsfNohBAV7UzMBqBfjATtwrYkaBdCiAqMJjPJOeVBey2Zdi1oT5dMe5tSWmbmeHo+IJ3jm5N2AS2piZn2tNxi7l+4A6NJYUx8GPcN62CN4dlUlzBv9DrIKCiV6ThCOJBz+SUkZKjvSX2i/Ow7GNHqSdAuhBAVJGcXY1bAxUlPUPm89eqEeJeXx8uH6DblxLl8jCYFL1cnIv1rrsQQ1hUd0PTy+NIyMw8s+pu0vBI6hXjx2k29Ha7xXHXcnA10KG9wJSXyQjiOneXrs3cM8cLX3dm+gxGtngTtQghRgdY5PtLfvdamMiE+0oiuLaq4PntLCPhaC63q5XBqHj/sSCK9Eb93/1p+gO0JWXi7OvHRpP54ubacVW+7Swd5IRyOZX32aD/7DkS0CS3nL5YQQjSDutZo12jl8Wl5Uh7flpyfzy6l8c2pQ7Anrk568orLePz73QD0bOfD5Z1DuLxLMH2i/HAy1JyH+GFHEl9sTgDg7Vv7WDLXLUX3CB+W7j4rmXYhHMjO09KETjQfCdqFEKKC+jShgwrl8bmSaW9LDiVL53h78HZzZtlDl/Lz7rOsP5LOnqQc9p3JZd+ZXN5fdwwfNycu6xTMsC7BXN45mJDy1R0A9ibl8I/FewGYeWUnruwWaq+X0Whapv2gZNqFcAhlJjO7E3MAaUInmocE7UIIUcH58vjaM+2h5eXx5/JLMJkVDLI+a5twvjxeOsc3t06h3jw6sguPjuzCufwS/jiSzvrD6fxxNJ3sQiPL9yazfG8yoAa5l3cJZmCHQP7x015Ky8xc2TWEmVd2svOraBxtrfaTGQUUlpbh4SIf34Swp0MpeRQZTXi7OtGxhVXuiJZJ3vWFEKKC+pbHB3q5oteBWYGMghJL5l20XjmFRs7mqNMhpHO8fQV5uXJDv0hu6BeJyaywOymb9YfSLFn4A8m5HEjO5YP1xwGIDfLkrVv61NqnwpEFe7sS7O1Kel4Jh1LypBxXCDvTSuP7RPu12PcV0bJI0C6EEBUk1rM83qDXEeilfohOy5WgvS04nKpm2SN83aRTsAMx6HX0i/anX7R/tVl4HfDhHf1b/P9Z93Affs9L58DZXAnahbAzrXN8X/ldFM1EgnYhhChXbDRZulLXVR4PajO69LyS8mZ0vjYenbC3Qynl89nDpTTekVXMwpvNCiZFwbmWJnUtRfcIH34/ki7N6IRwANI5XjQ3u/4Vmzt3LhdddBHe3t6EhIRw3XXXcfjw4Urb3HvvvcTFxeHu7k5wcDDXXnsthw4dqrTN6dOnGTt2LB4eHoSEhPDEE09QVlbWnC9FCNEKaE3oPF0M+HvUnZWzdJCXZnRtwqEKy72JlkGv17WKgB3Oz2s/KEG7EHaVkV/CqQx1Kl3fKMm0i+Zh179kv//+OzNmzGDLli2sXr0ao9HIyJEjKSgosGzTv39/5s+fz8GDB/n1119RFIWRI0diMpkAMJlMjB07ltLSUjZt2sTnn3/OggULeO655+z1soQQLZTWhC4qwKNea3BbOsjLWu1tgnSOF/akdZA/lJyHyazYeTRCtF1aaXxcsCe+9bjAL4Q12LU8fuXKlZVuL1iwgJCQEHbs2MHQoUMBuOeeeyyPt2/fnn/961/07t2bU6dOERcXx6pVqzhw4AC//fYboaGh9OnThzlz5vDUU0/xwgsv4OLi0qyvSQjRciVl1q9zvCbER9ZqbyvMZoUjqfkAdJXO8cIOYoM8cXPWU2Q0cSqjgDjpWC2EXfwt67MLO3CoOe05Oep6hwEBAdU+XlBQwPz584mNjSUqKgqAzZs3Ex8fT2jo+XVXR40axf3338/+/fvp27dvlf2UlJRQUnI+M5abq2ZPjEYjRqPRaq/H2rSxOfIYRfOR88H6EjLUKp8IX9d6/VwDPdS30NScYrv/P8j5YFuJWYXkl5ThbNAR5efi8D9nOR9apy6h3uxOymFvYhbRfq71fp6cD6IiOR+a5u+ETAB6R/q0ip+hnA/2Vd+fu8ME7WazmVmzZjFkyBB69uxZ6bEPPviAJ598koKCArp06cLq1astGfSUlJRKATtguZ2SklLtsebOncvs2bOr3L9q1So8POqXYbOn1atX23sIwoHI+WA92w/rAT15KSdZseJEndufztABBo4kprJixQqbj68+5Hywjb2Z6v91sKuZ1b+urHN7RyHnQ+viWaq+Ry3buAtdornBz5fzQVQk50PDmRT4O8EA6Mg7tYcVaXvsPSSrkfPBPgoLC+u1ncME7TNmzGDfvn38+eefVR67/fbbueqqq0hOTuaNN95gwoQJbNy4ETe3xi2x9Mwzz/Doo49abufm5hIVFcXIkSPx8XHcskej0cjq1au56qqrcHaWOTRtnZwP1vdJwhYgl5GD+zOiW0id20ckZvPfI39hNLgzZsxQ2w+wFnI+2NbJ9Sfg8DEu6hTBmDHx9h5OneR8aJ2y/kpk088HMXqEMGZMv3o/T84HUZGcD413IDmX0i1b8HQ1cOcNV2FoBWu0y/lgX1rFd10cImh/8MEHWbZsGX/88QeRkZFVHvf19cXX15dOnTpxySWX4O/vz+LFi7ntttsICwvjr7/+qrR9amoqAGFhYdUez9XVFVfXqmVlzs7OLeJkbSnjFM1DzgfrOZOtdo+PCfKu1880IkCdU3ouvxQnJ6d6Na+zNTkfbONoujp1oluEb4v6+cr50LrER6pzaA+m5DXq/1XOB1GRnA8Nt+O0GmD1jfLHzbV19c2S88E+6vszt2v3eEVRePDBB1m8eDFr164lNja2Xs9RFMUyJ33QoEHs3buXtLQ0yzarV6/Gx8eH7t2722zsQojWJb+kjKxCdV5RVIB7vZ4T7KVe/Cs1mckulLlgrdnh8uXepHO8sCft/EvLKyGzoNTOoxGibTGazMzfdBKAK7rWXY0nhDXZNWifMWMGCxcu5KuvvsLb25uUlBRSUlIoKlKzXSdOnGDu3Lns2LGD06dPs2nTJm6++Wbc3d0ZM2YMACNHjqR79+5MmjSJ3bt38+uvv/LPf/6TGTNmVJtNF0KI6iSWd47383DG261+Vz1dnPSW9dxTpYN8q1VsNHHynJppl87xwp48XZ1o56deVDyenm/n0QjRtizZeYbEzCKCvFy47eJoew9HtDF2DdrnzZtHTk4Ol19+OeHh4Zavb7/9FgA3Nzc2bNjAmDFj6NixI7fccgve3t5s2rSJkBD1CpfBYGDZsmUYDAYGDRrEHXfcweTJk3nxxRft+dKEEC2MFrRH1XO5N41lrfZcWau9tTqWlo/JrODr7kyoj1wMFvYVF6JOyzmWJkG7EM2lzGTm/XXHALj7sg64uxjsPCLR1th1TruiKLU+HhERUa+OzDExMQ7TuVkI0TIlZakVPpH+9SuN14T4uHI4NY+0PAnaW6tDFUrjHaFvgWjbOgZ78ceRdAnahWhGS3efJSGjkABPF+64JMbewxFtkF0z7UII4SgSs8oz7QENy7QHe6uZ1zQpj2+1DqeojYdkPrtwBHEhnoCUxwvRXExmhffXqln26ZfF4unqEH28RRsjQbsQQgCJmWqmPaqhmXYpj2/1LJn2cJnPLuyvY7CUxwvRnJbtOcuJcwX4eTgzeVB7ew9HtFEStAshBJBUnmmPbPCcdjXTni7l8a2WFrR3kUy7cAAdy+e0n8kuoqjUZOfRCNG6mc0K75Vn2e8aEouXZNmFnUjQLoRo8xRFscxpr+9yb5oQHymPb80y8kssF2S6hErQLuwvwNMFPw9nFEVK5IWwtV/2pXAsLR9vNyemDGlv7+GINkyCdiFEm5ddaCS/pAxoeKY91Ke8PF4y7a2Stj57dICHzGMUDkGn01lK5CVoF8J21Cz7UQCmDYnFp57LwQphCxK0CyHaPK0JXbC3K27ODVvGRSuPT8stqXNFDNHyHKzQOV4IR6GVyB+Xee1C2MyqA6kcSsnD29WJaUNi7T0c0cZJ0C6EaPMau9wbnG9EV2Q0WbL1oukUReGLzadYczDVruOQzvHCEcVpzegk0y6ETSiKwrtr1Cz71CHt8fWQLLuwL6n1E0K0eYmZ5cu9NbA0HsDdxYC3qxN5JWWk5pbgLeVzVrH3TA7P/W8/ALcPjObZa7o3uArCGqRzvHBE5zPtBXYeiRCt028H0ziQnIuni0Gy7MIhSKZdCNHmnV+jveGZdoBgaUZndfvO5Fr+vWjraa7/YBMnzzVvgGIyKxxJlc7xwvFoQfvJcwWUmcx2Ho0QrUvFLPvkwe3x93Sx84iEkKBdCCEqlMc3PNMOsuybLWhl6YM6BBLo6cLB5FyueXcDP+8+22xjOJ1ZSLHRjKuTnvaBns12XCHq0s7PHVcnPaUmM4nl719CCOtYfzidvWdycHc2MP1SybILxyBBuxCizWtKeTycn9eelitBu7VoZek39o9kxczLuDg2gIJSEw99vZP/W7yXYqPt16c+lKxeOOgc6o1Br7P58YSoL71eR4dgaUYnhLUpisI75Vn2SYNiCPRytfOIhFBJ0C6EaNOaska7xtJBXsrjrUJRFA6nnu/aHurjxlfTB/Lg8I7odGq5/A3NUC6vXTiQ0njhiLQSeWlGJ4T1bDh6jl2J2bg66bn7sg72Ho4QFhK0CyHatPS8EkrKzOh1EO7byKDdMqddMu3WkJpbQnahEYNeZwlMnAx6Hh/VhQV3XkyApwsHknMZ996fNi2XPySd44UD09ZqPyaZdtHGmc0Kc1ccZPJ//7L0IWmMiln22wfGEOwtWXbhOCRoF0K0adp80DAfN1ycGveWKOXx1qUFy+0DPap0jB/WOZgVD1/Gxe0DyC8p46Gvd/LPJbYplz9sWaNdOscLxxMXovZZOC6ZdtGGKYrCnOUH+OiPE/xxJJ1x7/3Jwi0JKIrS4H1tPp7BjoQsXJz03DtMsuzCsUjQLoRo05LKO8dHBjRuPjtUzLRLebw11BUsh/m68dXdA5kxPA6AhVvUcvlTViyXLywtI6G810HXcMm0C8djKY9Py29UgCJEa/D+2mPM33gKgF6RvpSUmfnnkn3c++UOsgpKG7Svt8uz7BMvjibUx83aQxWiSSRoF0K0aU1tQgcVMu1SHm8V54P2moNlJ4OeJ0Z1ZcGdF1nK5a9570+W70m2yhiOpOajKBDk5UKQNCISDqh9oCd6HeQVl8nKFaJN+nJLAm+uPgLAc9d0Z8kDQ/jn2G44G3SsOpDK1e9sYNPxc/Xa15YTGfx1MhMXg2TZhWOSoF0I0aYlZmrLvTVuPjucz7TnFZdRVGr7ruatXUMawF3eJYTlD1/KRe39yS8pY8ZXf/PizweanHnUOsdLabxwVG7OBqLKK4SkGZ1oa5buPstz/9sHwMNXdGTapbHo9TqmX9aBxQ8MoUOwJym5xdz+6VZe//UQRpO51v1p67JPuCiy0f1thLAlCdqFEG1aUnZ5pr0J5fHerk64Oatvp1Ii3zRGk9nSWKu+AXO4rztf330J91+ulsv/d+NJtp7MbNI4pHO8aAk6yrJvog1afziNR7/dhaLApEtieOSqzpUe79nOl2UPXcotA6JQFPjPuuPc/OFmTmcUVru/bacy2XQ8A2eDjvsv79gcL0GIBpOgXQjRpmmZ9qgmZNp1Op2UyFvJqXMFlJrMeLgYGlT94GTQ89TorkwcGA3ApxtONmkc0jletAQV57UL0RbsSMjkvoU7KDMrjOsdwezxPdDpdFW283Bx4tWbevGfif3wcXNiV2I2Y97dwJKdZ6psq2XZb+ofSTs/ybILxyRBuxAtVGJWIQVGe4+iZTOZFc5ma2u0Nz7TDhXWapcO8k2iZbg7h3qj11f9IFaXuy6NBWDNoVRONLJkWFEU6RwvWoQ4LdOebr0mjEI4qkMpudw5fxvFRjPDOgfz5s296/w7MbZXOL/MGmqZQjXr2108+u0u8orVD1B/n85iw9FzGPQ6HpAsu3BgErQL0QJlF5Yy5r1N/GuXgQPlc29Fw6XkFlNmVnA26JrcKVY6yFuHFix3a2TH9rhgL67sGoKiqGXyjZGWV0JWoRG9DjqFejVqH0I0hzjJtIs24nRGIZM/+4vc4jL6Rfsx745+9V6mtZ2fOoXqkRGd0evgp51nGPvun+xKzOa98iz7DX3bNfnivRC2JEG7EC1QQkYhxUYzhWU6pszfwYGzErg3htY5PsLPHUMjsroVSXm8dVjmkoc2vix9+mVq598fdiQ1eMmfimNoH+RZZZ14IRyJVh6fkltsyRwK0dqk5RUz6b9bScsroUuoN/+dehEeLk4N2oeTQc/MEZ347t5BtPNz53RmITfN28S6w+kY9DoevEKy7MKxSdAuRAuUWSEQyS4ycvunWzgoGfcGs8Zyb5pgKY+3Cm0ueZcmlKVf0iGAHhE+FBvNLNqa0ODnHy4fQzcpjRcOztfd2fLeIyXyojXKKTIy5b/bSMgoJCrAnS/uuhg/D5dG729A+wBWzLyMsb3CKTOrq4xc2yeCmEBPaw1ZCJuQoF2IFiijPGhv76XQK9KHrEIjt3+61RLwiPpJzGr6cm8arbxeyuMbL7+kjKTy/5OmNIDT6XTcXZ5t/3xzAiVlDVuG71CydI4XLUdcsBpsSAd50doUlZqY/vk2DibnEuTlypfTBjZ5KhuoF7vev60vb97cmxv6tuPp0V2tMFohbEuCdiFaoMwCNZsb6KYwf3J/ekX6kllQysRPtlrmBIu6JWU1fbk3jdaILl3K4xtNO3dDvF3x92x8JgVgTHw4YT5upOeVsHTX2QY9V5Z7Ey2JpYO8rNUuWhGjycyMr/5m26ksvN2c+PKui2kfZL1suE6n48b+kbx1Sx9CrHAhQAhbk6BdiBYos7xtvJcT+Lg78+W0gcS30wL3LRxJlcC9PpIyrZdp1xrRpeZKpr2xLB3bw5telu7ipGfK4PYAfPbnSRRFqdfzKq4TL+XxoiXQ1mqXZnSitTCbFZ74fjdrD6Xh5qznv1MvopsV/i4I0ZJJ0C5EC6Rl2r2c1UDE18OZhXcNpGc7HzLKA/ejErjXKdGqmXb1Sn1WoZHSMnOT99cWWXtt9IkXR+PhYuBQSh4bj2XU6zmNXSdeCHvROsgfl0y7aAXS8op55qe9LNl1Fie9jnm39+ei9gH2HpYQdidBuxAtkNaIzsv5/H1a4N4jwodz+aXc9slWjqVJ4F6T0jIzKeVZcWsEZ/4ezjgb1A706flSIt8Y1ugcX5GvhzMTBkQB8MmGE/V6zsEKpfGNWSdeiOamlccnZBTKBUNRLzlFRk44UAucMpOZtYdSueeL7Qyau5ZvtycC8OaE3gzvGmLn0QnhGCRoF6IF0hrReV6w4omfhwuLpg+ke7gP5/JLuPXjrVIyWYOz2UUoCrg56wn2cm3y/nQ6nWU/aVIi32CKoljK4605l3zakFh0Ovj9SHq9po0ctnK2XwhbC/Nxw9PFgMmskJAhHeRF3Z5beoB39jvxx9Fzdh1HYmYhb646zKWvrmPagu2sOpCKyazQP8afjyf159o+7ew6PiEciQTtQrRA5zPtVefpaoF7t/LA/bZPtkjgXg2tND7S3wOdzjoZ1WAfWau9sVJzS8gpMmLQ6yyZQ2uIDvRgVPcwAD7bcLLO7bXO8V1lPrtoIXQ6nZTIi3pTFIWNx9XpQpuO12/akDWVlJn4efdZ7vh0K5e9to731h4jJbcYfw9npl8ay+pHhvLj/YMZ2SOs2ccmhCNzqnsTIYSjycyvWh5fkb+nGrhP/GQLh1LyuO2TLXxzzyXEBVsvGGrpEq3YhE6jdZCXoL3htPnssUGeuDkbrLrvu4fGsnJ/Cot3neHxUV0s61pXPw7pHC9ano7BXuxJypELtKJOJ88VkFNUBsCeM81XI38kNY9v/kpk8c4ksgqNlvsv6xTELRdFcVX3UFydrPveL0RrIkG7EC1MSZmJvBL1D65XLb/BAZ4ufHX3JecD94/VwL2DBO5AheXe/JvehE5jWfZNyuMbzJbBcr9of/pE+bErMZuFWxJ45KrO1W6XW2zkTHbT14kXorlpmXYJ2kVddiVmW/594GwuJrOCwUb9OxRFYfHOM3y5JYGdp88fN8zHjQkDIrl5QJRVGsEK0RZIebwQLUx2+RVqg16Hex2X3QLKM+5dQr1Jy1NL5U+ekzmPAIlZanAWFWDNTLuUxzeWZbk3KzWhq0in0zH9slgAvtySQLHRVO12R8rHEObjhp9H09aJF6I5aVVUx9Pl/V3UrmLQXlBq4oQNp1T8fiSdR7/bzc7T2TjpdYzqEcr8qRex8ekreHRkFwnYhWiABmXas7OzWbx4MRs2bCAhIYHCwkKCg4Pp27cvo0aNYvDgwbYapxCiXEZ5abyfuzN6nbGOrSHQy5VFdw8sX789n4mfbGHVI0Pxdquhtr6NSMy0fqY91EfK4xvL1mXpo3uE0c7PnTPZRSzeeYbbLo6uss1ByzrxkmUXLUvHCnPazWZFVj4QNdKCdh0KCjp2J+XQyQYXSwH+OpkJwNDOwbxxcy/LhW0hRMPVK9N+9uxZpk+fTnh4OP/6178oKiqiT58+XHnllURGRrJu3TquuuoqunfvzrfffmvrMQvRpmlN6AI86x90B3m58tXdlxDp705yTjFLd5+11fBajKQsbU67Fcvjy4P2VCmPbxCjyczx8rLebuG2aQDnZNBz55D2AHy64QRmc9UmjlrneJnPLlqamEAPnPQ6CktNJMv7j6hBsdHEwWT1fS4+QH0P3JOUbbPj/X06C4Cx8WESsAvRRPXKtPft25cpU6awY8cOunfvXu02RUVFLFmyhLfffpvExEQef/xxqw5UCKHKKFCzuAGeDSvfDfJyZcqg9ry04iDfbU/i9oExthhei1BUauJc+VrqUh5vfyfPFVBqMuPpYqCdn/X+Py50y0VRvPPbUY6nF/D7kfQq6/9qJfrdpHO8aGGcDXraB3lyLC2f42n5Nv09Ei3X/rO5GE0KAZ7O9A0sZk8m7E7Kscmxykxm9pTvu2+0v02OIURbUq9M+4EDB3jttddqDNgB3N3due2229i8eTN33nmn1QYohKhMy7T7N2LO7fX92uGk17E7MdsSoLRFWhM6b1cnfN2tN01Aa0SXkV+CqZpMrqieVhrfOczbpmW93m7O3HpxFACf/nmi0mOKokjneNGixQV7AtKMTtRMK43vHelLtJf6N+rg2VxKy8xWP9bh1DwKS014uzrRURrgCtFk9QraAwMDG7TThm4vhKi/xpTHa4K8XLmym5pd/G57olXH1ZJoa7S383e32hrtoPYP0OvArKiBu6gfrSy9OTq2Tx0Si0GvY+OxDPafPZ9hOptTTF5xGU56nSyNKFokbV77MVmrXdTgfNDuR6Ar+Lo7UWoy2+QivtYtvk+0n/RYEMIKmtw9/uDBg8yfP59du3ZZYThCiLpYgvZGdreeMEDNNC7eecYmV9dbgiRL53jrdq416HUEekkzuoaydI5vhrL0dn7ujIkPB+CzP09a7j9UPs8zLtgLFydZWEW0PJZmdJJpFzXYlajOMe8d6YtOBz0jfAHYbYN57VrQ3jfKz+r7FqItatAnkxdffJHXX3/dcnvdunX06dOHJ554gosuuohFixZZfYBCiMrOZ9obF7QP6xxMiLcrmQWlrDmYas2htRi26Byv0Urk0/KkGVR9NXdZ+t3ly7/9vPuspWmglMaLlu78sm8StIuqMvJLSMxUL1j3audT6bstmtHtLG9CJ/PZhbCOBgXtP/zwQ6V57S+99BIPP/ww586d4/333+fll1+2+gCFEJVlNDFodzLoubF/JADfttESee2DizWb0GksQXuuZNrrI6/YaKl8aI7yeIBekX5c3D4Ao0nh802ngPNBuyz3JloqLWg/l19KdmGpnUcjHI2WTY8L9sSnvJdLfDs1077Hys3osgpKOXGuAIA+kmkXwirqFbR/8cUXfP7555w6dYpdu3ZZbm/cuBEvLy+++OILzGYzJ06c4IsvvuCLL76w9biFaLOaMqddo5XI/3EkneScIquMqyUwmxX+PHrO8uHFmsu9aaSDfMMcSVWD5VAfV/waOeWjMe4qz7Yv2nqawtKyZp1XL4QteLo6EeGrvv9IMzpxoV3aHPOo85nv+Eg10340LZ+iUpP1jlU+d75DkCf+jUwwCCEqq1fQHhMTQ/v27XFxcSE0NJSYmBiys7Px8fFh+PDhxMTEEBcXh06no3379sTEtN2lpISwtaZ0j9fEBnlycWwAZgV+3JFkraE5rIz8Ej76/ThXvLmeOz7bSnJOMc4GHd0jrD+HOtRHyuMb4nxZevMuszaiWyjtAz3IKTLy1dbTHE9Xs0LNMa9eCFuJC5ESeVG9neWBdJ9oP8t9YT5uhHi7YjIrlRpzNvlYUhovhNXVK2gfNmwYw4YNo1+/fixbtgwXFxdWrlzJmDFjGDp0KMOGDSM8PJyoqCjLbSGE9ZnMClmFTSuP12jZ9u+2J2FuhcuTKYrCXyczmfnNTgbNXcvcXw5xKqMQb1cnJg+KYcXDl9lkLeNgn/JMu5TH18v5tdGbN8Nt0OuYdqmabX/7t6OYzArebk6El2cqhWiJtBJ5ybSLisxmhd3lQfuFjeF6Raq3rbleu3aBoG+FCwRCiKZxasjGr7/+Otdeey1Dhgyha9eufPzxx5bHFixYwOjRo60+QCHEeTlFRpTy+Nrfo2nri4+JD+OFpfs5nVnI1pOZDIprHUs15hQZWfx3Eou2nuZohQ+uvSJ9uX1gNON6R+Dh0qC3vgbR5rSnSnl8vRxKtl8DuJv6R/LmqiPkFBkB6BbmY9UlAIVobpZl3yRoFxWczCggt7gMVye9+l5rPl8K3zvSl98OplqtGZ3ZrFhK8SVoF8J6GvTJtXfv3pw6dYqMjIwqa7E//vjj+PhIWaEQtpRZoAaCPm5OOBuatiyVh4sT43qH8/VfiXy/PbFFB+2KorA7KYdFWxL4ec9Zio3qUnbuzgau7RPBxIHRlmyCrWlBe3qulMfXRVEUDpXPJbdH0O7h4sTtA6P5YP1xu41BCGs630G+wM4jEY5EC6Lj2/nibNBjrBC09yrPvFurGd2x9HzySsrwcDHQJVTeU4Wwlkalmy4M2AHCw8ObPBghRO0y8tXSeG0t8KaaMCCKr/9KZMW+ZF64tgc+bk3L3tvDir3J/GfdMfafzbXc1yXUm9sviea6vu2a/TWFlJfHp+eXoCiKZG5rkZJbTG5xGQa9zpIhbG5TBrfnkw0nMJoU6RwvWjzt9ygxq5Biowk3Z4OdRyQcgdYYrnc1ndy1DvInzxWQU2TE171pfzO1+ey9In1xamJyQQhxXr1+m7755pt67zAxMZGNGzc2ekBCiJo1dY32C/WJ8qNTiBfFRjNLd521yj6b07ZTmTyw6G/2n83FxUnPDX3b8eP9g1g56zImD2pvl4sQweUXVIwmhaxCY7MfvyXRmtDFBnni6mSf4CLUx40Hh3ciNsiTEd1C7TIGIawlyMsFX3dnFAVOSLa9ik3HzvGPxXtZdzgNUyvs5VITLWivbvm1AE8Xy/Kne62Qbf87QT1WP2lCJ4RV1StonzdvHt26deO1117j4MGDVR7PyclhxYoVTJw4kX79+pGRkWH1gQohzq/R3pTO8RXpdDpuuUhtSPd9C1uz3WRWeGHpfgDGxoez9ZkreeuWPvSPCbBrdtvFSW/pNyAd5GunNaGz9zJrM0d0Yt3jlxPqI03oRMum052vWpEO8lW9tOIgX209zZ3ztzHs9XX8Z90x0lt5/5Fio4mDyWolWk1rpmvTx/acyW7y8XYmSud4IWyhXkH777//zquvvsrq1avp2bMnPj4+dOrUifj4eCIjIwkMDGTatGlER0ezb98+xo8fb+txC9EmaZn2QCuue3pd33Y46XXsTsqxzC9uquzCUj7544RNPwz9sCOR/Wdz8XZ1Yva1PRxqLVjLWu3SQb5Wh5JlbXQhrC0u2BOQZnQXUhSFk+fU6gMPFwNJWUW8/uthBs1dw4xFf7Pp2DkUpfVl3/efzaHMrBDk5UKkf/UrpvSOVEvk9yQ2LdOeW2y0NICVJnRCWFe957SPHz+e8ePHc+7cOf78808SEhIoKioiKCiIvn370rdvX/R6mbsihC1ZyuO9rBegBnm5MqJbKCv3p/DdtiSeG9e9SfszmRXuX/g3m09k8Ov+FL69dxAGvXUz37nFRl7/9TCgZkmDrDTH31pCfFw5nJpHWivP4DSVvdZoF6I1s3SQl0x7Jen5JRSWmtDrYMs/rmTV/lS+2prA36ezWb43meV7k+kQ5MnEgdHc2C/SoS4EN8XO8iZ0faL8aqxCs2Tam9hBfndiNooC0QEeDvd3WYiWrsGN6IKCgrjuuutsMBQhRF1skWkHuOWiKFbuT2HxziSeurpLk+YXv7/2GJtPqFNktidksWDTKe4qXw/bWt5bc5Rz+aV0CPZk8qD2Vt23NQSXd5CX8viaGU1mS/muZNqFsB5Lebxk2itJyCgEoJ2/Oz5uztzUP5Kb+kdy4GwuX/2VwOK/z3DiXAH/Wn6Q1349zDXx4dx+STT9ov1bdEPR2uaza3q280Wng7M5xaTnlVj+hjXUTlnqTQibkdS4EC2ItRvRaS7rFESojytZhUbWHExr9H62nMjgnTVHALiqu9rU6/VfD1lKEq3heHo+8zeeAuDZa7rj4uR4b2Pa3Ggpj6/ZyXMFGE0KXq5OtPOrvmRTCNFw2rJvJ84VtKlma3U5Vf53qH2gZ6X7u0f48K/r4tn6fyN4+fp4ekT4UFpm5qedZ7hx3maufmcDX24+RWmZ2R7DbrLd5dnzPlE1zzH3cnWiY/l505Rs+9/lneP71nKBQAjROI73aVcIUaMMGwXtTgY9N/WPBODbbY1rSJeRX8LMb3ZiVuDGfpF8PKk/QzoGUmw088T3u6324fGl5QcpMysM7xLM8C4hVtmntYVIpr1OWml851Av9FaePiFEWxbp74GLk57SMjNJWYX2Ho7D0DLtMYEe1T7u5erExIHRLHvoUpbMGMKEAZG4Oes5lJLHs//bz6d/nmjO4VpFRn4JiZlF6HTQK8q31m21EvndjewgryiKJdPeL0aa0AlhbRK0C9GCZBaomdtAT+vPFbu5v9pF/o+j6ZzNLmrQc81mhce+301qbglxwZ68eG0PdDodr9zQC08Xg6VMvqnWHU5j7aE0nPQ6/nlN0+be25I0oqub1oRO5rMLYV0GvY4OQWo2WTrIn3cqo/pM+4V0Oh19ovx47abebP3HCCZdEgPAn0fP2XyM1qaVxscFe9W5BGrv8qC+sZn2E+XrvLs66ekq7+tCWJ0E7UK0EIqiWMrj/T2tv/54+yBPBsYGoCjw446kBj330z9PsP5wOi5Oet6f2A9PV7VdRlSAB8+M6QY0vUzeaDIzZ9kBAO4c0t5SAuqIQny0TLsE7TVxlOXehGiN4rRmdDKv3eJ0ppppjw6oPtNeHV93ZyYPUoP2v09ntbgS+frMZ9fEt9OC9pxGddHXsuy9In0dctqaEC1do3+rSktLOXz4MGVlZdYcjxCiBnklZRhN6h9SW2TaASYMKF+zfUcS5nqWs+88ncVrK9VO7s+P60638MpX2G8fGM3gOLVM/skfdtd7vxf6fNMpTqQXEOjpwkNXdmrUPppLxfL41riEkDWc7xwvQbsQ1qbNT5agXVVxubf2QbVn2i/UMcSLAE8Xio1m9lphHfPm1JCgvVu4D056HZkFpSRlNazaDtTPAiDrswthKw0O2gsLC7nrrrvw8PCgR48enD59GoCHHnqIV155xeoDFEKossqz7O7OBtxdGt/dvTZj4sPxcnXidGYhW05m1Ll9TpGRh77eSZlZYWx8OBMvjq6yjU6n49Ub1TL5bacaVyZ/Lr+Ed9YcBeCJUV3qLPOzN608vthoJq9ELmxeKLfYyJnyKRiSaRfC+jpKpr2S7EIjecXqe3FDMu2g/g27uH0AAFtPZlp9bLZiNisNCtrdnA10DVffj/eeafi89r+1zvHShE4Im2hw0P7MM8+we/du1q9fj5ubm+X+ESNG8O2331p1cEKI82zVhK4idxcD43pHAPD99tpL5BVF4ekf95CUVURUgDtzb4yvcVmcimXyrzWiTP7NVUfIKy6jR4QPN5dXAzgydxcD3uVTBGRee1VHyrPsYT5u+Hm0jrWQhXAk2vSh4+kFUu3D+fns4b5uuDk3/KL3xbFq0P5XCwraT5wrIK+4DDdnfb0vjp5vRpfdoGMVlJRxOEXtUyJN6ISwjQYH7UuWLOH999/n0ksvrfQBvUePHhw/ftyqgxNCnJeZX75Gu5dtg5xbLlKD4hV7k8kpMta43cItCfyyLwVng473b+tXZ/Z74sWNK5PffzaHb7apFT0vjO+BoYV0Gg/2kQ7yNZHSeCFsq0OwJzqdWg11rvxvR1tWV+f4ugzsoAbt209lUWZqGfPatSx7fDtfnAz1+7jfO7J8XntiwzLte5JyMCsQ4etmWfJUCGFdDQ7a09PTCQmpusxSQUFBjVk2IUTT2WqN9gv1jvSlS6g3JWVmlu4+W+02+8/mMGf5QQCeGt2V3vUoh9Pr1TJ5jwaUySuKwuyfD6AoMK53BBeVlyi2BNq89nQHbkZXUmayy/ikCZ0QtuXmbCDKXw1QpUT+fKY9JqBh89k1XcN88HZzIr+kjIPJedYcms3sSlTnmPcuz57Xh5Zp33cmp0H9Z/6W+exC2FyDg/YBAwawfPlyy20tUP/0008ZNGiQ9UYmhKjEUh5v43JinU7HzQPUNdu/3151zfaCkjIe+monpWVmruwawl2XxtZ73xeWyZ+qo0x++d5k/jqZiZuznqev7tqAV2F/jV327XRGIfd9uYOFWxJsMSwLk1nhjk+3MmjuGhIyGt/VvzEsQXu4BO1C2Io2r12WfVPfVwFighqXaTfodZaLxlvr0e/FEVjms0f71fs5nUK8cHPWk1dSxokGTGPTOsf3bcCxhBAN0+Cg/eWXX+Yf//gH999/P2VlZbzzzjuMHDmS+fPn89JLL9lijEIIzq/RbutMO8D1fdvhbNCxJymHg+XraWueXbKPE+cKCPNx4/Wbeze4wub2SmXye2q8ml9UamLuikMA3DcsjnZ+7o17MXYSWl4en5pb//L4HQlZXP/BRlbuT+G5/+1j/9mGNwOqr++2J7LtVBZlZsXygas5KIrCofK5j11CZS1fIWwlLljNKkumvf5rtNdmYGzLaUZXbDRxqLwioD5N6DROBj09Ixq2XruiKNI5Xohm0OCg/dJLL2XXrl2UlZURHx/PqlWrCAkJYfPmzfTv398WYxRCAJkF6vzyABvPaQcI9HJlRLdQQA3uND/sSOKnnWfQ6+Dd2/o26gJCxTL5v05l8vnmU9Vu9/EfJziTXUQ7P3fuHRrXqNdhT5ZMez3Lz3/efZbbPtlCRkEpLgY9ZgWe+9/+Ri+RV5ucQiOv/3rYcjuxfP3i5pCcU0xucRkGvY64kMZ/gBZC1E4y7ec1dU47nG9Gt+1Upk3el61p35kcyswKQV6uDb7grZXI70mq30XjxMwiy9+tnu3kQqwQttKoddrj4uL45JNP+Ouvvzhw4AALFy4kPj7e2mMTQlSgZdoDmyHTDjChvCHd4p1nKCkzcSwtj2eX7APgkRGdLR9gGqNimfyrK6uWyZ/NLmLe78cAeGZMV5stcWdLIfVsRKcoCv9Zd+z/27vz+KjKs//j39my7wlZIAsBAsgqiyDiVrW4L621WqVFaxdtrFqfbvZXi61Vqm1tS2u1y1O0rdbWWte64fJo6wLKogKy7yQEsu/JZOb8/pg5QyIBsszMOcl83q8XL2FmMnMHD0m+c933denrfwscOTjruDw9f/MpSopzadWuOj2++uhd/Afi3uWbQj0SJGl3FEO7uTV+TE6y4t1D7/8rMFSEQnuMV9ob272h42Ulg6i0TxmVrqQ4l+pbvdp8wN7n2ruPeuvvbrjpRYFKe187yK8Jnp2fNDKNr+lABPU7tD/33HN68cUXD7v9xRdf1PPPPx+WRQE43KFGdPFReb1Ty0YoPy1B9a1ePft+pW54ZI3avD7NH5etr31i3KCf/6o5xZo3pvdt8j95fqPavX7NGZ2l86cWDPq1rDAi1QztR660d3b59a1/fhCqel97cql+9/lZGjsiRTedWSYp8HdxtC7+/fVRZaP+Ejwvf9msQO+CPXXRC+10jgeiwxz7VtHQrpaOLotXYx3zPHtOSpxSgqM4B8LjcmpWcJyZ3Ue/maF9IGfMp44KhPYNFY3y9qFT/upd5tb4/r8WgL7rd2j/7ne/K5/Pd9jthmHou9/9blgWBeBw0ZjT3p3L6dBngqHu1n99qI37m5STEqdfXH58WMauOZ0O3fOZw7fJv7uzVk+/XyGHQ/rBhZOG7FQKc3v8wSM0omto9WrRn1bqn6v2yumQ7rhkim67YFLo7/aa+aUal5uimpZO3fvSpl6fo78Mw9Dip9fLb0jnTc0PjffbU9sWlufvC3OW73EFbKMEIikjKU45weNU/WkqNtwc2ho/+OM4c8xmdNuHRmjvz3l20+jsZKUmuNXR5dfmqmPvKFgTfK2ZnGcHIqrfoX3Lli2aNGnSYbdPnDhRW7duDcuiABwuWiPfujO7yHf6/HI4pF9cfnwojIZDUVaSbg12hb/7hY3afrBZtz+9XpJ0xQlFmhJ8x38oMrfHN3V0qa2z5xudu2pa9Kn739Tb22uUHOfS/159gj5/YkmPx8S5nfrRRZMlSX95Z1dYmtI988GhbvzfO+84FWcFzndWNrT1qaISDqFKex6VdiDSzGr79oOxG9pD494GcZ7dNHdMtqRAMzrDsOe59urmDu2ta5PDIU0r7P/3UKfTEfq4Y51rb/f6tKEi8EYslXYgsvod2tPT07V9+/bDbt+6dauSk2kqBERCu9en1mDwi2ZoL8lO1snjciRJ1582VqeUjQj7a1w1t0QnjslSu9evz/7uba2vaFRqglvfXDAh7K8VTanxbiV4Al9iu59rf29nrT7127e0/WCLRqYn6J/Xn6RPTMjt9TlOGpejC6YVhKUpXUtHl+7690eSpOtPG6fCzCSNSI1XvDvQ9K6iPvLVdq/PH2qKxfZ4IPLGhprRxW5o3xWGzvGmaYXpinM7Vd3coR023b2wNjgNZNyIFKUmeAb0HIea0dUf9XEfBhve5ab2v+EdgP7pd2i/+OKLdfPNN2vbtm2h27Zu3ar/+Z//0UUXXRTWxQEIMKvsHpdDaQkDP5M3EL+4/Hj98QuzIxainU6HfvqZ6UqKc6m6OfB53nRmmbJTonN2P1IcDsdhHeSfWrtPV/5hhWpbOjV1VLqeLJ9/zG3i/+/848LSlO63/7dV+xvbVZiZqK+eNia0xsLMwA9a0dgiv/1gi7w+Qynx7tDrAoicccFK+9YYDu07w9A53pTgcYW2nNt19Ntgtsabpgcr7e/vOXql/dCot/43vAPQP/0O7ffcc4+Sk5M1ceJElZaWqrS0VMcdd5yys7P1s5/9LBJrBGKeGdozk+Ki/o1xRGq8zpqUJ2cYzrEfSfdt8mNHJOsL80ZH7LWiKTf10Kz2pa9s0U2PrlWnz68Fk/L096+eqNy0Yx81KEhPHHRTup3VLfrDGzskSbddMEkJnkMdfs0t8tHoIB+az56fyg94QBSYHeRj+0x7+CrtknRicHKKXZvRhUL7ILarm5X2TVVNavce3sfKtHpX4LWYzw5EXr9Ldunp6Xrrrbe0fPlyvf/++0pMTNS0adN06qmnRmJ9ABT9JnRWWHhiiUqykzU+L1Vx7gFNo7SdvGAov+vfH6miIbBF/sunlOq75x7Xr2Z+18wv1WOr9mrrgWbd+9Im/fDiKf1axx3PblCnz69TynK0YFJej/uKgqE9Gh3kN9E5Hogqc3v8rppW+cZYvBgLtHX6VBVsBhqOSrskzSnNlrTVlqHd7zf0fhgq7QXpCcpJiVd1c4fWVzSGuuZ3ZxiGVgcr7TShAyJvQPtsHQ6HFixYoAULFoR7PQB6EZrRnjJ8Q7vD4dCp48N/Zt5K5ti3ioZ2uZwO/fCiyVr4sYZzfWE2pbvyjyv0l3d26bMnFGnyyL41GHpt4wG9svGA3E6HFl84+bAKt1lp3xOVSnsgtE8ktANRMTI9QUlxLrV2+lR95OmTw5a5gyg90aOMpPB8/5xZkiG306F99W3aU9saeuPTDrZXN6upo0uJHtegmn06HA5NL0zXKxsP6IO99b2G9sqGdh1o6pDL6QiNiQMQOX0K7UuXLtVXvvIVJSQkaOnSpUd97I033hiWhQE4pKY5ujPaER7mue2UeLfuu2qmThvEmxJmU7pnP6jUD55ar8e+Ou+YRxY6unz64TOBbvxfPLk0tFW25xqjF9o30TkeiCqHw6GxI1L04b4GVbXF3pGUnaGt8eEL1klxbk0tTNea3fVauaPWVqF9TbAJ3dRR6XK7BrdjbVphRjC0936u3ayyH1eQqsQ4V6+PARA+fQrtv/jFL3TVVVcpISFBv/jFL474OIfDQWgHIiA07i1pYJ1gYY3LZhepo8uvsyfn9xqY++v750/SqxsPhJrSXTa76KiP/9//7tDOmlaNSI3X188Y1+tjirKCjejqItuIrrHdq33BDvUT85nRDkTLuFwztFu9kujbFRr3Ft7pRnNKs7Rmd71W7KjRpbMKw/rcgxGO8+ymQ2Pf6nu933yDgK3xQHT0KbTv2LGj198DiI5DM9qptA8l6YkelX+i97A8EPnpCbrpzDIteX6jfvL8Ri2YnK/0xN7fyNnf0K7fvLpVknTruROPOPrHrBLVtnSquaNLKfGRmU6wOVhlL0hPUDpvPgFRM3ZEILBWtcZipT2wgyiclXZJmluapd+9vt1259rN0D492EhuMMzQvr26RU3t3sO+h6zu1jkeQOT1a++M1+vV2LFj9dFHH0VqPQB6EQrtw/hMO/rmmvmBbe41LZ2696VNR3zcXc99pNZOn2YWZ+iS40cd8XFpCR5lBEN0JLfIb6QJHWAJc5fP/hjcHm9W2ovDXGmfPTpLDkfgTYGqxvawPvdAtXX6Ql9nw1Fpz04JzF43jMA89u46unxavy8wDYRKOxAd/QrtHo9H7e32+OIExBIztGcP4+7x6BuzKZ0k/eWdXVpfcfh5w5U7avX0+xVyOKQfXTzlmGffi6Jwrr37uDcA0WOG9gNtgY7fsWRndWQq7WkJHk0qCBzzscu89nUVDfL5DY1IjdfI9GOPE+2L6UXmFvme32c2VDSq0+dXVnJcqJkpgMjqd5eK8vJy3X333erq6orEegD0ojYGRr6h78ymdH5D+sFT6+X3H/pB3Oc3tPjpQPO5K04o1pQ+dPWNxqz2TXSOByxRnJUsl9OhDr9D+xtjp4V8R5dPlQ2Bg/zhPtMuSXNLsyVJK3fUhP25B6L7qLePTwkZKHNe+8fPta8OnmefEcbXAnB0/T68+O677+qVV17RSy+9pKlTpyo5uecXwn/9619hWxyAgBoq7fiYjzelu2R6viTp0Xf36KPKRqUnevStsyf06bkKg83o9kaoGZ1hGIe2x+fRhA6Ipji3UyVZSdpe3aLNVU0qzomNN8721rXJb0jJcS7lROBo2ZzSLP3pzR22Ode+Jgzz2T/OPNf+/p6elfY15nz2XkbBAYiMflfaMzIydOmll+rss8/WyJEjlZ6e3uMXgPDy+vxqaPNKkjIJ7Qgym9JJ0k+e36jGNq+avdIvXgk0n/ufBeP7vDMj0tvjKxva1dTeJbfTobG54a94ATi6aaMCb5at3dP7+K7hqHvn+EhUg+eUZkmSNlc1h3bDWWltt+p3uEwdlS6HQ9pX36aa5kO7NNZE4LUAHF2/K+3Lli2LxDoAHEFda+CHAYdDykwitOOQa+aX6rFVe7X1QLN++cpWbdvjVENblybmp+rKOcV9fp5Ib483z7OPGZGseDfzfIFoO744Q0++X6k1MRTazfPsJWE+z27KSo7T+LwUba5q1sodtTpnSn5EXqcvDjZ1aF99mxwOaWph+ApoqQkejclJ1raDLfpgb4M+MTFXVY3t2lffJqdDmkZoB6Kmz5V2v9+vu+++W/Pnz9cJJ5yg7373u2pri8Ghn0CU1bUEquwZiR65jtFQDLGle1O6h1fu0dtVgevjhxdNltvV941U5ti3vXVtEWlUtbmqWZI0Pi82tuUCdjMj2FDs/b0NPXpgDGeRmtHenVltX2HxuXZz1FtZbsoRx3sOlDk+7v3guXazyj4+LzViI0IBHK7PP9Xdeeed+t73vqeUlBSNGjVKv/rVr1ReXh7JtQGQVNMS2JJGEzr0pntTOkMOXTA1X3PHZPfrOUZmJMjhkNq8PlU3h3+b5xZCO2Cp8bkpinMaau7o0taDzVYvJyp21Uamc3x3c0LN6Kw91752T+CMeTjPs5vMyv2HwQ7ya0Lz2TnPDkRTn0P7n//8Z/32t7/Viy++qCeffFLPPPOMHn74Yfn9/kiuD4h5h8a9xVu8EtjV98+fpNQEtxJdhr599vh+f3y826WCtMCIoEhskd96INCEriw4egpAdLldThWnBCrsq3fVWbya6NhVY26Pj1ylfW6w0r6hslGN7d6Ivc6xrA01oQt/kJ4WqrQ3yDCMUKV9ZhhmwQPouz6H9t27d+u8884L/fmss86Sw+FQRUVFRBYGIIBxbziW/PQE/fuGk/Sd6T4VDHA+b2Foi3x4Q7thGNp6IFDZK8sjtANWGR3c6LJ69/AP7V0+f6ix5uicyFXa89ISNDo7SYYhvbfTmmq732/og2CvgkhU2iePTJPb6VB1c4f21rXpg331kqi0A9HW59De1dWlhISePwx6PB55vda9swjEgprgdmU6x+NoCtITlDmIzRhmM7pwd5CvbGhXS6dPbqcjohUvAEdXalbag5XS4ayivl1dfkNxbqfyUgf2RmZfHTrXbk1o33awWU0dXUr0uDQ+Am+MJnhcoaNNf393j9q9fqUluDUmh6/nQDT1uYOEYRi6+uqrFR9/6KfC9vZ2XXfddT1mtTOnHQivWma0IwrMsW/h3h6/JVhlH52TLE8/muMBCK/RqYHQvvVAsxpavUpPCm/DMjvZaTahy0qSM8INXOeWZusf7+217Fy7OZ99amF6vxqQ9sf0onRtqGzUIyt3SwpU2SP99wqgpz6H9kWLFh1228KFC8O6GACHq21lezwirygrUZK0pza8U0G2VHGeHbCDFI9UnJWo3bVtWrOnTqdPyLV6SRETjc7xJrPS/uHeBrV2dikpLrod1c3z7JGcmT6tMEN/W7knVESYwXl2IOr6/JUlEvPZlyxZon/961/auHGjEhMTddJJJ+nuu+/WhAkTJEm1tbVavHixXnrpJe3evVsjRozQJZdcojvuuEPp6YfmUDoch7/b97e//U1XXHFF2NcMRFttcHt8dgqhHZETqVntofPshHbAcjOKMgKhfXf9MA/tke8cbyrKStKojETtq2/T6l31OrksJ+KvaTIMI9RYcHpEQ3vP2e8zOc8ORJ2lexVff/11lZeX65133tHy5cvl9Xq1YMECtbQE3iGtqKhQRUWFfvazn2ndunV68MEH9cILL+jaa6897LmWLVumysrK0K9LLrkkyp8NEBk0okM0mLPaKxva5PWFbyqIGdrHEtoBy5nz2od7M7qdZuf4KJ27tmpe+6Pv7tHG/U1yOx2aPTpyQXp8Xqri3YciQyTfIADQu+ju4fmYF154ocefH3zwQeXm5mrVqlU69dRTNWXKFD3++OOh+8eOHas777xTCxcuVFdXl9zuQ8vPyMhQfn5+1NYOREsNoR1RMCIlXnFupzq7/Kqsb1dxGCpUhmGEzrSX5TKjHbCa2V187Z56+f3GsD2XbG6Pj0alXQqE9ifW7ItqM7qPKht1+9PrJUnfPHuCciPYcM/jcmryyDSt3l2vcbkpSk8cvv0QALuyNLR/XENDYGRFVlbWUR+TlpbWI7BLUnl5ub70pS9pzJgxuu6663TNNdf0um1ekjo6OtTR0RH6c2NjoyTJ6/Xauhu+uTY7rxHh5fcbqgueaU+Ld/b4f8/1gO7CcT0UZiRqe3WLth9sVEHa4H8oO9jUoYY2r5wOqSgjjms1ivj6gO7M62BMdryS4lxqau/Sxor6YTmG0e83tCt4zGdkWnS+7swqSpMUeDOkubVd8R5XRF+vpaNL5Q+vUkeXX6eV5eiaE4v69XkO5OvDtFGB0D6jKJ2vK8MM3y+s1de/d4dhGEaE19Infr9fF110kerr6/Xf//6318dUV1dr1qxZWrhwoe68887Q7XfccYfOOOMMJSUl6aWXXtLixYt1zz336MYbb+z1eW6//Xb98Ic/POz2Rx55RElJ0XlXFuiLFq/0vfcCb1D9fG6X3DTfRgQ98JFTH9U7dfkYn07KG/y3hs0NDt23waWcBEO3zfCFYYUABuvX653a2ujUFWN8mheGf+d2U9ch3b7aLafD0M/m+uSKwmYCw5BuW+VSk9ehr0/u0ri0yL7WX7c69V61U+lxhr49zaeUKBS+Gzullyuc+kSBf1DjRQH01NraqiuvvDJUmD4S21Tay8vLtW7duiMG9sbGRp1//vmaNGmSbr/99h733XbbbaHfz5gxQy0tLfrpT396xNB+66236pZbbunx3EVFRVqwYMFR/7Ks5vV6tXz5cn3yk5+Ux8PWpFiw/WCL9N6bSo536aILFvS4j+sB3YXjeljp+0gfrdyjzMJxOu+TZYNeU+2K3dKGjZo+OlfnnTdj0M+HvuPrA7rrfj1scO/U1v/sUFdGsc47b7LVSwu7d7bXSqvfU3FWsi48/+Sove6LTe/r+fVVcuVP1Hmnj4nY6zy2aq/ee2eDXE6HHvjCCZpd0v+z7AP9+kB75+GJ7xfWMnd8H4stQvsNN9ygZ599Vm+88YYKCwsPu7+pqUnnnHOOUlNT9cQTTxzzgpo7d67uuOMOdXR09Jgrb4qPj+/1do/HMyQu1qGyTgxeU2egIVh2cvwR/59zPaC7wVwPo3MCW2X31reH5ZraXh0YH1eWl8Y1ahG+PqA7j8ej2aXZ+t1/duj9vQ3D8trY1xA4/jg6Jzmqn9+8cTl6fn2VVu2uj9jrbtrfpB/9e6Mk6ZZPjte8cYObAMDXB3TH9WCNvv6dW7rZ1jAM3XDDDXriiSf06quvqrS09LDHNDY2asGCBYqLi9PTTz+thIRjN9pYu3atMjMzew3mwFBCEzpEU2hWe114ZrVvOcCMdsBuzBnbWw40q6Ft+J1h3Rka9xadzvEms4P8ql11YZ3AYWrp6NLXHl6ldq9fp44foetPGxv21wBgX5ZW2svLy/XII4/oqaeeUmpqqvbv3y9JSk9PV2JiYiiwt7a26q9//asaGxtDWwhGjBghl8ulZ555RlVVVTrxxBOVkJCg5cuX66677tI3v/lNKz81ICzMcW/ZhHZEQWFmoKfH3jDNag/NaB+Gza6AoSonJV4l2UnaVdOqtXvqddr4EVYvKazMzvHFWdHtUTQ+N1UZSR7Vt3q1bl+DZoR5lvltT63TtoMtykuL1y8+O33Ydv4H0DtLQ/v9998vSTr99NN73L5s2TJdffXVWr16tVasWCFJGjduXI/H7NixQ6NHj5bH49F9992nb3zjGzIMQ+PGjdO9996rL3/5y1H5HIBIYkY7oskc81bT0qmWji4lxw/8W0RdS6eqmwPX79gRhHbATmYWZ2pXTatW76obdqE9VGnPiW5odzodOmF0lpZvqNKKHbVhDe2PvbdH/1q9T06HtPSKGcpOYScpEGssDe3Halx/+umnH/Mx55xzjs4555xwLguwjZpmQjuiJy3Bo/REjxravNpT16qJ+QNvzLn1YKDKPiojcVDhH0D4zSjO0BNr9mn17jqrlxJWhmGEKu0lUd4eL0lzSwOhfeWOWl0Xpu3rm6uadNtT6yQFzrHPHZMdlucFMLQwQAqwsdqWQEMdQjuixTzXvrtmcFvkt1QFQvs4zrMDtjMzWAVeu6defv/wGftW3dyp1k6fnA6pMDMx6q8/tzQQqN/dWStfGP5eWzu7VP7warV7/TqlLEdfO33csT8IwLBEaAdsrLY10CSI0I5oMc+BDrYZHU3oAPuamJ+qRI9LTe1d2hbcFTMcmFX2kRmJine7ov76xxWkKiXerab2Ln1U2bcxTkez+Kn12nKgWSNS43XvZ4/nHDsQwwjtgI2ZlfbsFEI7oqMo2IxuzyCb0dGEDrAvt8upaYXpkjSstsib59lLsqN7nt3kdjk1Kzg3feWO2kE91+Or9uqxVXvldEi/uuJ4jUjlHDsQywjtgI3Vhs60880a0VGUFd7QPi43ddBrAhB+M4PhcvWuemsXEkZWnmc3zR0TGP02mNC+9UCTvv9k4Bz7TWeO10ljc8KyNgBDF6EdsCnDMEJz2hn5hmgJhfa6gYf2pnavKhvaJXGmHbAr81z7cKy0j7ao0i4FmtFJ0sqdtcdsptybtk6fyh9eozavTyeNzdYNZ3COHQChHbCt1k6fOrr8kqRMQjuipCjYvGlPbduAfuCUDlXZc1PjlZ7oCdvaAITPjOIMSdKWA81qaPNau5gwsUOlfeqoDCV4nKpt6Qx9LeyPHz6zXpuqmpSTEq9fXnG8XJxjByCLR74BODJzRnuc26nkuOg31EFsGpWZKIdDavP6VN3cOaBzlFs4zw7YXk5KvIqzkrS7tlVr99QPi3ntu0KVdutCe5zbqZnFmXprW41e2lClBE/fv3//Z0u1Hn13jxzBc+y5qQkRXCmAoYTQDthUbbet8Q4H77QjOuLdLuWnJaiyoV176loHFNpDTeg4zw7Y2sziDO2ubdWa3XVDPrTXt3aGdgyYUzCsMqc0S29tq9FPX9ykn764qd8f//UzyjR/HOfYARzC9njApszQzrg3RNtgO8gfakJHpR2ws1Azut311i4kDMzz7Hlp8Uq0eHfaxceP0qiMRCV6XP3+9ZlZhbrpzDJL1w/Afqi0AzZVQ2iHRYqykrRyZ+2AQ7s5o53QDtib2Yxuze46+f3GkJ4Dbofz7KbSnGS9+d0zrF4GgGGESjtgU6EZ7YR2RFlR1qFmdP3V2tmlvXWBjysjtAO2NiE/VQkep5rau7TtYP+bptnJzmrrO8cDQKQQ2gGbOlRpZ0Y7osvcHr97AJX27QdbZBiBHSLZKVy7gJ15XE5NK8yQNPRHv9mp0g4A4UZoB2yqttkM7YzMQnQVZw98Vjvn2YGh5dAW+XprFzJIu2qt7xwPAJFCaAdsqq6VSjusYVbaKxva5fX5+/Wx5nl2tsYDQ8PM4Lz24VNpZ3s8gOGH0A7YFI3oYJXc1HjFuZ3y+Q1V1rf362O3VFFpB4aSGcFK+5YDzWps91q8moFpaveqOrg7rZjQDmAYIrQDNhWa055CaEd0OZ0OFWYGm9H1c4s8M9qBoWVEaryKshJlGNLaIbpFfldw3Ft2cpzSEjhSBmD4IbQDNnXoTDuhHdE3kFntHV0+7QxuUS3Lo9IODBXmufahukXeDO1sjQcwXBHaARvq6PKpqaNLEiPfYI3irP53kN9Z3Sq/IaUmuJWbSi8GYKg4FNrrrV3IAO2qDbxZSBM6AMMVoR2wobqWwLlCl9PBVj9YIjSrva7vs9q7N6FzOBwRWReA8DND+9rddfL7DYtX03+7gjPaOc8OYLgitAM2VNPSIUnKTPLI6ST8IPoGsj2eJnTA0DSxIFUJHqca27u0vbrZ6uX0m3ksh0o7gOGK0A7YkFlp5zw7rFKU1f/QThM6YGjyuJyaVpghSVq9q97StQwEZ9oBDHeEdsCGzEo7oR1WMUN7TUunWoL9FY7FDO3jaEIHDDkzhui89rZOn/Y3BkZTUmkHMFwR2gEbCo17S6aZF6yRnuhRWoJbUt/GvnX5/KFttWVsjweGnKHaQd5slpmW4FZGEj1gAAxPhHbAhszQTqUdVjKbOu2pPXYzul21rfL6DCXFuTQyPTHSSwMQZmZo33KgWY3tXotX03e7gufZS7KTaYAJYNgitAM2VBMM7ZmEdlioP83ozCZ0Y0ek0DwRGIJGpMarKCtRhiG9v6fe6uX0GefZAcQCQjtgQ7XN5vZ4QjusU9SPWe1bu417AzA0hbbID6FmdHSOBxALCO2ADdW2sj0e1jND+94+nGmnCR0w9M0oypA0tM61U2kHEAsI7YANHWpER2iHdYoyA2fT+3KmfQvj3oAhb2ZJoNK+Zned/H4jKq/Z7vXpO//8QH/8z/YBfXyo0p5DpR3A8OW2egEADhdqRJdCaId1um+PNwzjiE2efH7jUKWd7fHAkHVcQZoSPE41tndpe3WzxkXhTbjHVu3V39/bI0nyG4a+curYPn9sZ5dfFfWBNxWptAMYzqi0Azbj8xuqY3s8bGBURqIcDqnN6ws1R+zNvro2dXT5Fed2hqrzAIYej8upaaMyJEXnXHuXz68/vHGown7Xcxv1+Kq9ff74vXWt8htSUpxLI1IYkQpg+CK0AzZT39opI7grMTOJ0A7rJHhcyktNkHT0DvJbDwaa0I3JSZbbxbcVYCibUZIhSVqzJ/Ln2l9Yv1+7a1uVmeTRonklkqRvP/6BXt1Y1aePN8+zF2clMe4NwLDGT1eAzZhb49MS3PIQgGCx4j50kDfHvZXlcZ4dGOqi1UHeMAw98Po2SdIX5o3W4gsn69MzR8nnN/S1h1dr1a7aYz4HneMBxAoSAWAzoSZ0bPWDDRRmBba77607cjO6Q03oOM8ODHUzijMkSZsPNKmx3Rux13lrW43W7WtUgsepRSeNltPp0N2XTtMZE3PV7vXrmmXvatP+pqM+R6hzfA7n2QEMb4R2wGZCTeg4zw4bKMoMVtprjlJppwkdMGzkpiaoMDNRhiG9v6c+Yq9jVtkvn10U+n7ncTl135UzNaskU43tXfrCn1YcdeQklXYAsYLQDthMDaEdNmJuj99zhB+cDcPQNirtwLAS6S3y6/Y16D9bquVyOvSlU8b0uC8xzqX/XTRb4/NSVNXYoS/8aWXozeyPM99MLMmi0g5geCO0AzbDjHbYSdExQvv+xnY1d3TJ7XSohGoXMCzMDG6RX707Ms3ofh/sGH/+1ILQ15juMpLi9NAX52hURqK2H2zRNctWqqWjq8djunz+0NelEma0AxjmCO2AzbA9HnZSFDzTXlHfri6f/7D7zSZ0o3OSFefmWwowHMwsCVTa1+6pl99vhPW599S26t8fVkqSvnLqmCM+riA9UQ99cY4ykzx6f2+DrvvrKnV2HfoaVNnQLq/PUJzbqYK0hLCuEQDshp+wAJthezzsJC81QXEup3x+Q5UN7YfdTxM6YPiZmJ+meLdTDW1eba9uCetz//E/2+XzGzqlLEdTRqUf9bHjclO07Jo5Sopz6T9bqvU/j70fehPBPM9enJUkp5NxbwCGN0I7YDO1LR2SCO2wB6fTocLMQLW9t1ntWw8EujvThA4YPuLcTk0rDATqNzYfDNvz1rZ06u/v7ZEkXXfa2D59zPFFGXpg4Sx5XA49836FfvTsBhmGoZ3B8+yjsznPDmD4I7QDNlPbEhixQ2iHXRQeZVb7VjrHA8PS2ZPzJUk/f2mTdoSp2v7QWzvV7vVryqg0nTQ2u88fd+r4EfrZZdPlcEgPvrVT9722VbuqzUo759kBDH+EdgwpDa1erd5dpwONh2/THS7MSnt2MnPaYQ/FwXPtH29GZxiGNleZ2+NTo74uAJFz9UmjNac0Sy2dPt3wyGp1dPkG9XytnV3689s7JQWq7A5H/7a0X3z8KC2+YJIk6WcvbdaTa/dJkkYzox1ADHBbvQCgN3UtndpyoFlbDjRpS9Wh/x5oCgRap0M687g8XTW3WKeWjRg259kMwzjUiC6FSjvswZzVvqe2rcft1c2damjzyumQxoyg2gUMJ26XU0uvmKFzf/WG1lc0aslzG3X7RZMH/HyPvbdXda1eFWcl6ZxgFb+/rp5fqurmTv3mta2qbg58r2RqBYBYQGiHpepbO7Vxf5O2VDUFQnowoJvfjHuTkxKv6uYOLd9QpeUbqlSUlagrTijWZ2cXaUTq0K5ON3V0yesLNNnJSiK0wx6KjrA9fkvwPHtxVpISPK6orwtAZOWnJ+jezx6vax58Vw++tVMnjsnWOVP6H7i7fH794T+BMW9fPnWM3K6Bb/T8nwXjVdPSob+tDJyN50w7gFhAaIdlVu6o1VV/fCcUUj9uVEaiyvJSND4vVeNyU1SWm6JxuSlKTfBoS1WTHlm5W4+v2qs9tW366Yub9MuXN2vB5HxdNbdY88Zk93vrnR3UBt+sSPS4lBhHCII9FAdD+96PbY/nPDsw/H1iYq6+cuoY/f6N7fr2P9/XlFFpKszsX1D+94eV2lvXpuzkOF02q3BQ63E4HPrxJVOVHOeW1+cPfX0CgOGM0A7LPPN+hbw+QzkpcZpemKFxeSkqy00NhfPk+CNfnmV5qVp84WR9++yJevaDCj28YrfW7qnXvz+o1L8/qNSYnGRdObdYl84sVOYQaujGuDfYkbk9vrq5Uy0dXaF/m4dCO+fZgeHsmwsmaMWOWr2/p143/m2N/v7VefL0sVpuGIZ+93qgyr7opNFh2ZXjcjr0/eD5dgCIBYR2WObNbdWSpDs/NTXUpba/EuNcumx2kS6bXaT1FQ16ZMVuPblmn7ZXt+jH//5I97y4SRdMLdBVJxZrZnGm7avvdcHQns15dthIepJHaQluNbZ3aW9dmybkB0L6lipmtAOxIM7t1G8+N0PnLf2PVu+u189f2qzvnjuxTx/7ny3V2lDZqESPS1+YVxLhlQLA8ET3eFhif0O7th9skdMhnTim72NfjmbyyHTd+ampWvH/ztKdn5qiSQVp6uzy619r9unS+9/W5b97Rz5/71vx7aKWSjtsyjzX3n1W+5Zgpb0sj9AODHdFWUm6+9JpkqQHXt+m1/s4v/13b2yTJF0xp0gZ9GoBgAEhtMMSb24NVNmnjkpXeqInrM+dEu/WVXNL9O8bT9aT5fN12axCuZwOrdxZq5014Zk1Gylsj4ddmVvkzWZ09a2dqm4OTHMYO4LQDsSC86YWaOGJxZKkW/6+9pjjVz/c26A3t9bI5XToS6eMicYSAWBYIrQPEQea2rX01a36v0p7b+/uq7e21UiSThqXE7HXcDgcOr4oQz+9bHqou2xVg73nux+a0U5oh70UB/8NmbPazfPsozISj9p/AsDw8v3zJ2lifqpqWjp106Nrj7qD7YFglf2i6SM1KiMxWksEgGGH0D5ErNheq1+/tl0v73Oqs8tv9XIGxTAMvRU8zz5/bORCe3cF6YEfFvYfoypgNbPSPpSa5yE2FGUG/g2Zs9q30DkeiEkJHpfuu2qmkuJcent7je57bWuvj9tV06LnP6yUJH31NKrsADAYhPYh4pwp+cpLjVeT16Hn11dZvZxB2VHdosqGdsW5nJpVkhmV18xLS5AkVdq+0h5sREdoh80UfuxMO03ogNg1dkSKfnzJFEnSL1/erBXbaw57zB/+s11+Qzp9wghNzE+L9hIBYFghtA8RHpdTn5tTJEn6yzu7LV7N4LwZ3Bo/syQjarPIC9IDob3K5pX2utCZ9niLVwL0ZM5C3lPXKsMwtOVAkySa0AGx6tMzC3XpzEL5DenGR9eE3nSWpOrmDj323l5J0ldPHWvVEgFg2CC0DyFXzB4ll8PQ+3sbtHZPvdXLGbC3tkZ3a7wk5aUPjUo7jehgV+Z51NZOn2pbOrvNaCe0A7HqRxdP1tgRyapq7NA3H3tf/uD59j+/tVMdXX5NL8rQiWOyLF4lAAx9hPYhJDslXjOzA98QH3prp7WLGSC/39Db2yPfhO7jCoLb4/fbPLSzPR52leBxKS8tsAPko8qm0Btg40akWrksABZKjnfrN1fOVJzbqVc3HtD//neHWjq69NDbuyRJ1506Rg7H8GigCwBWIrQPMacUBJrQPftBhQ42dVi8mv7bUNmo+lavUuLdml6YHrXXzQ9W2u3ciK7d61Nrp0+SlJVCaIf9mFvkX9t0QJKUmxqv9KTwjmwEMLQcV5CmxRdOkiTd/cJG/eCp9Wpo86o0J1kLJudbvDoAGB4I7UNMSYp0fFG6vD5Df1s59M62m/PZ55Zmye2K3uVnhvbq5g55ffbsvm9ujfe4HEplhBZsyJzV/trGQGjnPDsASbpyTrHOn1qgLr+hx1cHzrJ/+ZQxcjmpsgNAOBDah6DPzy2WJP31nV22DaBHEo357L3JSopTnMspw5AO2HSHQm1zcNxbUhzbCWFLZgf57dUtkqSyXLbGA5AcDoeWXDpVRVmB3hc5KfH69MxRFq8KAIYPQvsQdM7kPI1IjdeBpg49v26/1cvps84uv1buqJUknTQ2O6qv7XQ6lBs8j7u/oS2qr91XNS2BNxNoQge7MrfHm8bShA5AUFqCR/dfNUvTCtN12wXHKcETnekwABALCO1DUJzbqauC1fah1JBu7Z56tXl9yk6O04S86FfozLFv+xvsWWmvaw02oeM8O2yqKDOxx5+Z0Q6guymj0vX0DSfr4uOpsgNAOBHah6gr5xbL43Jo1a46fbi3werl9Il5nn3e2Gw5LTjnlpdmjn2zaaW9mRntsLeij1XaCe0AAACRR2gfonJTE3Te1AJJ0oNDpNr+1rbgfPYon2c3Haq027ODPOPeYHd5aQmKCzaQzEqOU3YKbzABAABEGqF9CLv6pNGSpGc+qFBNsz23fJtaOrq0Zne9JGn+WGtCe356YGuvXce+maGdM+2wK5fToVHBLfLjqLIDAABEBaF9CJtRnKnphenq7PLr0Xf3WL2co1q5s1ZdfkOFmYkqzk469gdEQH6avSvt5si3TEI7bMzcIk9oBwAAiA5C+xC3KFhtt/v4t7fNUW9R7hrfnTmr3e6VdrbHw85mFGVIkuaWZlm7EAAAgBhBaB/izp9WoJyUOFU2tOul9VVWL+eIzCZ0Vp1nlw6F9qrGdvn9hmXrOJI6tsdjCLjxzDK99s3TddH0kVYvBQAAICYQ2oe4eLdLV86x9/i3upZObahslBToHG+V3NR4ORyS12eEtqLbSQ2VdgwBLqdDpTnJcjiiPwECAAAgFhHah4GrTiyR2+nQyp21Wl9hv/Fvb2+vkWFI4/NSlJuaYNk6PC6nRgS7XVfZbIu81+dXQ5tXEpV2AAAAAIcQ2oeBvLQEnTMlX5I9q+3m1viTLOoa3525Rb7SZs3o6loDVXaHQ8pIIrQDAAAACCC0DxPm+Len1laEzkbbxVvBJnRWnmc3hTrI26zSbjahy0j0yOVk2zEAAACAAEL7MDGrJFNTRqWpw2bj3yrq27SjukVOhzR3jPXdpkMd5BvaLF5JT7XNNKEDAAAAcDhC+zDhcDi0aN5oSYHxb102Gf9mVtmnFmYoLcFj8Wq6h/YOi1fSU22r2YQu3uKVAAAAALATQvswcuH0kcpKjtO++ja9/JE9xr+9ZY56s7BrfHeHtsfbrNLOuDcAAAAAvSC0DyMJHpeuOKFIkvSgDRrSGYahN7dZP5+9O7s2oqsxt8enENoBAAAAHEJoH2YWnlgil9Ohd7bXauP+RkvXsu1gi6oaOxTndmpWSaalazEVpCdKkvY3tMswDItXc0gtM9oBAAAA9ILQPsyMzEjU2ZPzJFk//u2tYJV9dkmmEjwuS9diMrfHt3b61NTRZfFqDmF7PAAAAIDeENqHIbMh3RNr9qm+1brxb+Z8drtsjZekxDiX0hMDDfGqbLRFvqYl0BiP0A4AAACgO0L7MDSnNEsT81PV7vXrH+9ZM/7N5zf0drBz/Ek2aUJnMqvtdjrXXtfilURoBwAAANAToX0Ycjgcumb+aEnSn9/eJZ+/b2e3O7p82nawWa9tPKD/bqke1Jnv9RUNamzvUmq8W1NHpQ/4eSLh0Ng3+4T2GrbHAwAAAOiF2+oFIDIuPn6Uljy/UXvr2vTKR1VaMDlfhmGooc2rXTWt2l0b+LWrpiXw+5pWVTa2q3tO//75x+lLp4wZ0Oub89nnjsmS22Wv94YKzNDeaI/Q7vcbqmNOOwAAAIBeENqHqQSPS5efUKTfvb5dP3xmg5a+ukW7alrV1H705mtJcS7lpyVoe3WL7nruI43NTdEnJuT2+/XN8+wnjbXPeXZTns22xze2e0O7ITKTPRavBgAAAICdENqHsc+fWKI//meH9tW3aV99W+j2vLR4FWclqTgrWcVZSSrJTlJxdpKKs5JCI8e+8/gH+sd7e3XjI2v0r6+dpLK81D6/bkeXT+/urJVkryZ0JrPSXmWTSru5NT413q14tz267AMAAACwB0L7MFaYmaSHrpmjzVVNoXBemJmkxLhjB8MfXzJVO6tbtXJnra596D09WT6/z+et1+yuV7vXr5yUeI3PSxnspxF2een2qrSb494yOc8OAAAA4GPsddgYYXdyWY6+eHKpzpqUp7K81D4FdkmKczt1/8KZKsxM1O7aVl3/11Xq7PL36WPfCm2Nz5bD4Rjw2iPFdpX2ZprQAQAAAOgdoR1HlJ0Sr/9ddIKS41xasaNWi59e16eO8m8Gm9DNH2evUW+mgrRESYEKd7vXZ/FqpD21rZKkkRkJFq8EAAAAgN0Q2nFUE/JTtfRzM+RwSH9buUfL3tx51Mc3d3Tp/T31kuzZhE6S0hLdSvAELn07VNs3VzVJksb3o28AAAAAgNhAaMcxnXlcnm49d6Ik6cf/3qD/23TgiI99d0etuvyGirISVZSVFK0l9ovD4VBBeqDabodZ7YR2AAAAAEdCaEeffPmUMbpsVqH8hvT1R9Zo64GmXh9njnqbb9MquykvLTAP3epZ7X6/oc1VzZII7QAAAAAOR2hHnzgcDv34U1N0wuhMNXV06dqH3lNdsOt5d+Z59pNsOOqtO7tU2vfWtanN61Ocy6nR2fbcmQAAAADAOpaG9iVLluiEE05QamqqcnNzdckll2jTpk2h+2tra/X1r39dEyZMUGJiooqLi3XjjTeqoaGhx/Ps3r1b559/vpKSkpSbm6tvfetb6urqivanM+zFu116YOEsFWYmaldNq65/eJW8vkMd5WuaO/RRZaOkQOd4O8tLs8fYN3Nr/NjcFLldvIcGAAAAoCdLU8Lrr7+u8vJyvfPOO1q+fLm8Xq8WLFiglpYWSVJFRYUqKir0s5/9TOvWrdODDz6oF154Qddee23oOXw+n84//3x1dnbqrbfe0kMPPaQHH3xQP/jBD6z6tIa17h3l39leq8VPrw91lH97e6DKPjE/VTkp8VYu85jMsW9WV9o3hc6z22+ePQAAAADrua188RdeeKHHnx988EHl5uZq1apVOvXUUzVlyhQ9/vjjofvHjh2rO++8UwsXLlRXV5fcbrdeeuklbdiwQS+//LLy8vJ0/PHH64477tB3vvMd3X777YqLY/Z1uJkd5b/05/f0yIrdGp+boqvnl+rNrcGt8TY/zy5J+WZot/hMO03oAAAAAByNpaH948xt71lZWUd9TFpamtzuwNLffvttTZ06VXl5eaHHnH322br++uu1fv16zZgx47Dn6OjoUEdHR+jPjY2BLd1er1derzcsn0skmGuzwxpPHZelby0o0z0vbtGPnt2g4qwEvbn1oCRpzuh0W6zxaHKSAtdPZUObpWvdtD+4PT4nsd/rsNP1AOtxPaA7rgd0x/WA7rge0B3Xg7X6+vfuMMy9zRbz+/266KKLVF9fr//+97+9Pqa6ulqzZs3SwoULdeedd0qSvvKVr2jXrl168cUXQ49rbW1VcnKynnvuOZ177rmHPc/tt9+uH/7wh4fd/sgjjygpiWZgfWUY0iPbnFp50Kl4p6EOv0NOGVpygk8Jtno76HANndIPVrnllKGfneiTyxH9NfgM6VsrXPIZDt02o0s5CdFfAwAAAABrtLa26sorrwwVpo/ENtGqvLxc69atO2Jgb2xs1Pnnn69Jkybp9ttvH9Rr3Xrrrbrlllt6PHdRUZEWLFhw1L8sq3m9Xi1fvlyf/OQn5fF4rF6OJOmsLr8WLXtPq3bXS5KmFWXo0xfNtXZRfeDzG/rhmpfl80snnHKG8tOin5i3HWyR7503lehxauEl58rp7N87B3a8HmAdrgd0x/WA7rge0B3XA7rjerCWueP7WGwR2m+44QY9++yzeuONN1RYWHjY/U1NTTrnnHOUmpqqJ554oscFlZ+fr5UrV/Z4fFVVVei+3sTHxys+/vBGaR6PZ0hcrHZap8cj/e4Ls3Xxb97Uvvo2nVo2wjZrOxqPpLzUeFU0tKum1aei7OiveXtNm6TAefb4+IH3XrDT9QDrcT2gO64HdMf1gO64HtAd14M1+vp3bmn3eMMwdMMNN+iJJ57Qq6++qtLS0sMe09jYqAULFiguLk5PP/20EhJ6VkTnzZunDz/8UAcOHAjdtnz5cqWlpWnSpEkR/xwg5aTE629fPlHfOGu8rj1ljNXL6bNQM7qGNktenyZ0AAAAAI7F0kp7eXm5HnnkET311FNKTU3V/v37JUnp6elKTEwMBfbW1lb99a9/VWNjY2gLwYgRI+RyubRgwQJNmjRJn//853XPPfdo//79+v73v6/y8vJeq+mIjOLsJN10VpnVy+gXM7RbNaud0A4AAADgWCwN7ffff78k6fTTT+9x+7Jly3T11Vdr9erVWrFihSRp3LhxPR6zY8cOjR49Wi6XS88++6yuv/56zZs3T8nJyVq0aJF+9KMfReVzwNCVn5Yoybqxb2bn+PH5hHYAAAAAvbM0tB+rcf3pp59+zMdIUklJiZ577rlwLQsxIj89sBNjvwWV9o4un3bWtEqSJlBpBwAAAHAElp5pB6yUnx6stFsQ2rcfbJHPbyg1wa28NI5xAAAAAOgdoR0xq8BsRGfB9njzPPuEvFQ5HBYMiQcAAAAwJBDaEbPM2eyVDe19OoYRTpxnBwAAANAXhHbErNzgtvTOLr/qW71Rfe3NVc2SOM8OAAAA4OgI7YhZ8W6XspPjJEV/7Ju5Pb4sLyWqrwsAAABgaCG0I6aZs9qroniuvbWzS7tr6RwPAAAA4NgI7Yhp3c+1R8vWA4Gt8TkpccpOoXM8AAAAgCMjtCOm5VvQQd5sQleWS5UdAAAAwNER2hHTQmPfGtqi9pqhcW90jgcAAABwDIR2xLQ8C7bHm53jx3OeHQAAAMAxENoR0wrSEyVFtxHdoUo7neMBAAAAHB2hHTEtPz3QCC5alfaGNm/otcZxph0AAADAMRDaEdPyg5X2pvYutXR0Rfz1tgSr7AXpCUpP9ET89QAAAAAMbYR2xLSUeLdS492SotNBnvPsAAAAAPqD0I6YlxfqIB+N0B6otI/P4zw7AAAAgGMjtCPmFUQxtJsz2qm0AwAAAOgLQjtinjn2LRrb47ccYEY7AAAAgL4jtCPmRavSXt3coermTjkc0rhctscDAAAAODZCO2JefjC0R3rsm3mevSgzSUlx7oi+FgAAAIDhgdCOmJcf3B5fFeHt8Zs5zw4AAACgnwjtiHlRq7QfCIx7m5DP1ngAAAAAfUNoR8wzK+3VzR3q7PJH7HWotAMAAADoL0I7Yl5WcpziXIF/CgeaIlNtNwxDm6oI7QAAAAD6h9COmOdwOJSXHi8pch3kqxo71NTeJZfToTEjkiPyGgAAAACGH0I7IKkgLVFS5Ga1m1X20pxkxbtdEXkNAAAAAMMPoR3QoWZ0kaq0HzrPThM6AAAAAH1HaAcU+Q7ynGcHAAAAMBCEdkCHOshHanv8lmBon0BoBwAAANAPhHZAkd0e7/cb2lwVmNE+Pp/QDgAAAKDvCO2AIhva99a1qc3rU5zLqZKspLA/PwAAAIDhi9AOSCoIhvaqxnb5/UZYn3tzcGv82NwUuV38kwMAAADQdyQIQNKIlHg5HVKX31BNS2dYn3tT6Dw7neMBAAAA9A+hHZDkdjk1IjVeUvi3yJuV9jKa0AEAAADoJ0I7EGR2kK9saAvr827aT+d4AAAAAANDaAeC8rudaw+XLp9f2w+2SJIm0DkeAAAAQD8R2oGggvRESVJlGLfH76xpVafPr6Q4l0ZlJIbteQEAAADEBkI7EJQX3B6/P4yV9tB59twUOZ2OsD0vAAAAgNhAaAeCCiIwq90M7eM5zw4AAABgAAjtQFCo0h6B0M55dgAAAAADQWgHgkKV9sZ2GYYRluc0O8cz7g0AAADAQBDagSCze3xrp0+N7V2Dfr6OLp921rRKYtwbAAAAgIEhtANBCR6XMpI8ksIz9m37wRb5/IbSEtzKS4sf9PMBAAAAiD2EdqCb/OC59nCMfet+nt3hoHM8AAAAgP4jtAPdmFvkq8IQ2jnPDgAAAGCwCO1AN2YzuvBU2pslcZ4dAAAAwMAR2oFuQmPfGtsG/VzMaAcAAAAwWIR2oJvQ2LdBVtpbO7u0uzbQOX58Xsqg1wUAAAAgNhHagW7y0xMlDX57/NYDga3xOSlxyk6hczwAAACAgSG0A92Y3eMHO/LNbELH1ngAAAAAg0FoB7oxu8fXtXrV7vUN+Hk4zw4AAAAgHAjtQDdpCW4lelySBneufVOwczyhHQAAAMBgENqBbhwOx6FmdIPYIr8lWGmfkE8TOgAAAAADR2gHPiZ/kB3kG9q8oUZ2ZVTaAQAAAAwCoR34mPy0wVXazSp7QXqC0hI8YVsXAAAAgNhDaAc+ZrCV9s2cZwcAAAAQJoR24GMGH9rN8+yEdgAAAACDQ2gHPsbcHl85wO3xzGgHAAAAEC6EduBjDlXa2wb08YdmtNM5HgAAAMDgENqBjzFD+8GmDnX5/P362OrmDtW0dMrhkMblEtoBAAAADA6hHfiYnOR4uZ0O+Q3pYHNHvz7WrLIXZyUpKc4dieUBAAAAiCGEduBjnE6H8tIG1oxu/b5GSVJZLufZAQAAAAweoR3oRX87yPv8hn7z6hbd/cJGSdLxRekRWxsAAACA2MH+XaAXoQ7yfQjte2pb9Y2/r9V7u+okSedPK9AXTy6N6PoAAAAAxAZCO9ALs9JedZSxb4Zh6Ik1+/SDp9aruaNLKfFu/ejiyfrUjFFyOBzRWioAAACAYYzQDvSiIP3olfaGVq++9+SH+vcHlZKk2SWZ+sXlx6soKylqawQAAAAw/BHagV6EGtH1Uml/a1u1/ucf76uyoV1up0M3n1Wm604bK7eLFhEAAAAAwovQDvSioJdGdB1dPt370mb9/j/bZRhSaU6yfnn58ZpelGHRKgEAAAAMd4R2oBfdK+2GYWjrgWbd9OhabagMjHT73Jwiff/8SUqO558QAAAAgMghcQC9MEN7Z5dfv3l1q37z2lZ1dPmVlRynn3x6qhZMzrd4hQAAAABiAaEd6EWc26mclHhVN3fo58s3S5JOGz9CP71smnJTEyxeHQAAAIBYQWgHjmBkRoKqmzsU73bqe+cdpy/MK2GUGwAAAICoIrQDR1D+iXF6+v0K3XRmmcbnpVq9HAAAAAAxiNAOHMHZk/N1NmfXAQAAAFiIwdIAAAAAANgUoR0AAAAAAJsitAMAAAAAYFOEdgAAAAAAbIrQDgAAAACATRHaAQAAAACwKUI7AAAAAAA2RWgHAAAAAMCmCO0AAAAAANgUoR0AAAAAAJsitAMAAAAAYFOEdgAAAAAAbIrQDgAAAACATRHaAQAAAACwKUI7AAAAAAA2RWgHAAAAAMCmCO0AAAAAANgUoR0AAAAAAJtyW70AOzAMQ5LU2Nho8UqOzuv1qrW1VY2NjfJ4PFYvBxbjekB3XA/ojusB3XE9oDuuB3TH9WAtM3+aefRICO2SmpqaJElFRUUWrwQAAAAAEEuampqUnp5+xPsdxrFifQzw+/2qqKhQamqqHA6H1cs5osbGRhUVFWnPnj1KS0uzejmwGNcDuuN6QHdcD+iO6wHdcT2gO64HaxmGoaamJo0cOVJO55FPrlNpl+R0OlVYWGj1MvosLS2Nf1QI4XpAd1wP6I7rAd1xPaA7rgd0x/VgnaNV2E00ogMAAAAAwKYI7QAAAAAA2BShfQiJj4/X4sWLFR8fb/VSYANcD+iO6wHdcT2gO64HdMf1gO64HoYGGtEBAAAAAGBTVNoBAAAAALApQjsAAAAAADZFaAcAAAAAwKYI7QAAAAAA2BShfYi47777NHr0aCUkJGju3LlauXKl1UtClLzxxhu68MILNXLkSDkcDj355JM97jcMQz/4wQ9UUFCgxMREnXXWWdqyZYs1i0VELVmyRCeccIJSU1OVm5urSy65RJs2berxmPb2dpWXlys7O1spKSm69NJLVVVVZdGKEUn333+/pk2bprS0NKWlpWnevHl6/vnnQ/dzLcS2n/zkJ3I4HLr55ptDt3FNxJbbb79dDoejx6+JEyeG7ud6iD379u3TwoULlZ2drcTERE2dOlXvvfde6H5+prQvQvsQ8Pe//1233HKLFi9erNWrV2v69Ok6++yzdeDAAauXhihoaWnR9OnTdd999/V6/z333KOlS5fqgQce0IoVK5ScnKyzzz5b7e3tUV4pIu31119XeXm53nnnHS1fvlxer1cLFixQS0tL6DHf+MY39Mwzz+ixxx7T66+/roqKCn3605+2cNWIlMLCQv3kJz/RqlWr9N577+mMM87QxRdfrPXr10viWohl7777rn73u99p2rRpPW7nmog9kydPVmVlZejXf//739B9XA+xpa6uTvPnz5fH49Hzzz+vDRs26Oc//7kyMzNDj+FnShszYHtz5swxysvLQ3/2+XzGyJEjjSVLlli4KlhBkvHEE0+E/uz3+438/Hzjpz/9aei2+vp6Iz4+3vjb3/5mwQoRTQcOHDAkGa+//rphGIH/9x6Px3jsscdCj/noo48MScbbb79t1TIRRZmZmcYf//hHroUY1tTUZJSVlRnLly83TjvtNOOmm24yDIOvD7Fo8eLFxvTp03u9j+sh9nznO98xTj755CPez8+U9kal3eY6Ozu1atUqnXXWWaHbnE6nzjrrLL399tsWrgx2sGPHDu3fv7/H9ZGenq65c+dyfcSAhoYGSVJWVpYkadWqVfJ6vT2uh4kTJ6q4uJjrYZjz+Xx69NFH1dLSonnz5nEtxLDy8nKdf/75Pf7fS3x9iFVbtmzRyJEjNWbMGF111VXavXu3JK6HWPT0009r9uzZuuyyy5Sbm6sZM2boD3/4Q+h+fqa0N0K7zVVXV8vn8ykvL6/H7Xl5edq/f79Fq4JdmNcA10fs8fv9uvnmmzV//nxNmTJFUuB6iIuLU0ZGRo/Hcj0MXx9++KFSUlIUHx+v6667Tk888YQmTZrEtRCjHn30Ua1evVpLliw57D6uidgzd+5cPfjgg3rhhRd0//33a8eOHTrllFPU1NTE9RCDtm/frvvvv19lZWV68cUXdf311+vGG2/UQw89JImfKe3ObfUCAAD9V15ernXr1vU4n4jYM2HCBK1du1YNDQ365z//qUWLFun111+3elmwwJ49e3TTTTdp+fLlSkhIsHo5sIFzzz039Ptp06Zp7ty5Kikp0T/+8Q8lJiZauDJYwe/3a/bs2brrrrskSTNmzNC6dev0wAMPaNGiRRavDsdCpd3mcnJy5HK5DuvmWVVVpfz8fItWBbswrwGuj9hyww036Nlnn9Vrr72mwsLC0O35+fnq7OxUfX19j8dzPQxfcXFxGjdunGbNmqUlS5Zo+vTp+tWvfsW1EINWrVqlAwcOaObMmXK73XK73Xr99de1dOlSud1u5eXlcU3EuIyMDI0fP15bt27la0QMKigo0KRJk3rcdtxxx4WOTPAzpb0R2m0uLi5Os2bN0iuvvBK6ze/365VXXtG8efMsXBnsoLS0VPn5+T2uj8bGRq1YsYLrYxgyDEM33HCDnnjiCb366qsqLS3tcf+sWbPk8Xh6XA+bNm3S7t27uR5ihN/vV0dHB9dCDDrzzDP14Ycfau3ataFfs2fP1lVXXRX6PddEbGtubta2bdtUUFDA14gYNH/+/MPGxG7evFklJSWS+JnS7tgePwTccsstWrRokWbPnq05c+bol7/8pVpaWnTNNddYvTREQXNzs7Zu3Rr6844dO7R27VplZWWpuLhYN998s3784x+rrKxMpaWluu222zRy5Ehdcskl1i0aEVFeXq5HHnlETz31lFJTU0NnzNLT05WYmKj09HRde+21uuWWW5SVlaW0tDR9/etf17x583TiiSdavHqE26233qpzzz1XxcXFampq0iOPPKL/+7//04svvsi1EINSU1ND/S1MycnJys7ODt3ONRFbvvnNb+rCCy9USUmJKioqtHjxYrlcLn3uc5/ja0QM+sY3vqGTTjpJd911lz772c9q5cqV+v3vf6/f//73kiSHw8HPlHZmdft69M2vf/1ro7i42IiLizPmzJljvPPOO1YvCVHy2muvGZIO+7Vo0SLDMAIjOm677TYjLy/PiI+PN84880xj06ZN1i4aEdHbdSDJWLZsWegxbW1txte+9jUjMzPTSEpKMj71qU8ZlZWV1i0aEfPFL37RKCkpMeLi4owRI0YYZ555pvHSSy+F7udaQPeRb4bBNRFrLr/8cqOgoMCIi4szRo0aZVx++eXG1q1bQ/dzPcSeZ555xpgyZYoRHx9vTJw40fj973/f435+prQvh2EYhkXvFwAAAAAAgKPgTDsAAAAAADZFaAcAAAAAwKYI7QAAAAAA2BShHQAAAAAAmyK0AwAAAABgU4R2AAAAAABsitAOAAAAAIBNEdoBAAAAALApQjsAAAAAADZFaAcAIMZdffXVcjgccjgc8ng8ysvL0yc/+Un96U9/kt/vt3p5AADENEI7AADQOeeco8rKSu3cuVPPP/+8PvGJT+imm27SBRdcoK6uLquXBwBAzCK0AwAAxcfHKz8/X6NGjdLMmTP1ve99T0899ZSef/55Pfjgg5Kke++9V1OnTlVycrKKior0ta99Tc3NzZKklpYWpaWl6Z///GeP533yySeVnJyspqYmdXZ26oYbblBBQYESEhJUUlKiJUuWRPtTBQBgSCG0AwCAXp1xxhmaPn26/vWvf0mSnE6nli5dqvXr1+uhhx7Sq6++qm9/+9uSpOTkZF1xxRVatmxZj+dYtmyZPvOZzyg1NVVLly7V008/rX/84x/atGmTHn74YY0ePTranxYAAEOK2+oFAAAA+5o4caI++OADSdLNN98cun306NH68Y9/rOuuu06//e1vJUlf+tKXdNJJJ6myslIFBQU6cOCAnnvuOb388suSpN27d6usrEwnn3yyHA6HSkpKov75AAAw1FBpBwAAR2QYhhwOhyTp5Zdf1plnnqlRo0YpNTVVn//851VTU6PW1lZJ0pw5czR58mQ99NBDkqS//vWvKikp0amnniop0PBu7dq1mjBhgm688Ua99NJL1nxSAAAMIYR2AABwRB999JFKS0u1c+dOXXDBBZo2bZoef/xxrVq1Svfdd58kqbOzM/T4L33pS6Ez8MuWLdM111wTCv0zZ87Ujh07dMcdd6itrU2f/exn9ZnPfCbqnxMAAEMJoR0AAPTq1Vdf1YcffqhLL71Uq1atkt/v189//nOdeOKJGj9+vCoqKg77mIULF2rXrl1aunSpNmzYoEWLFvW4Py0tTZdffrn+8Ic/6O9//7sef/xx1dbWRutTAgBgyOFMOwAAUEdHh/bv3y+fz6eqqiq98MILWrJkiS644AJ94Qtf0Lp16+T1evXrX/9aF154od5880098MADhz1PZmamPv3pT+tb3/qWFixYoMLCwtB99957rwoKCjRjxgw5nU499thjys/PV0ZGRhQ/UwAAhhYq7QAAQC+88IIKCgo0evRonXPOOXrttde0dOlSPfXUU3K5XJo+fbruvfde3X333ZoyZYoefvjhI45ru/baa9XZ2akvfvGLPW5PTU3VPffco9mzZ+uEE07Qzp079dxzz8np5McRAACOxGEYhmH1IgAAwPDxl7/8Rd/4xjdUUVGhuLg4q5cDAMCQxvZ4AAAQFq2traqsrNRPfvITffWrXyWwAwAQBuxHAwAAYXHPPfdo4sSJys/P16233mr1cgAAGBbYHg8AAAAAgE1RaQcAAAAAwKYI7QAAAAAA2BShHQAAAAAAmyK0AwAAAABgU4R2AAAAAABsitAOAAAAAIBNEdoBAAAAALApQjsAAAAAADb1/wG8ydewvnsrfwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for plotting\n",
    "last_60_days_prices = scaler.inverse_transform(data_normalized[-sequence_length:].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the last 60 days and the predicted next days\n",
    "plot_final_predictions(\n",
    "    last_60_days=last_60_days_prices,\n",
    "    future_prices=future_prices,\n",
    "    symbol=\"AAPL\",\n",
    "    sequence_length=sequence_length,\n",
    "    pred_steps=pred_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time-series-stock-prediction-oD0mulHE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
